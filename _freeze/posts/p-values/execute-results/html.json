{
  "hash": "1f1c3f9b4629f1a95c8545d0f88f3ded",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Frequentist statistics and p-values\"\nauthor: \"Stefano Coretta\"\ndescription: \"Learn about p-values and how to avoid common interpretation pitfalls\"\ndate: 2024-09-27\nexecute: \n  freeze: auto\nformat: \n  html:\n    css: [webex.css]\n    include-after-body: [webex.js]\n    embed-resources: false\ndraft: true\n---\n\n\n\n\n\n\n\n\n::: callout-warning\n## Prerequisites\n\n- [Frequentist and Bayesian statistics](freq-bayes.qmd).\n:::\n\n## Introduction\n\nThe frequentist approach to probabilities has dominated the field of applied statistics for a bit less than a century. Researchers are usually taught frequentist statistics and the majority of studies employs frequentist statistics. This is the framework that generates *p*-values.\n\nAlas, it is now known that there is a lot of misunderstanding around the real meaning of *p*-values and around what frequentist statistics can do for researchers. This post tries to shed light on these issues and prepare you so that you can read the literature that uses frequentist statistics without been led astray by the common misinterpretations.\n\n## What is a *p*-value?\n\nLet's imagine we are interested in knowing if being a bilingual speaker has a facilitating effect in a memory task. We run a study with monolingual and bilingual participants in which they have to perform a memory task and we note their accuracy in recall. 0% accuracy means they got all the trials wrong and 100% accuracy means they got all trials right.\n\nWe want to compare the average accuracy of the monolingual and bilingual group of participants. Specifically, our (quite vague) hypothesis is that the accuracy of the monolingual group should be lower than that of the bilingual group. We decide to take a frequentist approach to assess this hypothesis.\n\n::: callout-warning\nBe ready for a wild ride! Frequentist inference is just very counterintuitive and our brains try to make it intuitive, by distorting the correct interpretation.\n:::\n\nFirst we assume that the following hypothesis, called the **null hypothesis**, is **true**.\n\n::: callout-note\n### Null hypothesis $H_0$\n\nThe average accuracy of monolingual participants is the same as that of bilingual participants.\n:::\n\nLet's say we found that the average accuracy of monolinguals is 80% and that of bilinguals is 85%. The difference in accuracy between the two groups is 5 percent points.\n\nNow we calculate the probability of finding a difference that is 5 percent points (i.e. the difference we found) or larger, given that the null hypothesis is true (i.e. given that in the real world there is no difference in accuracy between the two groups).\n\nWe run a significance test and that gives us a *p*-value of 0.025.\n\nThis means there is a 2.5% probability that, even if in the real world there is no difference in accuracy between the two groups, we would find a difference of 5 percentage points or bigger.\n\nThis means that in the world in which there is no difference between groups, there is a quite low probability (2.5%) of finding a difference that is that big or bigged. Usually, we set a threshold below which take the risk of rejecting the null hypothesis: in social sciences, this is usually 0.05 (i.e. 5%). Results that are below the threshold are said to be \"statistically significant results\". Those who are not below the threshold are said to be \"non-significant\" (please, don't say \"insignificant\").\n\nSince the *p*-value we got, 0.025 is below the threashold of 0.05, we take the risk of rejecting the null hypothesis and we declare the results as statistically significant.\n\nNote that rejecting the null hypothesis does not mean we can accept the alternative hypothesis that there is a difference between the two groups. We did find a difference between the two groups, that's 5 percentage points. But the only thing that frequentist statistics can tell us is whether we can take the risk or not of rejecting the null hypothesis.\n\nFrequentist statistics cannot corroborate the null hypothesis nor the alternative hypothesis. It only tells us if we can risk rejecting the null or not. Even if a result is significant, that does not mean we have evidence for a difference. The only evidence we can get from frequentist statistics is for rejecting the null hypothesis.\n\n## Statistical significance is not theoretically significant\n\nBut why does a *p*-value not tell us if the results we got are true? That's because of the type of probability a *p*-value is: *p*-values are conditional probabilities.\n\n::: callout-tip\n## *p*-value\n\nA *p*-value is the probability of finding a difference as extreme or more extreme than the one found, *given that there is no difference*.\n:::\n\nWe could write this in mathematical notation as $P(d|h)$, the probability of the data $d$ (the difference we got), given $h$ (the hypothesis that the difference is 0).\n\nMore often than not, we are actually intestered in a different probability, $P(h|d)$: the probability of our hypothesis $h$ (this does not have to be null) given the data we obtained (our results).\n\nWith non-conditional probabilities, like a coin toss, if the probability of head is $p = 0.8$, then you know that the probability of tail is $q = 1 - 0.8 = 0.2$. Alas, while $q = 1 - p$, $P(h|d)$ is not equal to $1 - P(d|h)$ (nor $P(h)$ for that mattter), that's why a *p*-value does not tell you the probability of the hypothesis given the data nor the probability of the hypothesis.\n\nFrequently, researchers interpret a significant result as evidence for the existence of a difference between groups but that is a mistake. You should be very careful when reading discussion sections in papers: you will very commonly find misinterpretations like this one.\n\nStatistical significance is not theoretically meaningful because of the real nature of *p*-values. Moreover, statistical significance is binary: a result either is significant or not. There are no \"almost significant\" results or results \"approaching significance\". Yet, these and similar wordings are very common in the literature.\n\n\n::: callout-note\n#### Quiz 1\n\n\n\n\n\n**True or false?** \n\na. The p-value is the probability that the null hypothesis is true. <select class='webex-select'><option value='blank'></option><option value=''>TRUE</option><option value='answer'>FALSE</option></select> \n\nb. The smaller the p-value the more probable the alternative hypothesis is. <select class='webex-select'><option value='blank'></option><option value=''>TRUE</option><option value='answer'>FALSE</option></select> \n\nc. The p-value is the probability that the result is due to chance alone. <select class='webex-select'><option value='blank'></option><option value=''>TRUE</option><option value='answer'>FALSE</option></select> \n\nd. The p-value is the probability of wrongly rejecting the null hypothesis. <select class='webex-select'><option value='blank'></option><option value=''>TRUE</option><option value='answer'>FALSE</option></select> \n\nf. The smaller the p-value the stronger the effect. <select class='webex-select'><option value='blank'></option><option value=''>TRUE</option><option value='answer'>FALSE</option></select> \n\ng. The p-value is the probability of finding an effect equal to or greater than the one found. <select class='webex-select'><option value='blank'></option><option value=''>TRUE</option><option value='answer'>FALSE</option></select>\n\n\n\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}