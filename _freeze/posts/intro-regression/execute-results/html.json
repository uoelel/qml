{
  "hash": "f4d57532fbeceb236880f7e6fce8c076",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Introduction to regression models\"\ndescription: \"Learn the basics of regression models, a powerful and flexible, yet simple, way of modelling data\"\nauthor: \"Stefano Coretta\"\ndate: 2024-06-03\n---\n\n\n\n\n::: callout-warning\n#### Prerequisites\n\n- [Statistical variables](stat-variables.qmd)\n:::\n\n## Regression (to the mean)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(brms)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: Rcpp\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading 'brms' package (version 2.21.0). Useful instructions\ncan be found by typing help('brms'). A more detailed introduction\nto the package is available through vignette('brms_overview').\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'brms'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:stats':\n\n    ar\n```\n\n\n:::\n:::\n\n\n\n\n**Regression models** (also called linear models or linear regression models) are a type of statistical model.\n\nIn the most general sense, regression models can be used to estimate the underlying process that generates some data. Somewhat more specifically, regression models use the given data to estimate the set of parameters of the equation that is believed to have generated the data.\n\n::: callout-note\n### Why \"regression\"?\n\n...\n:::\n\nThe formula of an example of simplest regression model is the following:\n\n$$\ny \\sim Gaussian(\\mu, \\sigma)\n$$\n\n-   $y$ is the data\n\n-   $\\sim$ means \"is distributed according to\", or \"is generated by\".\n\n-   $Gaussian(\\mu, \\sigma)$ is a Gaussian probability distribution with the parameters $\\mu$ (the mean) and $\\sigma$ (the standard deviation).\n\n## Simulating Gaussian data\n\nLet's move onto an example of how we can use regression models to estimate the mean and standard deviation of data that is distributed according to a Gaussian probability distribution.\n\nAlas, it is very difficult to find in nature data that is truly Gaussian, so we will simulate some.\n\nTo make things a little bit more worldly, we will simulate data of human adult height.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nset.seed(62854)\n\nheight <- tibble(\n  h = round(rnorm(200, 165, 8))\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nheight |> \n  ggplot(aes(h)) +\n  geom_density(fill = \"darkgreen\", alpha = 0.2) +\n  geom_rug() +\n  geom_vline(aes(xintercept = mean(h)), colour = \"purple\", linewidth = 1)\n```\n\n::: {.cell-output-display}\n![](intro-regression_files/figure-html/height-plot-1.png){width=672}\n:::\n:::\n\n\n\n\n## Fitting a Bayesian regression model\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(brms)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nh_1 <- brm(\n  h ~ 1,\n  data = height,\n  chain = 1\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.8e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.023 seconds (Warm-up)\nChain 1:                0.02 seconds (Sampling)\nChain 1:                0.043 seconds (Total)\nChain 1: \n```\n\n\n:::\n:::\n\n\n\n\n## Model summary\n\nWe can now inspect the summary of the model.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(h_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: h ~ 1 \n   Data: height (Number of observations: 200) \n  Draws: 1 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 1000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   165.19      0.59   164.02   166.30 1.00      535      531\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     8.39      0.43     7.58     9.28 1.00      694      498\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\n\nLet's break that down bit by bit:\n\n-   The first few lines are a reminder of the model we fitted:\n\n    -   `Family` is the chosen distribution for the outcome variable, a Gaussian distribution.\n\n    -   `Links` lists the link functions.\n\n    -   `Formula` is the model formula.\n\n    -   `Data` reports the name of the data and the number of observations.\n\n    -   Finally, `Draws` has information about the MCMC draws.\n\n-   Then, the `Regression Coefficients` are listed as a table. In this model we only have one regression coefficient, `Intercept` which corresponds to $\\mu$ from the formula above. The table has 7 named columns:\n\n    -   `Estimate`, the mean estimate of the coefficient (i.e. the mean of the posterior distribution of the coefficient).\n\n    -   `Est.error`, the error of the mean estimate (i.e. the standard deviation of the posterior distribution of the coefficient).\n\n    -   `l-95% CI` and `u-95% CI`, the lower and upper limits of the 95% Bayesian Credible Interval.\n\n    -   `Rhat`, `Bulk_ESS`, `Tail_ESS` are diagnostics of the MCMC chains.\n\n-   Then `Further distributional parameters` are tabled. Here we only have `sigma` which is $\\sigma$ from the formula above. The table has the same columns as the `Regression Coefficients` table.\n\n## Plot the posterior distributions of the coefficients\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(h_1, combo = c(\"dens\", \"trace\"))\n```\n\n::: {.cell-output-display}\n![](intro-regression_files/figure-html/h-1-plot-1.png){width=672}\n:::\n:::\n\n\n\n\nThe plot above shows the posterior distribution of the estimated coefficient `b_Intercept` and `sigma`, which correspond to the $\\mu$ and $\\sigma$ of the formula above, respectively.\n\nFor the `b_Intercept` coefficient, the posterior probability encompasses values between 163 and 167 cm, approximately. But some values are more probable then others: the values in the centre of the distribution have a higher probability density then the values on the sides. In other words, values around 165 cm are more probable than values below 164 and above 166, for example.\n\nHowever, looking at a full probability distribution like that is not super straightforward. Credible Intervals (CrIs) can help summarise the posterior distributions.\n\n## Interpreting Credible Intervals\n\nAnother way of obtaining summaries of the coefficients is to use the `posterior_summary()` function. Ignore the last three lines.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposterior_summary(h_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               Estimate  Est.Error        Q2.5       Q97.5\nb_Intercept  165.188975 0.58641126  164.023908  166.303072\nsigma          8.391712 0.42565293    7.584844    9.279723\nIntercept    165.188975 0.58641126  164.023908  166.303072\nlprior        -6.030282 0.06212402   -6.160126   -5.914505\nlp__        -712.569786 0.98626592 -715.219667 -711.585583\n```\n\n\n:::\n:::\n\n\n\n\nThe 95% CrI of the `Intercept` is between 164 and 166. This means there is a 95% probability, or that we can be 95% confident, that the `Intercept` value is within that range.\n\nFor `sigma`, the 95% CrI is between 7.6 and 9.3. So, again, there is a 95% probability that the `sigma` value is between those values.\n\n## Reporting\n\nWe could report the model and the results like this.\n\n::: callout-tip\nWe fitted a Bayesian regression using the brms package [XXX] in R [XXX]. The outcome variable was height and we did not include any predictor, to estimate the overall mean and standard deviation of height.\n\nBased on the model results, there is a 95% probability that the mean is between 164 and 166 cm and that the standard deviation is between 7.6 and 9.3 cm.\n:::\n\n::: callout-note\n#### Next\n\n- [Introduction to regression models (Part II): include numeric predictors](intro-regression-predictors.qmd)\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}