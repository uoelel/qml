{
  "hash": "068612cc75c93af423e4147fd8a267cf",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Regression models: interactions between numeric predictors\"\ndescription: \"Add and interpret interactions of two numeric predictors\"\nauthor: [\"Elizabeth Pankratz\", \"Stefano Coretta\"]\ndate: 2024-10-02\nexecute: \n  freeze: auto\nformat: \n  html:\n    css: [webex.css]\n    include-after-body: [webex.js]\n    embed-resources: false\n---\n\n\n\n\n\n::: callout-warning\n## Prerequisites\n\n- [Regression models: interactions between categorical predictor](regression-interactions-catcat.qmd).\n\n- [Regression: binary outcome variables](regression-bernoulli.qmd).\n:::\n\n## Introduction\n\nHere you will learn how to include interactions between two numeric predictors and how to interpret the output. Relative to the other combination of predictor types, numeric-numeric interactions are more difficult to interpret: this is because a numeric predictor can take on many numeric values, and the numebr of combinations of values from two numeric predictors is even larger.\n\n## The data: Pankratz 2021\n\nLet's read the data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nsi <- read_csv(\"data/pankratz2021/si.csv\")\n```\n:::\n\n\n\nThe variables in this data frame that we'll refer to in this tutorial are:\n\n-   `weak_adj`: The weaker adjective on the tested scale (paired with the stronger adjective in `strong_adj`).\n-   `SI`: Whether or not a participant made a scalar inference for the pair of adjectives in `weak_adj` and `strong_adj` (`no_scalar` if no, `scalar` if yes).\n-   `freq`: How frequently the `weak_adj` co-occurred with a stronger adjective on the same scale in a large corpus.\n-   `semdist`: A measure of the semantic distance between `weak_adj` and `strong_adj`. A negative score indicates that the words are semantically closer; a positive score indicates that the words are semantically more distant (the units are arbitrary).\n\nBefore we leap into modelling, though, let's look in more detail at our predictors `freq` and `semdist`. A little bit of pre-processing is needed here, and the next sections will walk you through it.\n\n#### Recoding `SI`\n\nNow `SI` is coded with `no_scalar` for \"participant did not make a scalar inference\" and `scalar` for \"participant made a scalar inference\".\n\nLet's convert `SI` into a factor with levels in the order `c(\"no_scalar\", \"scalar\")` (note that this is the same order as the default alphabetical order, but it does not hurt to specify it ourselves).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsi <- si %>%\n  mutate(\n    SI = factor(SI, levels = c(\"no_scalar\", \"scalar\"))\n  )\n```\n:::\n\n\n\n#### Transforming and centring `freq`\n\nFrequencies notoriously yield an extremely skewed distribution, so it's common practice in corpus linguistics to log-transform them before including them in an analysis.\n\nHere's how the frequencies look right out of the box:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression-interactions-numnum_files/figure-html/freq-skew-plot-1.png){width=672}\n:::\n:::\n\n\n\nLog-transforming the frequencies helps to reduce the skewness. Use `mutate()` to take the log of `freq` and store the result in a new column called `logfreq`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsi <- si %>% \n  mutate(\n    logfreq = log(freq)\n  )\n```\n:::\n\n\n\nThis is how the distribution of logged frequency looks like:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression-interactions-numnum_files/figure-html/logfreq-plot-1.png){width=672}\n:::\n:::\n\n\n\nWhen we're interpreting the model estimates below, we'll be talking about the effect on scalar inferencing of **one unit change on the log frequency scale.** One unit change corresponds to moving from 0 to 1, from 1 to 2, 3 to 4, and so on...\n\n::: {.callout-important collapse=\"true\"}\n#### Extra: Unit changes and non-linear transformations\n\nWhen working with logged-transformed variables, you should keep in mind that a unit change on the logged scale does not corresponds to a unit change on the original scale.\n\nBecause the log transformation is **non-linear**, a unit change on the log frequency scale (e.g., going from 0 to 1, or going from 2 to 3) corresponds to different changes on the frequency scale, **depending on which unit change on the log scale we evaluate.**\n\nTo illustrate this:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression-interactions-numnum_files/figure-html/log-nonlinear-plot-1.png){width=672}\n:::\n:::\n\n\n\nThe orange bars show that a unit change between 1 and 2 on the log scale corresponds to a change of 4.7 on the frequency scale, while the grey bars show that a unit change between 2 and 3 on the log scale corresponds to a change of 12.7 on the frequency scale.\n\nWe won't be back-transforming log frequencies into plain old frequencies in this tutorial, but if you ever do, bear this in mind.\n\nA good rule of thumb is to compute and report the change in the original scale that is associated with **a unit change from the mean on the transformed scale to one unit above or below the mean.**\n:::\n\nWe're nearly done with `logfreq`---all that's left is to centre it. To centre a variable, we compute its mean and subtract that from every observation of the variable. The goal is to get a new version of variable that has a mean of zero.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsi <- si %>% \n  mutate(\n    logfreq_c = logfreq - mean(logfreq)\n  )\n```\n:::\n\n\n\nDon't take our word! If you take the mean of the centred variable, it will be 0.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nround(mean(si$logfreq_c))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\n\nThe `round()` function is included because, for technical computational reasons, the mean actually comes out as an *incredibly* small number near 0, rather than 0 precisely. But in practice we can consider this incredibly small number near 0 to be 0. If you want to know why it is not precisely 0, see [Floating Point Arithmetic](https://docs.python.org/3/tutorial/floatingpoint.html).\n\n`logfreq_c` is now ready to be used in our analyses. Next, you'll take care of semantic distance on your own, and then we'll move on to the modelling.\n\n#### Centring `semdist`\n\nUse `mutate()` to create a new, centred variable based on `semdist` that is called `semdist_c`. Verify that the mean of `semdist_c` is 0, and display `semdist_c` using a density plot with a rug, as above.\n\n\n\n::: {.cell}\n\n:::\n\n\n\n## Visualise the data\n\nCheck [Advanced plotting](plot-advanced.qmd#pankratz-2021) for a tutorial on how to plot the data.\n\n\n## Fitting the model\n\nHere's the mathematical specification of the model we'll fit:\n\n$$\n\\begin{aligned}\n\\text{SI} & \\sim Bernoulli(p) \\\\\nlogit(p) & = \\beta_0 + (\\beta_1 \\cdot logfreq\\_c) + (\\beta_2 \\cdot semdist\\_c) + (\\beta_3 \\cdot logfreq\\_c \\cdot semdist\\_c)\\\\\n\\end{aligned}\n$$\n\nWrite `brm()` code for this model, and name the variable you assign the model output to as `si_bm`.\n\n-   The model formula should specify that we are predicting `SI` as a function of `logfreq_c`, `semdist_c`, and their interaction.\n-   Use the appropriate model family.\n-   The data comes from `si`.\n-   Specify a seed for reproducibility.\n-   Specify a file path to save the model fit to a file.[^1]\n\n[^1]: **The first time you run a model with this argument, that's the version of the model that will be saved and loaded every time after.** Beware of this if you use the same `file` argument to run a model that's different from the final one you'll want to run!\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsi_bm <- brm(\n  ...\n)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\nYour model summary should look similar to this one.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(si_bm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: bernoulli \n  Links: mu = logit \nFormula: SI ~ logfreq_c + semdist_c + logfreq_c:semdist_c \n   Data: si (Number of observations: 2006) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept              -0.70      0.05    -0.80    -0.59 1.00     4178     2934\nlogfreq_c               0.53      0.05     0.43     0.63 1.00     3686     3123\nsemdist_c               0.08      0.01     0.06     0.11 1.00     3964     2798\nlogfreq_c:semdist_c     0.05      0.01     0.03     0.07 1.00     4117     3181\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\n### Interpreting the estimates\n\nBefore we move to interpreting the estimate of this model, let's just clarify something about numeric predictors and centred numeric predictors.\n\n::: {.callout-note appearance=\"minimal\"}\nWhen a predictor is categorical and any contrast coding is used, the intercept of the model indicates the mean outcome value at the reference level of that predictor, and the other coefficients estimate the difference in mean outcome value between the other levels of the predictor and the value at the reference. If using indexing, rather than contrast coding, as we are in this course, then you get a coefficient for each level of the categorical predictor which is the estimate of the mean for that level.\n\nWith **numeric predictors**, the **intercept indicates the mean outcome value when the numeric predictor is at 0**.\n:::\n\nThe numeric predictor gets one coefficient estimate: this is the **change in the outcome value when the numeric predictor goes from 0 to 1** (referred to as a **unit change**).\n\nNote that the same estimate applies when going from 1 to 2, 2 to 3, 4.5 to 5.5 and so on. In other words, the change in the outcome value applies across any unit change of the numeric predictor.\n\n::: callout-tip\n#### Numeric predictors\n\nThe estimated coefficient of a numeric predictor corresponds to the **change in the outcome value for each unit change** in the predictor.\n:::\n\nNow, when a continuous predictor is centred, `0` corresponds to whatever value is the mean of the non-centred predictor (`1` in the centred predictor corresponds to the non-centred predictor mean + 1 unit, `-3` corresponds to the mean - 3 units, whatever the units of the non-centred predictor are, and so on).\n\n::: {.callout-tip appearance=\"minimal\"}\nThis means that the **intercept of a model with centred numeric predictors corresponds to the outcome value when the numeric predictors are at their mean**.\n:::\n\nNow, let's interpret the model estimates.\n\n-   `Intercept`, $\\beta_0$: When **centred log frequency and centred semantic distance are 0**, there is a 95% probability that the log-odds of a scalar inference being made lie between --0.80 and --0.59.\n-   `logfreq_c`, $\\beta_1$: When **centred** **semantic distance is 0**, for a **unit change in (centred) log frequency**, the change in log-odds of making a scalar inference is between 0.42 and 0.63 at 95% probability.\n-   `semdist_c`, $\\beta_2$: When **centred** **log frequency is 0**, for a **unit change in (centred) semantic distance**, the change in the log-odds of making a scalar inference is between 0.06 and 0.11, at 95% confidence.\n\nSince semantic distance and log frequency are **centred**, 0 on the centred predictors corresponds to the mean of the non-centred versions of the predictors. So we can also say:\n\n-   `Intercept`, $\\beta_0$: When **log frequency and semantic distance are at their mean**, there is a 95% probability that the log-odds of a scalar inference being made lie between --0.80 and --0.59.\n-   `logfreq_c`, $\\beta_1$: When **semantic distance is at its mean**, for a **unit change in log frequency**, the change in log-odds of making a scalar inference is between 0.42 and 0.63 at 95% probability.\n-   `semdist_c`, $\\beta_2$: When **log frequency is at its mean**, for a **unit change in semantic distance**, the change in the log-odds of making a scalar inference is between 0.06 and 0.11, at 95% confidence.\n\n::: {.callout-note appearance=\"minimal\"}\nNote how now we say \"log frequency\" and \"semantic distance\", instead of \"centred log frequency\" and \"centred semantic distance\"\n:::\n\nIt is customary, when reporting results with centred predictors, to use the \"non-centred\" version of the report (so you would write e.g. \"when log frequency is at its mean\" rather than \"when centred log frequency is at 0\").\n\nAs for `logfreq_c:semdist_c`, $\\beta_3$: As usual, this coefficient has **two interpretations.**\n\nInterpreting the interaction between two numeric predictors:\n\n**(1) A unit change in (centred) log frequency is associated with a positive adjustment to the effect of (centred) semantic distance** between 0.03 and 0.08 log-odds, at 95% probability.\n\n-   In other words: As centred log frequency increases, the effect of semantic distance on the probability of a scalar inference being made increases as well.\n\n**(2) A unit change in (centred) semantic distance is associated with a positive adjustment to the effect of (centred) log frequency** between 0.03 and 0.08 log-odds at 95% confidence.\n\n-   In other words: As semantic distance increases, the effect of log frequency on the probability of a scalar inference being made increases as well.\n\nThe next section will walk you through how you can calculate conditional posterior probabilities when the model contains continuous predictors.\n\n### Conditional posterior probabilities\n\nIn previous weeks, to compute conditional posterior probabilities for categorical predictors, we would take the model equation, substitute the $\\beta$ coefficients with the estimates of the model and then calculate the posterior probabilities by substituting the predictor variables with 0 or 1s depending on the level we wanted to predict for.\n\nBack then, it was easy to know which values to set the predictors equal to. But what do we do now that we have a numeric predictor that can, in principle, take on any value?\n\nRather than looking at the full range of values that the numeric predictor can take on, **in practice we still choose just a few representative values to set the predictor to.**\n\nA common practice is to use the mean of the predictor and then the value that corresponds to 1 standard deviation above the mean and 1 standard deviation below the mean.\n\nIf you wish you can try and work out how to do the calculations yourself using the draws, but luckily there is a function from the brms package that simplifies things.\n\nThe function is `conditional_effects()` (aptly named!). It takes the model as the first argument and then a string specifying the effects to plot, here `\"logfreq_c:semdist_c\"` for \"`logfreq_c`, `semdist_c` and their interaction\".\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconditional_effects(si_bm, effects = \"logfreq_c:semdist_c\")\n```\n\n::: {.cell-output-display}\n![](regression-interactions-numnum_files/figure-html/condeff-1.png){width=672}\n:::\n:::\n\n\n\nYou will see that the plot has picked representative values for `semdist_c` (by default these are the mean, and mean $\\pm$ 1 standard deviation. You can calculate the mean and SD of `semdist_c` yourself to compare). Since we have listed `logfreq_c` first, the plot shows the entire range of values in `logfreq_c`.\n\n**The effect of log frequency gets stronger with increasing semantic distance.**\n\nWe can invert the predictors to see the effect of semantic distance at representative values of log frequency.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconditional_effects(si_bm, effects = \"semdist_c:logfreq_c\")\n```\n\n::: {.cell-output-display}\n![](regression-interactions-numnum_files/figure-html/condeff-2-1.png){width=672}\n:::\n:::\n\n\n\nHere we see that, equivalently, the effect of semantic distance increases with increasing log frequency.\n\n::: {.callout-note collapse=true}\n#### Using `conditional_effects()`\n\nPrior to this week, we've been calculating and plotting posterior probabilities ourselves using the draws obtained with `as_draws_df()`.\n\nThe `conditional_effects()` function is a great way of quickly plotting conditional posterior probabilities. If you feel like, go back to the models from previous weeks and try the function with those!\n\nThere is also a package, [bayesplot](http://mc-stan.org/bayesplot/), that has a lot of function that make plotting easier, so feel free to have a look at it if you wish.\n\nFinally, [tidybayes](http://mjskay.github.io/tidybayes/) can also be helpful to work with draws and plotting and it's worth looking.\n:::\n\n## Reporting\n\nNow, let's write a full report of model and results. Fill in the blanks yourself.\n\n> We fitted a Bayesian model to scalar inference (absent vs present), using a Bernoulli distribution. We included logged word frequency and semantic distance, and their interaction as predictors. Both predictors were centred.\n>\n> The model's results suggest that, when logged frequency and semantic distance are at their mean, the probability of a scalar inference being present is between ... and ...% ($\\beta$ = ..., SD = ...). For each unit increase of logged frequency, when semantic distance is at its mean, the probability increases by ... to ... log-odds ($\\beta$ = ..., SD = ...). For each unit increase of semantic distance, when logged frequency is at its mean, the probability increases by ... to ... log-odds ($\\beta$ = ..., SD = ...). The positive effect of logged word frequency increases with increasing semantic distance, by ... to ... log-odds ($\\beta$ = ..., SD = ...).\n\n::: {.callout-warning appearance=\"minimal\"}\nIn the report we are just reporting log-odds for the continuous predictors. This is because the change in percent points depends on where on the continuous predictor scale you are calculating the change in percent points.\n\nIn real research contexts you would also report percent points at representative values of the continuous predictor (or values that are relevant for the research question).\n\nTypical representative values are mean and mean $\\pm$ 1SD, as with the plots.\n\nThings get much more complicated when working with the interaction term and generally percent points are not reported in that context.\n\nEspecially with Bernoulli models that include continuous predictors, the best way to understand and present the model results is by plotting them!\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}