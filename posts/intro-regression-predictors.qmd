---
title: "Introduction to regression models (Part II): include numeric predictors"
description: "Learn how to include numeric predictors in your regression models"
author: "Stefano Coretta"
date: 2024-07-26
---

::: callout-warning
## Prerequisites

- (Introduction to regression models)[posts/intro-regression.qmd].
:::

## A straight line

A regression model is a statistical model that estimates the relationship between an outcome variable and one or more predictor variables (more on outcome/predictor below).

Regression models are based on the **formula of a straight line**.

$$
y = \alpha + \beta * x
$$

### Back to school

You might remember from school when you were asked to find the values of $y$ given certain values of $x$ and specific values of $\alpha$ and $\beta$.^[Depending on which country you did your schooling in, you might be used to slightly different notation, like $mx + c$. In $mx + c$, $m$ is $\beta$ and $c$ is $\alpha$.].

For example, you were given the following formula of a line:

$$
y = 3 + 2 * x
$$

and the values $x = (2, 4, 5, 8, 10, 23, 36)$. The homework was to calculate the values of $y$ and maybe plot them on a Cartesian coordinate space.

```{r}
#| label: homerwork

library(tidyverse)

line <- tibble(
  x = c(2, 4, 5, 8, 10, 23, 36),
  y = 3 + 2 * x
)

ggplot(line, aes(x, y)) +
  geom_point(size = 4) +
  geom_line(colour = "red") +
  labs(title = bquote(italic(y) == 3 + 2 * italic(x)))
```

Using the formula, we are able to find the values of $y$. Note that in $y = 3 + 2 * x$, $\alpha = 3$ and $\beta = 2$. Importantly, $\alpha$ is the value of $y$ when $x = 0$. $\alpha$ is commonly called the **intercept** of the line.

$$
\begin{align}
y & = 3 + 2 * x\\
& = 3 + 2 * 0\\
& = 3\\
\end{align}
$$

And $\beta$ is the number to add to 3 for each **unit increase of $x$**. $\beta$ is commonly called the **slope** of the line.

$$
\begin{align}
y & = 3 + 2 * x\\
& = 3 + 2 * 1 = 3 + 2 = 5\\
& = 3 + 2 * 2 = 3 + (2 + 2) = 7\\
& = 3 + 2 * 3 = 3 + (2 + 2 + 2) = 9\\
\end{align}
$$

The following plot should clarify.

```{r}
#| label: unit-increase
#| echo: false

line <- tibble(
  x = 0:3,
  y = 3 + 2 * x
)

ggplot(line, aes(x, y)) +
  geom_point(size = 4) +
  geom_line(colour = "red") +
  annotate("path", x = c(0, 0, 1), y = c(3, 5, 5), linetype = "dashed") +
  annotate("path", x = c(1, 1, 2), y = c(5, 7, 7), linetype = "dashed") +
  annotate("path", x = c(2, 2, 3), y = c(7, 9, 9), linetype = "dashed") +
  annotate("text", x = 0.25, y = 4.25, label = "+2") +
  annotate("text", x = 1.25, y = 6.25, label = "+2") +
  annotate("text", x = 2.25, y = 8.25, label = "+2") +
  scale_y_continuous(breaks = 0:15) +
  labs(title = bquote(italic(y) == 3 + 2 * italic(x)))
```

In the plot, the dashed line indicates the increase in $y$ for every unit increase of $x$ (i.e., every time $x$ increases by 1, $y$ increases by 2).

Now, in the context of research, you usually measure $x$ (the predictor variable) and $y$ (the outcome variable). Then you have to estimate $\alpha$ and $\beta$.

This is what regression models are for: given the measured $y$ and $x$, the model estimates $\alpha$ and $\beta$.

You can play around with [this web app](https://stefanocoretta.shinyapps.io/lines/) by changing the intercept and slope of the line. Note the alternative notation:

- The intercept is $\beta_0$ rather than $\alpha$.
- The slope is $\beta_1$ rather than $\beta$.

Let's use this alternative notation going forward (using $\beta$'s with subscript indexes will help understand the process of extracting information from regression models).

## Add some error...

Of course, measurements are noisy: they usually contain some error. Error can have many different causes, but we are usually not interested in learning about those causes. Rather, we just want our model to be able to deal with error.

Look at the following plot. The plot shows measurements of $x$ and $y$. The points corresponding to the observed measurements are almost on a straight line, but not quite. The vertical distance between the observed points and the expected straight line is the **residual error** (red lines in the plot).

```{r}
#| label: error
#| echo: false

set.seed(4321)
x <- 1:10
y <- (1 + 1.5 * x) + rnorm(10, 0, 2)

line <- tibble(
  x = x,
  y = y
)

m <- lm(y ~ x)
yhat <- m$fitted.values
diff <- y - yhat  
ggplot(line, aes(x, y)) +
  geom_segment(aes(x = x, xend = x, y = y, yend = yhat), colour = "red") +
  geom_point(size = 4) +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE) +
  scale_x_continuous(breaks = 1:10) +
  labs(title = bquote(italic(y) == beta[0] + beta[1] * italic(x) + epsilon))
```

The blue line in the plot is the expected line as estimated by a regression model (you'll learn how to run such model below), based on the $x$ and $y$ observations.

The formula of this regression model is the following:

$$
y = \beta_0 + \beta_1 * x + \epsilon
$$

where $\epsilon$ is the error. In other words, $y$ is the sum of $\beta_0$ and $\beta_1 * x$, plus some error.

Given that the error $\epsilon$ is assumed to come from a distribution defined as $Gaussian(0, \sigma)$, the formula can be rewritten by using a probability distribution (see (Introduction to regression models)[posts/intro-regression.qmd]).

$$
\begin{align}
y & \sim Gaussian(\mu, \sigma)\\
\mu & = \beta_0 + \beta_1 * x\\
\end{align}
$$

## Run a regression model

