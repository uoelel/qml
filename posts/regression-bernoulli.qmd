---
title: "Regression: binary outcome variables"
description: "Learn how to fit regression models with binary outcome variable using the Bernoulli/binomial distribution"
author: ["Stefano Coretta", "Elizabeth Pankratz"]
date: 2024-09-09
execute: 
  freeze: auto
draft: true
---

```{r}
#| label: setup

library(tidyverse)
theme_set(theme_light())
library(brms)
```


::: callout-warning
## Prerequisites

- [Regression: Indexing of categorical predictors](regression-indexing.qmd).
:::

## Binary outcomes and the Bernoulli family

**Binary outcome variables** are very common in linguistics. These are categorical variable that have **two levels**, e.g.: 

  - yes / no
  - grammatical / ungrammatical
  - Spanish / English
  - indirect object (*gave the girl the book*) / to-PP (*gave the book to the girl*)
  - correct / incorrect

So far you have been fitting regression models in which the outcome variable was numeric and continuous. However, a lot of studies use binary outcome variables and it thus important to learn how to deal with those. This is what this post is about.

When modelling binary outcomes, what the researcher is usually interested in is the probability of obtaining one of the two levels. For example, in a lexical decision task one might want to know the probability that real words were recognised as such (in other words, we are interested in accuracy: incorrect or correct response).

Let's say there is an 80% probability of responding correctly. So ($p()$ stands for "probability of"):

- $p(\text{correct}) = 0.8$
- $p(\text{incorrect}) = 1 - p(\text{correct}) = 0.2$

You see that if you know the probability of one level (correct) you automatically know the probability of the other level, since there are only two levels and the total probability has to sum to 1.

The distribution family for binary probabilities is the **Bernoulli family**. The Bernoulli family has only one parameter, $p$, which is the probability of obtaining one of the two levels (one generally picks which level).

With out lexical decision task example, we can write:

$$
\begin{align}
\text{resp}_{correct} & \sim Bernoulli(p)\\
p & = 0.8
\end{align}
$$

You can read it as:

> The probability of getting a correct response follows a Bernoulli distribution with $p$ = 0.8.

If you randomly sampled from $Bernoulli(0.8)$ you would get "correct" 80% of the times and "incorrect" 20% of the times.

Now, what we are trying to do when modelling binary outcome variables is to estimate the probability $p$ from the data. But there is a catch: probabilities are bounded between 0 and 1 and regression models don't work with bounded variables out of the box!

Bounded probabilities are transformed into an unbounded numeric variable. The following section explains how.

## Probability and log-odds

As we have just learnt probabilities are bounded between 0 and 1 but we need something that is not bounded because regression models don't work with bounded numeric variables.

This is where the **logit function** comes in: the logit function (from "*log*istic un*it*") is a mathematical function that transforms probabilities into log-odds.

The plot below shows the correspondence of probabilities (on the *y*-axis) and log-odds (on the *x*-axis), as marked by the black S-shaped line. You should remember that when $p = 0.5$ then the log-odds are 0. Since probabilities can't be smaller than 0 and greater than 1, the black line slopes in either direction and it approaches 0 and 1 on the *y*-axis without ever reaching them (in mathematical terms, it's an *asymptotic* line).

```{r}
#| label: p-log-odds
#| warning: false
dots <- tibble(
  p = seq(0.1, 0.9, by = 0.1),
  log_odds = qlogis(p)
)

p_log_odds <- tibble(
  p = seq(0, 1, by = 0.001),
  log_odds = qlogis(p)
) %>%
  ggplot(aes(log_odds, p)) +
  geom_hline(yintercept = 0.5, linetype = "dashed") +
  geom_hline(yintercept = 0, colour = "#8856a7", linewidth = 1) +
  geom_hline(yintercept = 1, colour = "#8856a7", linewidth = 1) +
  geom_vline(xintercept = 0, alpha = 0.5) +
  geom_line(linewidth = 2) +
  # geom_point(data = dots, size = 4) +
  geom_point(x = 0, y = 0.5, colour = "#8856a7", size = 4) +
  annotate("text", x = -4, y = 0.8, label = "logit(p) = log-odds") +
  scale_x_continuous(breaks = seq(-6, 6, by = 1), minor_breaks = NULL, limits = c(-6, 6)) +
  scale_y_continuous(breaks = seq(0, 1, by = 0.1), minor_breaks = NULL) +
  labs(
    title = "Correspondence between log-odds and probabilities",
    x = "Log-odds",
    y = "Probability"
  )
p_log_odds
```

When you fit a regression model with a binary outcome and a Bernoulli family.

## Fitting a Bernoulli model

$$
\begin{align}
\text{Num_Preds} & \sim Bernoulli(p) \\
logit(p) & = \beta_0 + \beta_1 \cdot \text{Stim} \\
\beta_0 & \sim Gaussian(\mu_0, \sigma_0) \\
\beta_1 & \sim Gaussian(\mu_1, \sigma_1)
\end{align}
$$

```{r}
verb_org <- read_csv("data/brentari2024/verb_org.csv") |> 
  mutate(
    Num_Predicates = factor(Num_Predicates, levels = c("single", "multiple"))
  )
```

```{r}
verb_org |> 
  ggplot(aes(Group, fill = Num_Predicates)) +
  geom_bar(position = "fill")
```

```{r}
mvp_bm <- brm(
  Num_Predicates ~ 0 + Group,
  family = bernoulli,
  data = verb_org,
  cores = 4,
  seed = 1329,
  file = "data/cache/regression-bernoulli_mvp_bm"
)
```

```{r}
summary(mvp_bm, prob = 0.8)
```


```{r}
conditional_effects(mvp_bm)
```
