---
title: "QML - Week 4"
subtitle: "Bayesian inference"
author: "Stefano Coretta"
format: revealjs
bibliography: references.bib
---

## Probabilities

```{r}
#| label: setup
#| include: false

library(tidyverse)
theme_set(theme_light())
library(brms)
library(bayesplot)
```

::: callout-note
**Probability**

-   Probability of an event occurring.

-   Probabilities can only be **between 0 and 1**.

    -   ‚õîÔ∏è 0 means **impossible**.
    -   ü§∑ 0.5 means **it can happen but it can also not happen**.
    -   ‚úÖ 1 means **certain**.
:::

## Probabilities

![](/img/probabilities.png){fig-align="center"}

## 

::: callout-note
### **Probability**

-   Probability of an event occurring: 0 to 100% probability.

-   **Probability of a statistical variable being some numeric value**: a bit more complicated...
:::

. . .

::: {.callout-tip appearance="simple"}
**We need probability distributions!**
:::

## Distribution of probability

<br>

![](/img/grubabilities.png){fig-align="center"}

## Probability distributions

::: {.callout-note appearance="simple"}
A **probability distribution** describes *how the probabilities are distributed over the values* that a variable can take on.
:::

. . .

::: callout-note
### Two types of probability distributions

-   Discrete probability distributions.

-   Continuous probability distributions.
:::

. . .

::: {.callout-tip appearance="simple"}
You learned about discrete and continuous variables in Week 2!

**Discrete variables (numeric or categorical) follow discrete probability distributions and continuous variables follow continuous probability distributions.**
:::

## Calculating probability distributions

::: {.callout-note appearance="simple"}
You will never need to calculate probability distributions by hand, but it is useful to know about the two mathematical functions that are used for that purpose.

-   The **Probability Mass Function** (PMF) for discrete probability distributions.

-   The **Probability Density Function** (PDF) for continuous probability distributions.
:::

## Probability Mass Function

![](/img/dice.png){fig-align="center"}

## Probability Density Function

```{r}
#| label: pdf

alpha <- 4
beta <- 1.5
beta_df <- tibble(
  x = seq(0, 1, length.out = 100),
  density = dbeta(x, alpha, beta)
)

ggplot(beta_df, aes(x = x, y = density)) +
  geom_line(color = "darkorange", linewidth = 1.2) +
  labs(x = "Proportion", y = "Density")
```

## Parameters

::: {.callout-note appearance="simple"}
- **Probability distributions can be summarised with a set of parameters.**
  
- Different types of probability distributions have a different number of parameters and different parameters.
:::

. . .

::: callout-tip
### Gaussian distribution

The **Gaussian probability distribution** is a continuous probability distribution and it has two parameters:

- The mean $\mu$.
- The standard deviation (SD) $\sigma$.

Go to [Seeing Theory](https://seeing-theory.brown.edu/probability-distributions/index.html#section2) (by Daniel Kunin).
:::

::: notes
Seeing Theory was created by Daniel Kunin while an undergraduate at Brown University. The goal of this website is to make statistics more accessible through interactive visualizations (designed using Mike Bostock‚Äôs JavaScript library D3.js).

<https://seeing-theory.brown.edu/index.html#3rdPage>
:::

## Simulate Gaussian data

::: callout-note
### Human height

- Simulate human height as a Gaussian variable.

- `rnorm()`: number of observations, mean, SD.

:::

. . .

```{r}
#| label: height
#| echo: true

set.seed(812)
height <- rnorm(200, 165, 8) |> round()

height[1:10]
```

## Kernel Density Estimation

::: {.callout-note appearance="simple"}

- PDF constructs the density curve of theoretical distributions.

- **Kernel Density Estimation** (KDE) constructs the density curve of empirical data (observations).

:::

. . .

```{r}
#| label: height-plot
#| fig-align: center

ggplot() +
  aes(height) +
  geom_density(fill = "darkgreen", alpha = 0.2) +
  geom_rug(alpha = 0.25)
```

## Statistical modelling

::: {.callout-note appearance="simple"}

- When you simulate data, you know the population mean and SD.

- In research, you don't. You just have observations.

:::

. . .

::: {.callout-tip}
### Statistical modelling

- Statistical modelling allows you to estimate the mean and SD of the population from the sample. This is statistical inference.

- **Bayesian Gaussian models** do exactly that: estimate mean and SD.

:::

## Gaussian model of height


$$
\begin{align}
h & \sim Gaussian(\mu, \sigma)\\
\mu & = ...\\
\sigma & = ...
\end{align}
$$

## Gaussian model of height

$$
\begin{align}
h & \sim Gaussian(\mu, \sigma)\\
\mu & = P_\mu\\
\sigma & = P_\sigma
\end{align}
$$

::: {.callout-tip appearance="simple"}

- $P$ is a **posterior probability distribution**.

:::

## Gaussian model of height: posteriors

```{r}
#| label: h-bm

h_tib <- tibble(h = height)

h_bm <- brm(
  h ~ 1,
  family = gaussian,
  data = h_tib,
  file = "cache/h_bm"
)
```

```{r}
#| label: h-bm-plot

mcmc_dens(h_bm, pars = c("b_Intercept", "sigma"),
  facet_args = list(
    labeller = labeller(
      .default = as_labeller(c(b_Intercept = "mu", sigma = "sigma"))
    )
  )
)
```

## A bit harder: N = 5

::: callout-note
### Sample size

- Sample size matters: the lower the sample size, the higher the uncertainty.

- Let's try with just 5 observations (out of 200).
:::

## A bit harder: N = 5

```{r}
#| label: h-bm-small

set.seed(4256)
h_tib <- tibble(h = sample(height, 5))

h_bm_small <- brm(
  h ~ 1,
  family = gaussian,
  data = h_tib,
  file = "cache/h_bm_small"
)
```


```{r}
#| label: h-bm-small-plot

mcmc_dens(h_bm_small, pars = c("b_Intercept", "sigma"),
  facet_args = list(
    labeller = labeller(
      .default = as_labeller(c(b_Intercept = "mu", sigma = "sigma"))
    )
  )
)
```

## Summary

::: {.callout-note appearance="simple"}

- We discovered probabilities and probability distributions.

- Theoretical probability distributions can be described with PMF/PDF and parameters.

- KDE is used for empirical distributions.

- (Bayesian) Gaussian models infer the population probability distribution from the data, by estimating posterior probability distributions of mean and SD.

- Sample size affects uncertainty.

:::
