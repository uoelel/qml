[
  {
    "objectID": "tutorials/tutorial-w09.html",
    "href": "tutorials/tutorial-w09.html",
    "title": "QML tutorial – Week 9",
    "section": "",
    "text": "This tutorial will give you some practice working with continuous predictors. You’ll begin by fitting a model with a binary outcome and one continuous predictor. Then you’ll fit a model with the same binary outcome, but now predicted by an interaction between two continuous predictors.\nOK, let’s dive in! Set up the Rmd file you’ll use for this tutorial:"
  },
  {
    "objectID": "tutorials/tutorial-w09.html#scalar-inferences-and-scalar-diversity",
    "href": "tutorials/tutorial-w09.html#scalar-inferences-and-scalar-diversity",
    "title": "QML tutorial – Week 9",
    "section": "1 Scalar inferences and scalar diversity",
    "text": "1 Scalar inferences and scalar diversity\nThe data we’ll be using comes from Experiment 2 in The role of relevance for scalar diversity (Pankratz and van Tiel 2021, DOI: https://www.doi.org/10.1017/langcog.2021.13).\nThis study looks at when people make so-called scalar inferences. A scalar inference happens when you encounter a sentence like “Fatima ate some of the cookies”, and you infer that Fatima didn’t eat all of the cookies. In particular, this study looks at the phenomenon of scalar diversity: the observation that scalar inferences are made at different rates for different words. For example, for “The food was cheap” (where “cheap” is a weaker scalar word), people do often infer that the food wasn’t free (a stronger word on the same scale of price). But, for “The village was pretty”, people don’t often infer that the village wasn’t beautiful. In this tutorial, we’ll look at the influence of a couple different predictors on whether or not people make scalar inferences.\n\n1.1 Read and process the data\nYou can get the data for this tutorial by right-clicking and downloading the following link: si.csv.\nUse read_csv() to read in the CSV and assign it to a variable called si (for scalar inference).\n\nsi &lt;- ...\n\n\nsi\n\n\n\n  \n\n\n\nThe variables in this data frame that we’ll refer to in this tutorial are:\n\nweak_adj: The weaker adjective on the tested scale (paired with the stronger adjective in strong_adj).\nSI: Whether or not a participant made a scalar inference for the pair of adjectives in weak_adj and strong_adj (0 if no, 1 if yes).\nfreq: How frequently the weak_adj co-occurred with a stronger adjective on the same scale in a large corpus.\nsemdist: A measure of the semantic distance between weak_adj and strong_adj. A negative score indicates that the words are semantically closer; a positive score indicates that the words are semantically more distant (the units are arbitrary).\n\nThe first model we’ll fit will only look at SI as a function of frequency. The second will add an interaction between frequency and semantic distance.\nBefore we leap into modelling, though, let’s look in more detail at our predictors freq and semdist. A little bit of pre-processing is needed here, and the next sections will walk you through it.\n\n1.1.1 Recoding SI\nNow SI is coded with 0 for “participant did not make a scalar inference” and 1 for “participant made a scalar inference”.\nLet’s recode SI so that 0 becomes no_scalar and 1 becomes scalar.\n\nsi &lt;- si %&gt;%\n  mutate(\n    SI = ...\n  )\n\n\n\n1.1.2 Transforming and centring freq\nFrequencies notoriously yield an extremely skewed distribution, so it’s common practice in corpus linguistics to log-transform them before including them in an analysis.\nHere’s how the frequencies look right out of the box:\n\nsi %&gt;% \n  ggplot(aes(x = freq)) +\n  geom_density(fill = 'grey', alpha = 0.5) +\n  geom_rug()\n\n\n\n\nLog-transforming the frequencies helps to reduce the skewness. Use mutate() to take the log of freq and store the result in a new column called logfreq:\n\nsi &lt;- si %&gt;% \n  mutate(\n    ...\n  )\n\nThe result should look like this:\n\nsi %&gt;% \n  ggplot(aes(x = logfreq)) +\n  geom_density(fill = 'grey', alpha = 0.5) +\n  geom_rug()\n\n\n\n\nWhen we’re interpreting the model estimates below, we’ll be talking about the effect on scalar inferencing of one unit change on the log frequency scale. One unit change corresponds to moving from, e.g., 3 to 4 on this plot, or from 2 to 3, and so on…\n\n\n\n\n\n\nExtra: Unit changes and non-linear transformations\n\n\n\n\n\nWe’ve been talking a lot in recent weeks about transforming and back-transforming, so you might be asking yourself: what do unit changes on the log scale equate to on the original frequency scale?\nBecause the log transformation is non-linear, a unit change on the log frequency scale (e.g., moving from 1 to 2, or moving from 2 to 3) corresponds to different changes on the frequency scale, depending on which unit change on the log scale we evaluate.\nTo illustrate this:\n\n\n\n\n\nThe orange bars show that a unit change between 1 and 2 on the log scale corresponds to a change of 4.7 on the frequency scale, while the grey bars show that a unit change between 2 and 3 on the log scale corresponds to a change of 12.7 on the frequency scale.\nWe won’t be back-transforming log frequencies into plain old frequencies in this tutorial, but if you ever do, bear this in mind.\nA good rule of thumb is to compute and report the change in the original scale that is associated with a unit change from the mean on the transformed scale to one unit above or below the mean.\n\n\n\nWe’re nearly done with logfreq—all that’s left is to centre it. As we saw in the lecture, to centre a variable, we compute its mean and subtract that from every observation of the variable. The goal is to get a new version of variable that has a mean of zero.\n\nsi &lt;- si %&gt;% \n  mutate(\n    logfreq_c = logfreq - mean(logfreq)\n  )\n\nRun the following to verify that the mean is zero:\n\nround(mean(si$logfreq_c))\n\n[1] 0\n\n\nThe round() function is included because, for technical computational reasons, the mean actually comes out as an incredibly small number near 0, rather than 0 precisely. But in practice we can consider this incredibly small number near 0 to be 0 in practice. If you want to know why it is not precisely 0, see Floating Point Arithmetic.\nlogfreq_c is now ready to be used in our analyses. Next, you’ll take care of semantic distance on your own, and then we’ll move on to the modelling.\n\n\n1.1.3 Centring semdist\nUse mutate() to create a new, centred variable based on semdist that is called semdist_c. Verify that the mean of semdist_c is 0, and display semdist_c using a density plot with a rug, as above."
  },
  {
    "objectID": "tutorials/tutorial-w09.html#visualise-the-data",
    "href": "tutorials/tutorial-w09.html#visualise-the-data",
    "title": "QML tutorial – Week 9",
    "section": "2 Visualise the data",
    "text": "2 Visualise the data\nThe model we’ll fit in this tutorial will be predicting SI as a function of logfreq_c, semdist_c and the interaction between the two. Before fitting the model, let’s take a moment to plot the proportion of scalar inferences as a function of centred log frequency and as a function of semantic distance.\n\n2.1 Visualising scalar inferencing and centred log frequency\nThe variable SI is binary: no_scalar if a scalar inference was not made, scalar if it was. We like to visualise binary outcomes as proportions on the y-axis and centred log frequency on the x-axis.\nThis type of plot (proportion ~ continuous) requires a small trick.\nSI is categorical, but we want to show the proportion of scalar vs no_scalar across values of centred log frequency. We can achieve this by temporarily converting SI to numeric and subtract 1, and then using geom_smooth() with a binomial family, like so:\n\nsi %&gt;%\n  mutate(SI = as.numeric(SI) - 1) %&gt;%\n  ggplot(aes(logfreq_c, SI)) +\n  geom_point() +\n  geom_smooth(method = \"glm\", method.args = list(family = \"binomial\"))\n\nWarning: Computation failed in `stat_smooth()`\nCaused by error:\n! y values must be 0 &lt;= y &lt;= 1\n\n\n\n\n\nThe function as.numeric() works with factors by converting each level into a number: try as.numeric(si$SI) to see what it does. Then, we subtract 1 so that 1 becomes 0 and 2 becomes 1.\nThe plot suggests that there is a somewhat positive relationship between centred log frequency and probability of the participant making a scalar implication. In other words, the higher the log frequency, the higher the probability of a scalar implication.\n\n\n\n\n\n\nExtra: Adding text geometries\n\n\n\n\n\nLet’s add geom_text() to show the corresponding weak_adj on the plot.\nTo achieve this, we need to calculate the proportion of SI ourselves, to use as the y-axis. See the following code.\n\nsi %&gt;%\n  group_by(weak_adj) %&gt;%\n  summarise(\n    prop_si = mean(as.numeric(SI) - 1),\n    logfreq_c = mean(logfreq_c)\n  ) %&gt;%\n  ggplot(aes(x = logfreq_c, y = prop_si)) +\n  geom_text(aes(label = weak_adj)) +\n  labs(\n    x = 'Log co-occurrence frequency (centred)',\n    y = 'Proportion of scalar inferences'\n  ) +\n  ylim(0, 1)\n\nWarning: Removed 50 rows containing missing values (`geom_text()`).\n\n\n\n\n\nNote that we only created prop_si for visualisation! We won’t use this variable in our modelling; instead, we’ll be using the 0/1-coded variable SI.\n\n\n\n\n\n2.2 Visualising scalar inference and semantic distance\nNow it’s your turn! Reproduce the plot above this time using semdist_c instead of logfreq_c.\n\n\n\n\n\n\nExtra: Frequency and semantic distance together\n\n\n\n\n\n…"
  },
  {
    "objectID": "tutorials/tutorial-w09.html#fitting-the-model",
    "href": "tutorials/tutorial-w09.html#fitting-the-model",
    "title": "QML tutorial – Week 9",
    "section": "3 Fitting the model",
    "text": "3 Fitting the model\nHere’s the mathematical specification of the model we’ll fit:\n\\[\n\\begin{aligned}\n\\text{SI} & \\sim Bernoulli(p) \\\\\nlogit(p) & = \\beta_0 + (\\beta_1 \\cdot logfreq\\_c) + (\\beta_2 \\cdot semdist\\_c) + (\\beta_3 \\cdot logfreq\\_c \\cdot semdist\\_c)\\\\\n\\beta_0 & \\sim Gaussian(\\mu_0, \\sigma_0) \\\\\n\\beta_1 & \\sim Gaussian(\\mu_1, \\sigma_1) \\\\\n\\beta_2 & \\sim Gaussian(\\mu_2, \\sigma_2) \\\\\n\\beta_3 & \\sim Gaussian(\\mu_3, \\sigma_3) \\\\\n\\end{aligned}\n\\]\nWrite brm() code for this model, and call it si_freq_dist_bm.\n\nThe model formula should specify that we are predicting SI as a function of logfreq_c, semdist_c, and their interaction.\nUse the appropriate model family.\nThe data comes from si.\nThe backend is cmdstanr.\nSave the model to a file.1 ]\n\n\nsi_freq_dist_bm &lt;- brm(\n  ...\n)\n\nYour model summary should look similar to this one.\n\nsummary(si_freq_dist_bm)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: SI ~ logfreq_c + semdist_c + logfreq_c:semdist_c \n   Data: si (Number of observations: 2006) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept              -0.70      0.05    -0.80    -0.59 1.00     4208     3179\nlogfreq_c               0.53      0.05     0.42     0.63 1.00     3469     2569\nsemdist_c               0.08      0.01     0.06     0.11 1.00     3893     3330\nlogfreq_c:semdist_c     0.05      0.01     0.03     0.08 1.00     3776     2734\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n3.1 Interpreting the estimates\nBefore we move to interpreting the estimate of this model, let’s just clarify something about numeric predictors and centred numeric predictors.\nWhen a predictor is categorical, the intercept of the model indicates the mean outcome value at the reference level of that predictor, and the other coefficients estimate the difference in mean outcome value between the other levels of the predictor and the value at the reference.\nWith numeric predictors, the intercept indicates the mean outcome value when the numeric predictor is at 0.\nThe numeric predictor gets one coefficient estimate: this is the change in mean outcome value when the numeric predictor goes from 0 to 1 (referred to as a unit increase).\nNote that the same estimate applies when going from 1 to 2, 2 to 3, 4.5 to 5.5 and so on. In other words, the change in mean outcome value applies across any unit increase of the numeric predictor.\n\n\n\n\n\n\nNumeric predictors\n\n\n\nThe estimated coefficient of a numeric predictor corresponds to the change in the outcome mean for each unit increase in the predictor.\n\n\nNow, when a continuous predictor is centred, 0 corresponds to whatever value is the mean of the non-centred predictor (1 in the centred predictor corresponds to the non-centred predictor mean + 1, -3 corresponds to the mean -3, and so on).\nThis means that the intercept of a model with centred numeric predictors corresponds to the mean outcome value when the numeric predictors are at their mean.\nLet’s interpret the model estimates.\n\nIntercept, \\(\\beta_0\\): When both log frequency and semantic distance are at their mean, the mean log-odds of making a scalar inference is –0.70, and there is a 95% probability that the value lies between –0.80 and –0.59 log-odds.\nlogfreq_c, \\(\\beta_1\\): When semantic distance is at its mean, for a unit increase in log frequency, the change in the mean log-odds of making a scalar inference is 0.53 log-odds. There is a 95% probability that the true change lies between 0.42 and 0.63 log-odds.\nsemdist_c, \\(\\beta_2\\): When log frequency is at its mean, for a unit increase in semantic distance, the change in the mean log-odds of making a scalar inference is 0.08 log-odds. There is a 95% probability that the true change lies between 0.06 and 0.11 log-odds.\nlogfreq_c:semdist_c, \\(\\beta_3\\): As usual, this coefficient has two interpretations.\n\nInterpreting the interaction between two numeric predictors:\n(1) A unit increase in log frequency is associated with a positive adjustment of 0.05 log-odds to the effect of semantic distance, and there is a 95% probability that the true adjustment is between 0.03 and 0.08 log-odds.\n\nIn other words: As log frequency increases, the effect of semantic distance (i.e., the difference between semantically closer and semantically more distant words) increases as well.\n\n(2) A unit change in semantic distance is associated with a positive adjustment of 0.05 log-odds to the effect of log frequency, and there is a 95% probability that the true adjustment is between 0.03 and 0.08 log-odds.\n\nIn other words: As semantic distance increases, the effect of log frequency (i.e., the difference between lower- and higher-frequency words) increases as well.\n\nThe next section will walk you through how this computation works with a continuous predictor.\n\n\n3.2 Conditional posterior probabilities in log-odds\nIn previous weeks, to compute conditional posterior probabilities for categorical predictors, we would take the model equation, substitute the \\(\\beta\\) coefficients with the estimates of the model and then calculate the posterior probabilities by substituting the predictor variables with 0 or 1s depending on the level we wanted to predict for.\nBack then, it was easy to know which values to set the predictors equal to. But what do we do now that we have a numeric predictor that can, in principle, take on any value?\nRather than looking at the full range of values that the numeric predictor can take on, in practice we still choose just a few values to set the predictor to.\nIn fact, we can use 0s and 1s again, as long as we bear in mind their new interpretation, since here we centred the numeric predictors: 0 in the centred predictor corresponds to the mean of the corresponding non-centred predictor, 1 is one unit above the mean and -1 is one unit below.\nLet’s calculate the posterior probabilities by using 0 and 1 for logfreq_c and semdist_c. Here’s the maths:\n\\[\n\\begin{aligned}\n\\text{log freq. = 0, sem. dist. = 0:}     && \\beta_0 &+ (\\beta_1 \\cdot 0) + (\\beta_2 \\cdot 0) + (\\beta_3 \\cdot 0) &&= \\beta_0  &\\\\\n\\text{log freq. = 0, sem. dist. = 1:}    && \\beta_0 &+ (\\beta_1 \\cdot 0) + (\\beta_2 \\cdot 1) + (\\beta_3 \\cdot 0) &&= \\beta_0 + \\beta_2 &\\\\\n\\text{log freq. = 1, sem. dist. = 0:}   && \\beta_0 &+ (\\beta_1 \\cdot 1) + (\\beta_2 \\cdot 0) + (\\beta_3 \\cdot 0) &&= \\beta_0 + \\beta_1 &\\\\\n\\text{log freq. = 1, sem. dist. = 1:}  && \\beta_0 &+ (\\beta_1 \\cdot 1) + (\\beta_2 \\cdot 1) + (\\beta_3 \\cdot 1) &&= \\beta_0 + \\beta_1 + \\beta_2 + \\beta_3 &\\\\\n\\end{aligned}\n\\]\nNow, try to follow what this code is doing (and revisit Tutorial 8 if you’d like a refresher).\n\nsi_freq_dist_draws &lt;- as_draws_df(si_freq_dist_bm)\n\nsi_freq_dist_draws_cond &lt;- si_freq_dist_draws %&gt;% \n  # Recall our trick for combining the values of multiple predictors \n  # in one column name, separated by \"_\"\n  mutate(\n    '0_0' = b_Intercept,                # beta_0\n    '0_1' = b_Intercept + b_semdist_c,  # beta_0 + beta_2\n    '1_0' = b_Intercept + b_logfreq_c,  # beta_0 + beta_1\n    '1_1' = b_Intercept + b_logfreq_c + b_semdist_c + `b_logfreq_c:semdist_c`\n    # ^ beta_0 + beta_1 + beta_2 + beta_3\n  ) %&gt;% \n  select('0_0':'1_1') %&gt;% \n  pivot_longer('0_0':'1_1', names_to = 'predvals', values_to = 'sampled_logodds') %&gt;% \n  separate(predvals, into = c('logfreq_c', 'semdist_c'), sep = '_')\n\nCheck si_freq_dist_draws_cond.\n\n\n3.3 Conditional posterior probabilities as probabilities\nTheminimum that you should always do is to report and interpret the model’s coefficient estimates and 95% CrIs in the scale in which the model was fit (here, log-odds).\nHowever, you should always also compute the conditional posterior probabilities in the original scale (here, it is probability of the participant having made a scalar inference) . Once you have calculated these, you are able to report the mean probability and corresponding 95% CrIs of making a scalar inference for different values of logfreq_c and semdist_c and you’ll be able to create plots that nicely visualise the model’s estimates.\nAt this point, there are enough rows in this table that reporting these means and 95% CrIs in prose might get a little unwieldy for you to write, not to mention difficult for the reader to follow. An alternative option is to display the probability space estimates in a table, and/or as a density plot as usual.\nIn the following code, we are transforming the log-odds posterior probabilities into probabilities, using the same method from previous tutorials.\n\nsi_freq_dist_draws_cond %&gt;% \n  group_by(logfreq_c, semdist_c) %&gt;% \n  summarise(\n    # Summarise in log-odds space\n    logodds_mean = mean(sampled_logodds),\n    logodds_95_lo = round(quantile2(sampled_logodds, probs = 0.025), 2),\n    logodds_95_hi = round(quantile2(sampled_logodds, probs = 0.975), 2),\n    # Transform into probabilities\n    p_mean   = round(plogis(logodds_mean), 2),\n    p_q95_lo = round(plogis(logodds_95_lo), 2),\n    p_q95_hi = round(plogis(logodds_95_hi), 2)\n  )\n\n`summarise()` has grouped output by 'logfreq_c'. You can override using the\n`.groups` argument.\n\n\n\n\n  \n\n\n\n\n\n3.4 Visualising the conditional posteriors\n\nsi_freq_dist_draws_cond &lt;- si_freq_dist_draws_cond %&gt;% \n  mutate(\n    sampled_probs = plogis(sampled_logodds)\n  )\n\nsi_freq_dist_draws_cond %&gt;% \n  ggplot(aes(x = sampled_probs, fill = logfreq_c)) +\n  geom_density(alpha = 0.5) +\n  facet_wrap(~ semdist_c, nrow = 2, labeller = labeller(semdist_c = label_both)) +\n  labs(x = 'Probability of making a scalar inference',\n       y = 'Probability density',\n       title = 'Conditional posterior probability distributions\\nof the probability of making a scalar inference',\n       fill = 'Log freq.')\n\n\n\n\nThe interaction is quite small, so we don’t see a massive difference of differences here between semdist_c = 0 and semdist_c = 1. If you want to learn how to incorporate other values of semdist_c beyond 0 and 1 that will illustrate the interaction more clearly, have a look at the box below. Otherwise, you’ve made it to the end—congratulations!\n\n\n\n\n\n\nExtra: Using different values of semdist_c\n\n\n\n\n\nBecause the interaction is so small, we can see it more clearly if we visualise more extreme values of semdist_c. Here, we’ll use –5 (five units below the mean), 0 (the mean), and 5 (five units above the mean).\nI chose those values because they cover a fair amount of semdist_c’s range. In your own practice, you can choose other values based on what seems representative of your predictor.\nFor now, we’ll keep evaluating log frequency at 0 and 1 (though in principle, you could use other values for that predictor too).\nThe linear expression that we’re evaluating for different predictor values looks like this:\n\\[\n\\beta_0 + (\\beta_1 \\cdot logfreq\\_c) + (\\beta_2 \\cdot semdist\\_c) + (\\beta_3 \\cdot logfreq\\_c \\cdot semdist\\_c)\n\\]\nSubstituting in every combination of our chosen predictor values gives us the following:\n\\[\n\\begin{aligned}\n\\text{log freq. = 0}, ~& \\text{sem. dist. = –5:}& \\beta_0 &+ (\\beta_1 \\cdot 0) + (\\beta_2 \\cdot -5) + (\\beta_3 \\cdot 0) &&= \\beta_0 - 5\\beta_2 &\\\\\n\\text{log freq. = 0}, ~& \\text{sem. dist. = 0:} & \\beta_0 &+ (\\beta_1 \\cdot 0) + (\\beta_2 \\cdot 0)  + (\\beta_3 \\cdot 0) &&= \\beta_0  &\\\\\n\\text{log freq. = 0}, ~& \\text{sem. dist. = 5:} & \\beta_0 &+ (\\beta_1 \\cdot 0) + (\\beta_2 \\cdot 5)  + (\\beta_3 \\cdot 0) &&= \\beta_0 + 5\\beta_2 &\\\\\n\\text{log freq. = 1}, ~& \\text{sem. dist. = –5:}& \\beta_0 &+ (\\beta_1 \\cdot 1) + (\\beta_2 \\cdot -5) + (\\beta_3 \\cdot -5) &&= \\beta_0 + \\beta_1 - 5\\beta_2 - 5\\beta_3 &\\\\\n\\text{log freq. = 1}, ~& \\text{sem. dist. = 0:} & \\beta_0 &+ (\\beta_1 \\cdot 1) + (\\beta_2 \\cdot 0)  + (\\beta_3 \\cdot 0) &&= \\beta_0 + \\beta_1 &\\\\\n\\text{log freq. = 1}, ~& \\text{sem. dist. = 5:} & \\beta_0 &+ (\\beta_1 \\cdot 1) + (\\beta_2 \\cdot 5)  + (\\beta_3 \\cdot 5) &&= \\beta_0 + \\beta_1 + 5\\beta_2 + 5\\beta_3 &\\\\\n\\end{aligned}\n\\]\nAs always, every \\(\\beta\\) above corresponds to a column in the posterior draws data frame. So, this working-out tells us how we need to modify and combine those columns in order to get the conditional posterior probability distributions for each combination of values. Here’s how it looks in R code (note the multiplication by five!):\n\n# Get the conditional posterior probability distributions \n# in log-odds space.\nsi_freq_dist_draws_cond_5 &lt;- si_freq_dist_draws %&gt;% \n  mutate(\n    '0_-5' = b_Intercept - 5*b_semdist_c,\n    '0_0'  = b_Intercept,\n    '0_5'  = b_Intercept + 5*b_semdist_c,\n    '1_-5' = b_Intercept + b_logfreq_c - 5*b_semdist_c - 5*`b_logfreq_c:semdist_c`,\n    '1_0'  = b_Intercept + b_logfreq_c,\n    '1_5'  = b_Intercept + b_logfreq_c + 5*b_semdist_c + 5*`b_logfreq_c:semdist_c`\n  ) %&gt;% \n  select('0_-5':'1_5') %&gt;% \n  pivot_longer('0_-5':'1_5', names_to = 'predvals', values_to = 'sampled_logodds') %&gt;% \n  separate(predvals, into = c('logfreq_c', 'semdist_c'), sep = '_')\n\n# Add a column that back-transforms samples to probability space.\nsi_freq_dist_draws_cond_5 &lt;- si_freq_dist_draws_cond_5 %&gt;% \n  mutate(\n    sampled_probs = plogis(sampled_logodds)\n  )\n\n# Make the density plot.\nsi_freq_dist_draws_cond_5 %&gt;% \n  ggplot(aes(x = sampled_probs, fill = logfreq_c)) +\n  geom_density(alpha = 0.5) +\n  facet_wrap(~ semdist_c, nrow = 3, labeller = labeller(semdist_c = label_both)) +\n  labs(x = 'Probability of making a scalar inference',\n       y = 'Probability density',\n       title = 'Conditional posterior probability distributions\\nof the probability of making a scalar inference',\n       fill = 'Log freq.')\n\n\n\n\nNow we can really see the difference between differences: the effect of log frequency increases as semantic distance increases."
  },
  {
    "objectID": "tutorials/tutorial-w09.html#summary",
    "href": "tutorials/tutorial-w09.html#summary",
    "title": "QML tutorial – Week 9",
    "section": "4 Summary",
    "text": "4 Summary\n\n\n\n\n\n\nData processing\n\nWhen using frequencies from a corpus as a predictor, it is common to log-transform them.\nIf you have a 0/1-coded outcome variable, you can use the mean() function to to get the proportion of 1s in that variable.\nUsing group_by() %&gt;% mutate() (instead of group_by() %&gt;% summarise()) adds a new column containing the summary information without losing any of the existing columns.\n\nVisualisation\n\ngeom_text() creates a scatter plot like geom_point() does, but it uses text labels instead of points (you have to tell it which text to use in aes()).\nEven though we visualise binary outcomes as proportions, we still fit the Bernoulli model to the original variable that contains values of 0 and 1.\n\nModelling\n\nWith a continuous predictor, the \\(\\beta\\) coefficient that the model estimates represents the change in the outcome variable that is associated with one unit change (in fact, one unit increase) in the predictor.\nWhen two continuous predictors A and B interact, then the \\(\\beta\\) coefficient estimated for the interaction term corresponds to the change in predictor A’s effect that accompanies a unit increase in predictor B (and equivalently, the change in predictor B’s effect that accompanies a unit increase in A).\nWhen reporting a model’s estimates, you should always report and interpret the population-level coefficients’ means and 95% CrIs. It’s also generally a good idea to compute the conditional posterior probability distributions, because with these, you can report and/or visualise for all conditions of interest what the model thinks the outcome variable’s mean values will be."
  },
  {
    "objectID": "tutorials/tutorial-w09.html#footnotes",
    "href": "tutorials/tutorial-w09.html#footnotes",
    "title": "QML tutorial – Week 9",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe first time you run a model with this argument, that’s the version of the model that will be saved and loaded every time after. Beware of this if you use the same file argument to run a model that’s different from the final one you’ll want to run!↩︎"
  },
  {
    "objectID": "tutorials/tutorial-w07.html",
    "href": "tutorials/tutorial-w07.html",
    "title": "QML tutorial - Week 7",
    "section": "",
    "text": "We will model data from Koppensteiner et al, 2016. Shaking Takete and Flowing Maluma. Non-Sense Words Are Associated with Motion Patterns. DOI: 10.1371/journal.pone.0150610. The study looked at the relationship between sound iconicity and body motion. Read the paper abstract for an overview.\nTo download the file with the data right-click on the following link and download the file: takete_maluma.txt. (Note that tutorial files are also linked in the Syllabus). Remember to save the file in data/ in the course project folder."
  },
  {
    "objectID": "tutorials/tutorial-w07.html#takete-and-maluma",
    "href": "tutorials/tutorial-w07.html#takete-and-maluma",
    "title": "QML tutorial - Week 7",
    "section": "",
    "text": "We will model data from Koppensteiner et al, 2016. Shaking Takete and Flowing Maluma. Non-Sense Words Are Associated with Motion Patterns. DOI: 10.1371/journal.pone.0150610. The study looked at the relationship between sound iconicity and body motion. Read the paper abstract for an overview.\nTo download the file with the data right-click on the following link and download the file: takete_maluma.txt. (Note that tutorial files are also linked in the Syllabus). Remember to save the file in data/ in the course project folder."
  },
  {
    "objectID": "tutorials/tutorial-w07.html#read-and-process-the-data",
    "href": "tutorials/tutorial-w07.html#read-and-process-the-data",
    "title": "QML tutorial - Week 7",
    "section": "2 Read and process the data",
    "text": "2 Read and process the data\nCreate a new .Rmd file first, save it in code/ and name it tutorial-w07 (the extension .Rmd is added automatically!).\nI leave to you creating title headings in your file as you please. Remember to to add knitr::opts_knit$set(root.dir = here::here()) in the setup chunk and to attach the tidyverse.\nGo ahead and read the data. Since this file is a .txt file, you will have to use the read_tsv() function, rather than the read_csv() function. (read_tsv() is in the package readr, which comes with the tidyverse, so you don’t need to attach any extra packages for this.) Assign the data table to a variable called motion.\n\nmotion &lt;- ...\n\nThe data has the following columns:\n\nTak_Mal_Stim: the stimulus (Maluma vs Takete).\nAnswer: accuracy (CORRECT vs WRONG).\nCorr_1_Wrong_0: same as Answer but coded as 1 and 0 respectively.\nRater: study design info.\nFemale_0: participant’s gender (alas, as binary gender).\n\nThe study specifically analysed the accuracy of the responses (Answer) but in this tutorial we will look instead at the response itself (whether they selected takete or maluma).\nAlas, this piece of information is not coded in a column in the data, but we can create a new column based on the available info.\n\nWhen the stimulus is Takete and the answer is CORRECT then the participant’s response was Takete.\nWhen the stimulus is Takete and the answer is WRONG then the participant’s response was Maluma.\nWhen the stimulus is Maluma and the answer is CORRECT then the participant’s response was Maluma.\nWhen the stimulus is Maluma and the answer is WRONG then the participant’s response was Takete.\n\nNow, go ahead and create a new column called Response using the mutate() and the case_when() function.\nWe have not encountered this function yet, so here’s a challenge for you: check out its documentation to learn how it works. You will also need to use the AND operator &: this allows you to put two statements together, like Tak_Mal_Stim == \"Takete\" & Answer == \"CORRECT\" for “if stimulus is Takete AND answer is CORRECT”.\n(If you are following the documentation but case_when() is still giving you mysterious errors, make sure that your version of dplyr is the most current one. To do this, run packageVersion(\"dplyr\") in the console. You want the output to be 1.1.0. If it’s not, you’ll need to update tidyverse by re-installing it.)\n\nmotion &lt;- motion %&gt;%\n  mutate(\n    ...\n  )\n\nThe column Tak_Mal_Stim has quite a long and redundant name. Let’s change it to something shorter: Stimulus.\nYou can do so with the rename() function from dplyr. The new name goes on the left of the equals sign =, and the current name goes on the right.\n\nmotion &lt;- motion %&gt;%\n  rename(Stimulus = Tak_Mal_Stim)\n\nIf you have done things correctly, you should have a Response column that looks like this (only showing relevant columns).\n\nmotion %&gt;%\n  # select() allows you to select specific columns\n  select(Stimulus, Answer, Response)\n\n\n\n  \n\n\n\nBefore moving on, here’s another challenge: Plot the response by stimulus to get a descriptive picture of what the data looks like. In other words, make a plot that tells us something about the responses given for each kind of stimulus. (To help you along, identify what kind of data this is, and think about what kind of visualisation is appropriate for this data type.)\nNow we can use Response as our outcome variable!"
  },
  {
    "objectID": "tutorials/tutorial-w07.html#model-response",
    "href": "tutorials/tutorial-w07.html#model-response",
    "title": "QML tutorial - Week 7",
    "section": "3 Model Response",
    "text": "3 Model Response\nWe want to model Response as a function of Stimulus. Since Response is binary, we have:\n\\[\n\\begin{align}\n\\text{Resp} & \\sim Bernoulli(p) \\\\\nlogit(p) & = \\beta_0 + \\beta_1 \\cdot \\text{Stim} \\\\\n\\beta_0 & \\sim Gaussian(\\mu_0, \\sigma_0) \\\\\n\\beta_1 & \\sim Gaussian(\\mu_1, \\sigma_1)\n\\end{align}\n\\]\nGo through the formulae above and try to understand what is going on with each.\nNow, it’s time to model the data with brm() (you need to attach the brms package):\n\nAdd the formula (referring to lecture materials if needed).\nSpecify the family and data.\nAdd backend = \"cmdstanr\".\nRemember to save the model to a file using the file argument.\n\n\nresp_bm &lt;- brm(\n  ...\n)\n\nRun the model and then check the summary:\n\nsummary(resp_bm)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: Response ~ Stimulus \n   Data: motion (Number of observations: 460) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept         -0.81      0.15    -1.11    -0.53 1.00     4017     2747\nStimulusTakete     1.78      0.21     1.37     2.20 1.00     3743     2395\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n3.1 Random MCMC\nIf you compare the output of your model and the output shown here, you might notice that the values for the estimates and error are different.\nThis is because of the random component of the MCMC algorithm: every time you re-run a Bayesian model, the results will be slightly different.\nOne way to make your results reproducible (i.e. they return exactly the same values every time), is to set a seed, i.e. a number that is used for (pseudo-)random generation of numbers (which is used in the MCMC algorithm).\nThe quickest way to set a seed is to use the seed argument of the brm() function. You can set to any number. Note that by saving the model output to a file with the file argument you are also ensuring reproducibility, as long as you don’t delete the file.\nThe best (and safest) practice is to both set a seed and save the output to file."
  },
  {
    "objectID": "tutorials/tutorial-w07.html#results",
    "href": "tutorials/tutorial-w07.html#results",
    "title": "QML tutorial - Week 7",
    "section": "4 Results",
    "text": "4 Results\nNow, look at the population-level effects.\nWhich coefficients from the formulae above do Intercept and StimulusTakete correspond to?\n\n4.1 Intercept\nLet’s go through the Intercept together. I will leave working out the interpretation of StimulusTakete to you.\nBarring small differences in the results due to the random component of the MCMC, the estimate and error for the Intercept are –0.81 and 0.15. Before moving on, test your recall: Do you remember from the lecture which unit are these estimates in?\nThey are in log-odds. This is because of the \\(logit(p)\\) part in the formula above: to model \\(p\\) we had to use the logit (mathematical) function, which outputs log-odds.\nSo now, so that we can more easily interpret and report our results, we need to convert log-odds back into probabilities. We can do that with the plogis() R function.\nAs an exercise, convert the estimate of the Intercept to a probability. What probability does –0.81 log-odds correspond to? Again, do this before moving on.\nSince the Intercept is \\(\\mu_0\\) from the formula above, you can say that the mean probability of \\(\\beta_0\\) is the probability that corresponds to –0.81 log-odds (which you should have found is approximately 0.3 or 30% probability).\n\\(\\beta_0\\) is the probability of choosing “takete” when the stimulus is “maluma”. (Why? To understand this, think about the order of the levels in Response and Stimulus, and what that means for how they are coded.)\nBased on the model and data, there is 30% probability of choosing the response “takete” when the stimulus is “maluma”, at 95% confidence.\n\n\n4.2 StimulusTakete\nNow, what about StimulusTakete? Work through this for yourself, using what you’ve learned so far.\nWhich coefficient in the formula above does it correspond to? How do we interpret the posterior probability of this coefficient?\nHow does this coefficient relate to the intercept, i.e., the log-odds of choosing “takete” when the stimulus is “maluma”?\nIf you’re feeling particularly adventurous, before you move on to the next section, compute the probability of choosing the response “takete” when the stimulus is “takete”."
  },
  {
    "objectID": "tutorials/tutorial-w07.html#plotting",
    "href": "tutorials/tutorial-w07.html#plotting",
    "title": "QML tutorial - Week 7",
    "section": "5 Plotting",
    "text": "5 Plotting\nLet’s plot the results.\nFirst, let’s plot the posterior probabilities of the Intercept and StimulusTakete.\n\n5.1 Posterior probability distributions\nThe first step is to obtain the draws with as_draws_df().\n\nresp_bm_draws &lt;- as_draws_df(resp_bm)\n\nresp_bm_draws\n\n\n\n  \n\n\n\nGo ahead, plot the density of b_Intercept (use geom_density()).\n\nresp_bm_draws %&gt;%\n  ggplot(...) +\n  ...\n\nAnd separately, plot the density of b_StimulusTakete.\n\n\n5.2 Conditional posterior probability distributions\nNow, let’s plot the conditional (posterior) probability distributions of the log-odds when the stimulus is “maluma” and when it is “takete”.\nThere are a few data-manipulation steps involved before we can get our data into a format that ggplot can handle. First up, we’ll use our draws to compute the conditional posterior probability distributions for each level of Stimulus.\nDo you remember how to do this? Here’s a hint:\n\n\\(logit(p_{stim = maluma}) = \\beta_0\\)\n\\(logit(p_{stim = takete}) = \\beta_0 + \\beta_1\\)\n\nIdentify which parts of resp_bm_draws correspond to \\(\\beta_0\\) and \\(\\beta_1\\).\nNow, mutate the data following the equations above, so that you have two new columns: one called Maluma and one called Takete, with the conditional log-odds of getting a “takete” response when the stimulus is “maluma” and “takete” respectively.\n\nresp_bm_draws &lt;- resp_bm_draws %&gt;%\n  mutate(\n    Maluma = ...,\n    Takete = ...\n  )\n\nNow we’ve got the data we need for plotting the conditional posterior distributions, but it’s not quite in the right format yet. So let’s reformat the data so that it’s easier to plot!\nAt this point, you should have two columns, Maluma and Takete, each containing log-odds values. Something like this (not necessarily containing these values exactly):\nMaluma     Takete\n-0.59      0.83\n-0.53      0.93\n...\nBut plotting would be easier if we could have instead something like:\nStimulus     logodds\nMaluma       -0.59\nMaluma       -0.53\nTakete       0.83\nTakete       0.93\n...\nso that you could use logodds as the x-axis and Stimulus as fill.\nWe can achieve that by using the so-called pivot functions: they are very powerful functions, but we will only look at their simple use here.\nMore specifically, we need the pivot_longer() function: this is used in cases where you have columns named after some groups (e.g., our Maluma and Takete columns here), and instead you want the group names to be in a column of their own (e.g., one called Stimulus) while the values are in another column (e.g., one called logodds).\nHere’s how it works (never mind the warning):\n\nresp_bm_draws_l &lt;- resp_bm_draws %&gt;%\n  # Let's select the columns from .chain to Takete\n  select(.chain:Takete) %&gt;%\n  # Now let's pivot the Maluma and Takete columns\n  pivot_longer(\n    cols = Maluma:Takete,\n    # Name of the column to store the original column names in\n    names_to = \"Stimulus\",\n    # Name of the column to store the original values in\n    values_to = \"logodds\"\n  )\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\nresp_bm_draws_l\n\n\n\n  \n\n\n\nThis looks more like what we want!\nNow we can easily plot the conditional probability distributions of the response “takete” when the stimulus is “maluma” and “takete”.\n\nresp_bm_draws_l %&gt;%\n  ggplot(aes(logodds, fill = Stimulus)) +\n  geom_density(alpha = 0.5) +\n  geom_rug()\n\n\n\n\nNow, this is well and good, but often it’s easier to think about our results in terms of probabilities, rather than log-odds. So what do we do if we want to plot the probability of choosing “takete”? Before looking ahead, try to guess for yourself what the solution will be.\nIt’s as we’ve seen before! We convert log-odds into probabilities with the plogis() function, like so:\n\nresp_bm_draws_l %&gt;%\n  mutate(\n    prob = plogis(logodds)\n  ) %&gt;%\n  ggplot(aes(prob, fill = Stimulus)) +\n  geom_density(alpha = 0.5) +\n  labs(\n    title = \"Probability of takete response\"\n  )\n\n\n\n\nThe distributions themselves look pretty similar, but have a look at the numbers on the x axis. We’re back in probability space, hurrah!\nThe take-away message from this plot: We can be quite confident that different stimuli result in different probabilities of choosing “takete”, and that it is very much more probable that participants choose “takete” when the stimulus is in fact of the “takete” type!"
  },
  {
    "objectID": "tutorials/tutorial-w07.html#credible-intervals",
    "href": "tutorials/tutorial-w07.html#credible-intervals",
    "title": "QML tutorial - Week 7",
    "section": "6 Credible intervals",
    "text": "6 Credible intervals\nIn the lecture, we learnt about the empirical rule, quantiles and credible intervals.\nNow let’s look in more detail at how to obtain credible intervals (CrIs) for any probability distribution.\nFor sake of illustration, let’s calculate CrIs of different levels (60%, 80%, 90%) for the conditional (posterior) probabilities of the model above. (Note that you can calculate CrIs also for the (non-conditional) posterior probability distributions).\nThe following code uses the quantile2() function from the posterior package.\nThe posterior package should already be installed, but if not make sure you install it (to know if a package is installed, go to the Packages tab in the bottom-right panel of RStudio and use the search box).\nThen you also need to attach the package. Add the code for attaching packages in the setup chunk and make sure you run the chunk to attach the package!\nFor this function, we’ll again need to use the longer-format version of the draws we obtained from our model. This data is in resp_bm_draws_l.\n\nresp_bm_draws_l %&gt;%\n  group_by(Stimulus) %&gt;%\n  summarise(\n    # Calculate the lower and upper bounds of a 60% CrI\n    q60_lo = round(quantile2(logodds, probs = 0.2), 2),\n    q60_hi = round(quantile2(logodds, probs = 0.8), 2),\n    # Transform into probabilities\n    p_q60_lo = round(plogis(q60_lo), 2),\n    p_q60_hi = round(plogis(q60_hi), 2)\n  )\n\n\n\n  \n\n\n\nWe have already seen group_by() and summarise() before, so I leave understanding this part to you.\nThe new piece of code is quantile2(logodds, probs = 0.2) and quantile2(logodds, probs = 0.8):\n\nThe quantile2() function gives you the percentile based on the specified probability (the probs argument). So probs = 0.2 returns the 20th percentile and probs = 0.8 returns the 80th percentile.\nThe round() function is used to round numbers to the nearest integer or decimal:\n\nThe second argument of round() is the number of digits you want to round to, here 2.\n\nFinally, we convert log-odds to probabilities with the plogis() function.\n\nThe output of the code gives us the 60% CrI of the probability of obtaining a “takete” response when the stimulus is “maluma” and “takete” respectively, both in log-odds and probabilities.\nBased on the output we can say the following:\n\nWe can be 60% confident that the probability of a “takete” response is between 28% and 33% (–0.94 to –0.69 in log-odds) with the “maluma” stimuli, and it is between 70% and 75% with the “takete” stimuli.\n\nNice!\nNow I leave to you calculating the 80% and 90% CrIs. Can you tell which values to use with the probs argument for an 80% and 90% interval?"
  },
  {
    "objectID": "tutorials/tutorial-w07.html#summary",
    "href": "tutorials/tutorial-w07.html#summary",
    "title": "QML tutorial - Week 7",
    "section": "7 Summary",
    "text": "7 Summary\n\n\n\n\n\n\nRead and process data\n\nUse read_tsv() for tab separated files.\ncase_when() is a more flexible version of ifelse() that allows you to specify multiple values based on multiple conditions.\nThe pivot functions allow you to reshape the data. pivot_longer() makes the data table longer by moving the names of specified columns into a column of its own and by moving the values to a column of its own. Check out pivot_wider() for the opposite procedure.\n\nModelling\n\nWe can model binary outcomes with brm(family = bernoulli()).\nTo make your analysis reproducible, set a seed using the seed argument and save the model to a file using the file argument.\nYou can transform log-odds back to probabilities using the plogis() function.\n\nCredible intervals\n\nCalculate the lower and upper bounds of CrIs using the quantile2() function from the posterior package."
  },
  {
    "objectID": "tutorials/tutorial-w04.html",
    "href": "tutorials/tutorial-w04.html",
    "title": "QML tutorial - Week 4",
    "section": "",
    "text": "Let’s pick up from where we left off last week.\nIn week 3, you learnt how to use ggplot2 to create bar charts and how to create different panels. Bar charts are great for categorical data.\nThis week you will learn how to make density and violin plots, which work very well with numeric data in combination of categorical data (for example, when you want to show values of a numeric variable across different groups).\nThen, you will use the brm() function from the brms package to model continuous variables.\nBut first, let’s read some data and transform it so that we can use it in the tutorial."
  },
  {
    "objectID": "tutorials/tutorial-w04.html#mutating-data",
    "href": "tutorials/tutorial-w04.html#mutating-data",
    "title": "QML tutorial - Week 4",
    "section": "1 Mutating data",
    "text": "1 Mutating data\n\n1.1 The alb_vot data\nLet’s play around with the Albanian VOT data we used in class.\nThe data comes from the paper by Coretta and colleagues, 2022. Northern Tosk Albanian. DOI: 10.1017/S0025100322000044. This is an IPA Illustration of Northern Tosk Albanian. It contains data from voiceless and voiced consonants of Nothern Tosk Albanian, as uttered in a set of words by 5 speakers.\nTo download the file with the data right-click on the following link and download the file: alb_vot.csv. (Note that tutorial files are also linked in the Syllabus). Remember to save the file in data/ in the course project folder.\nCreate a new .Rmd file first, save it in code/ and name it tutorial-w04 (the extension .Rmd is added automatically!).\nI leave to you creating headings in your file as you please. Remember to to add knitr::opts_knit$set(root.dir = here::here()) in the setup chunk and to attach the tidyverse.\nNow you can create a new code chunk and read the alb_vot.csv file in R. Remember how to do this?\n\nalb_vot &lt;- ...\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nIt’s one of the read_*() functions.\n\n\n\nGreat!\n\nalb_vot\n\n\n\n  \n\n\n\nThe data frame has the following columns:\n\nspeaker: the speaker ID.\nfile: the sound file the measurements are take from.\nlabel: the label in the interval the measurements are taken from.\nrelease: the time of the consonant’s closure release from the beginning of the sound file in seconds.\nvoi_onset: the onset of voicing of the vowel following the target consonant from the beginning of the sound file in seconds.\nconsonant: the IPA transcription of the target consonant.\n\nNow we need to create a new column with the Voice Onset Time (vot) values. Note that VOT is simply the difference between the onset of voicing (voi_onset) and the time of release (release). We will do this in the next section.\n\n\n1.2 Create a new column\nTo change or create columns, we can use the mutate() function from the dplyr package. This is another tidyverse package, and it’s attached when you attach the tidyverse. The summarise() and count() functions you used in Week 2 are from this same package.\nLet’s calculate VOT now.\n\nalb_vot &lt;- alb_vot %&gt;%\n  mutate(\n    vot = voi_onset - release\n  )\n\nSince we are mutating alb_vot, we need to assign (&lt;-) the output of mutate() back into alb_vot. Check the alb_vot data frame now to view the newly created column vot (to view the data frame, click on alb_vot in the Environment tab, in the top-right panel of Rstudio).\nWell done!\n\n\n\n\n\n\nWarning\n\n\n\nNote that the following code, without the pipe, gives you the same result.\nalb_vot &lt;- mutate(alb_vot, vot = voi_onset - release)\n\n\nNormally, VOT is measured in milliseconds (ms), but the vot column is now in seconds (because release and voi_onset are in seconds).\nLet’s change this. We can overwrite the vot column like so:\n\nalb_vot &lt;- alb_vot %&gt;%\n  mutate(\n    # Multiply by 1000 to get ms from s\n    vot = (voi_onset - release) * 1000\n  )\n\nCheck alb_vot now to see that it worked.\n\n\n\n\n\n\nMutate\n\n\n\nThe function mutate() allows you to create new columns or to change existing ones."
  },
  {
    "objectID": "tutorials/tutorial-w04.html#density-plots",
    "href": "tutorials/tutorial-w04.html#density-plots",
    "title": "QML tutorial - Week 4",
    "section": "2 Density plots",
    "text": "2 Density plots\n\n\n\n\n\n\nDensity plots\n\n\n\nDensity plots show the distribution (i.e. the probability density) of the values of a continuous variable.\nThey are created with geom_density().\n\n\nVOT is a numeric continuous variable so density plots are appropriate.\nTo plot the probability density of a continuous variable, you can use the density geometry. Remember, all geometry functions start with geom_.\nFill in the … in the following code to create a density plot of VOT values in alb_vot.\n\nalb_vot %&gt;%\n  ggplot(aes(x = vot)) +\n  ...\n\nNote that to create a density plot, you only need to specify the x-axis. The y-axis is the probability density, which is automatically calculated (a bit like counts in bar charts, remember?).\n\n2.1 Make things cosy with a rug\nThe density line shows you a smoothed representation of the data distribution over the VOT values, but you might also want to see the raw data.\nYou can do so by adding the rug geometry. Go ahead and add a rug…\n\nalb_vot %&gt;%\n  ggplot(aes(vot)) +\n  geom_density() +\n  ...\n\nYou should get the following:\n\n\n\n\n\nNice huh?\n\n\n\n\n\n\nRug\n\n\n\nRaw data can be shown with a rug, i.e. ticks on the axes that mark where the data is.\nYou can add a rug with geom_rug().\n\n\n\n\n\n\n\n\nQuiz 1\n\n\n\nWhat can you notice about the distribution of VOT values?\n\n\n\n\n\n\nHint\n\n\n\n\n\nAre there multiple peaks in the distribution?"
  },
  {
    "objectID": "tutorials/tutorial-w04.html#filtering-data",
    "href": "tutorials/tutorial-w04.html#filtering-data",
    "title": "QML tutorial - Week 4",
    "section": "3 Filtering data",
    "text": "3 Filtering data\nWe are getting closer and closer to start modelling our VOT data to learn more about them.\nNormally, you would approach data modelling by using the entire data set and including as many variables as it makes sense. Since you are just starting to learn how to model data with brms, we will first take a pedagogical approach and start small.\nYou will have noticed in the density plot that there seems to be two groups of data: and you would be right! The data contains VOT values for both voiceless and voiced plosives.\nFor now, we will filter the data so that we include only voiceless plosives that don’t have a VOT longer than 20 ms. (Again, we are doing this for pedagogical reasons, not because this is what you would do with this data in real-life research!).\nBefore we can move on onto filtering data, we first need to learn about logical operators.\n\n3.1 Logical operators\nThere are four main logical operators:\n\nx == y: x equals y.\nx != y: x is not equal to y.\nx &gt; y: x is greater than y.\nx &lt; y: x is smaller than y.\n\nLogical operators return TRUE or FALSE depending on whether the statement they convey is true or false. Remember, TRUE and FALSE are logical values.\nTry these out in the Console:\n\n# This will return FALSE\n1 == 2\n\n[1] FALSE\n\n# FALSE\n\"apples\" == \"oranges\"\n\n[1] FALSE\n\n# TRUE\n10 &gt; 5\n\n[1] TRUE\n\n# FALSE\n10 &gt; 15\n\n[1] FALSE\n\n# TRUE\n3 &lt; 4\n\n[1] TRUE\n\n\n\n\n\n\n\n\nLogical operators\n\n\n\nLogical operators are symbols that compare two objects and return either TRUE or FALSE.\nThe most common logical operators are ==, !=, &gt;, and &lt;.\n\n\n\n\n\n\n\n\nQuiz 2\n\n\n\n\n\nWhich of the following is wrong?\n\n 3 &gt; 1 \"a\" = \"a\" \"b\" != \"b\" 19 &lt; 2\n\nWhich of the following returns c(FALSE, TRUE)?\n\n 3 &gt; c(1, 5) c(\"a\", \"b\") != c(\"a\") \"apple\" != \"apple\" c(\"A\", \";\") == c(\"B\", \";\")\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n2a.\nCheck for errors in the logical operators.\n2b.\nI tricked you! ; is the Roman semicolon, and ; is the Greek question mark!\n\n\n\n\n\nNow let’s see how these work with filter()!\n\n\n3.2 The filter() function\nFiltering in R with the tidyverse is straightforward. You can use the filter() function.\nfilter() takes one or more statements with logical operators.\nLet’s try this out. The following code filters the consonant column so that only the consonant p is included.\n\nalb_vot %&gt;%\n  filter(consonant == \"p\")\n\n\n\n  \n\n\n\nNeat! What if we want to include all consonants except p? Easy, we use the non-equal operator !=.\n\nalb_vot %&gt;%\n  filter(consonant != \"p\")\n\n\n\n  \n\n\n\nAnd if we want only k from speaker s02? We can include multiple statements separated by a comma!\n\nalb_vot %&gt;%\n  filter(consonant == \"k\", speaker == \"s02\")\n\n\n\n  \n\n\n\nCombining statements like this will give you only those rows where both conditions apply. You can add as many statements as you need.\nNow try to filter the data so that you include only speaker s04 and VOT values that are shorter than 20 ms.\n\nalb_vot %&gt;%\n  filter(...)\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nDo you remember the logical operator for “smaller than”?\n\n\n\nThis is all great, but what if we want to include multiple consonants?\nTo do that we need another operator: %in%.\n\n\n3.3 The %in% operator\n\n\n\n\n\n\n%in%\n\n\n\nThe %in% operator is a special logical operator that returns TRUE if the value to the left of the operator is one of the values in the vector to its right, and FALSE if not.\n\n\nTry these in the Console:\n\n# TRUE\n5 %in% c(1, 2, 5, 7)\n\n[1] TRUE\n\n# FALSE\n\"apples\" %in% c(\"oranges\", \"bananas\")\n\n[1] FALSE\n\n\nBut %in% is even more powerful because the value on the left does not have to be a single value, but it can also be a vector! We say %in% is vectorised because it can work with vectors (most functions and operators in R are vectorised).\n\n# TRUE, TRUE\nc(1, 5) %in% c(4, 1, 7, 5, 8)\n\n[1] TRUE TRUE\n\nstocked &lt;- c(\"durian\", \"bananas\", \"grapes\")\nneeded &lt;- c(\"durian\", \"apples\")\n\n# TRUE, FALSE\nneeded %in% stocked\n\n[1]  TRUE FALSE\n\n\nTry to understand what is going on in the code above before moving on.\n\n\n3.4 Now filter the data\nNow we can filter alb_vot to include only the voiceless plosives p, t, k and VOT values smaller than 20 ms.\n\nalb_vot_vl &lt;- alb_vot %&gt;%\n  filter(\n    consonant %in% c(\"p\", \"t\", \"k\"),\n    vot &lt; 20\n  )\n\nThis should not look too alien! The first statement, consonant %in% c(\"p\", \"t\", \"k\") looks at the consonant column and, for each row, it returns TRUE if the current row value is in c(\"p\", \"t\", \"k\"), and FALSE if not.\n\n\n\n\n\n\nExplanation\n\n\n\n\n\nIf it’s not very clear what is going on, try the following code.\nalb_vless &lt;- alb_vot %&gt;%\n  mutate(voiceless = consonant %in% c(\"p\", \"t\", \"k\"))\nalb_vless\n\nalb_vless %&gt;%\n  filter(voiceless == TRUE)\n\n\n\nThe second statement, vot &lt; 20, filters in VOT values that are less than 20 ms.\nYou might have noticed that we need to assign the output of filter(), which is a data frame, to a new variable alb_vot_vl (“vl” for “voiceless”). This will be the data frame you will use in the next section."
  },
  {
    "objectID": "tutorials/tutorial-w04.html#modelling-with-brm",
    "href": "tutorials/tutorial-w04.html#modelling-with-brm",
    "title": "QML tutorial - Week 4",
    "section": "4 Modelling with brm()",
    "text": "4 Modelling with brm()\nIn the lecture, we have seen that we can assume voiceless VOT values to be distributed according to a Gaussian distribution (you may also have encountered Gaussian distributions under the name “normal distribution”).\n\\[vot \\sim Gaussian(\\mu, \\sigma)\\]\nWe can calculate the sample mean and standard deviation easily, using the summarise() function, as you learnt in Week 2. Go ahead and complete the code.\n\nalb_vot_vl %&gt;%\n  summarise(\n    ...\n  )\n\nBut now we want to make inference from our sample (N = 24) to the entire population of VOT values of the Albanian voiceless plosives /p/, /t/, /k/.\nWe want to estimate the following probability distributions and parameters:\n\\[\n\\begin{align}\nvot & \\sim Gaussian(\\mu, \\sigma) \\\\\n\\mu & \\sim Gaussian(\\mu_1, \\sigma_1) \\\\\n\\sigma & \\sim TruncGaussian(\\mu_2, \\sigma_2)\n\\end{align}\n\\]\nIn other words, we are trying to estimate the following hyperparameters: \\(\\mu_1\\), \\(\\sigma_1\\), \\(\\mu_2\\), \\(\\sigma_2\\).\nWe can achieve this by modelling the data using the brm() function from the brms package (BRM stands for Bayesian Regression Model). We will go into more details about what this means in the weeks to follow. For now, note that regression is a synonym of linear model, a type of modelling approach that you are learning in this course.\nAdd library(brms) in the setup chunk and run that chunk again so that the brms package is attached. Then, run the following code.\n\nalb_vot_bm &lt;- brm(\n  vot ~ 1,\n  family = gaussian(),\n  data = alb_vot_vl,\n  backend = \"cmdstanr\"\n)\n\nYou will see the message Compiling Stan program... followed by Start sampling. Something like in the figure below.\n\nWe will get back to what all this means in the coming weeks. For now know that Stan is a programming language that can run Bayesian models. brm() is an R interface to Stan, so that you can use R and R syntax to run Bayesian models without having to learn Stan! (If you feel adventurous, nobody stops you from learning Stan too, although it is not required for this course).\nThe “sampling” part is about the Markov-Chain Monte Carlo (or MCMC) sampling algorithm. For a quick intro on MCMC, check the Extra box below.\n\n\n\n\n\n\nExtra: Markov-Chain Monte Carlo\n\n\n\n\n\n\n\n\n\n\n\nMCMC\n\n\n\nMarkov-Chain Monte Carlo refers to a set of algorithms used to estimate parameters of unknown distributions, based on repeated draws from parameter values and observed data.\n\n\nYou don’t need to understand the details of this. For now, just remember that several draws are made (i.e. the algorithm is run for several iterations), and that the model runs 4 chains of these iterations.\nWe will learn more about chains and iterations in the coming weeks and you will know how to use information about these to diagnose the robustness of the estimated parameters.\n\n\n\n\n4.1 Model summary\nNow we can inspect the model output using the summary() function (don’t confuse this with the summarise() function, used to get statistical summaries of data).\n\nsummary(alb_vot_bm)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: vot ~ 1 \n   Data: alb_vot_vl (Number of observations: 24) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    11.62      0.59    10.48    12.78 1.00     2559     2158\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.88      0.44     2.18     3.88 1.00     2327     2013\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWe have seen the summary of this model during the lecture. Do you remember how to interpret it?\n\n\n\n\n\n\nQuiz 3\n\n\n\n\n\nThe 95% CrI of \\(\\mu\\) (Intercept) includes 12.9. TRUEFALSE\nThe standard deviation \\(\\sigma_2\\) of \\(\\sigma\\) (sigma) is 0.44 TRUEFALSE\n\n\n\n\nYou can use the tidy() function from the broom.mixed package to extract the information on the estimated parameters as a table (remember to attach the package in the setup chunk; you might also need to install the package).\n\ntidy(alb_vot_bm)\n\n\n\n  \n\n\n\nThe term column gives us the two main parameters we wanted to estimate:\n\n(Intercept) is actually \\(\\mu\\).\nsd__Observation is \\(\\sigma\\).\n\nNote that sd__Observation in the tidy table corresponds to sigma in the model summary above.\nNow, for each of the two parameters, you get estimate and std.error. There are the hyperparameters of the probability distributions for \\(\\mu\\) and \\(\\sigma\\).\n\n(Intercept) (\\(\\mu\\)): estimate is \\(\\mu_1\\): 11.62 ms, std.error is \\(\\sigma_1\\): 0.57 ms.\nsd__Observation (\\(\\sigma\\)): estimate is \\(\\mu_2\\): 2.86 ms, std.error is \\(\\sigma_2\\): 0.44 ms.\n\nIn notation:\n\\[\n\\begin{align}\nvot & \\sim Gaussian(\\mu, \\sigma) \\\\\n\\mu & \\sim Gaussian(11.62, 0.57) \\\\\n\\sigma & \\sim TruncGaussian(2.86, 0.44)\n\\end{align}\n\\]\nBased on the model summary and on the tidy summary, we also know the 95% Credible Intervals (CrIs) of the probability distributions of \\(\\mu\\) and \\(sigma\\). They are in the columns of the tidy summary called conf.low and conf.high.\n\n\n\n\n\n\nCredible Interval\n\n\n\nA 95% (Bayesian) Credible Interval indicates the range of values we can be 95% confident contains the true parameter value.\n\n\n\nAt 95% confidence, the mean (\\(\\mu\\)) VOT value for Albanian voiceless stops is between 10.48 and 12.78 ms.\nAs for the standard deviation (\\(\\sigma\\)), we can be 95% confident that it is between 2.18 and 3.88 ms.\n\n\n\n4.2 Reporting\nHow would you report all that?\nReporting statistical models is quite formulaic, so you can use the following as a template and replace values as needed when you need to report other models.\n\nWe fitted a Bayesian model of the VOT of Albanian voiceless plosives, using a Gaussian distribution as the distribution family. According to the model, there is a 95% probability that the VOT mean is between 10.48 and 12.78 ms (\\(\\beta\\) = 11.6, SD = 0.59), and there is a 95% probability that the VOT SD is between 2.18 and 3.88 (\\(\\beta\\) = 2.87, SD = 0.43).\n\nIt is common to report both the CrIs and the estimate and standard deviation of each parameter.\nIt is also helpful to visualise probability distributions by plotting probability densities. You will learn how to do this next week!\nIn the meantime, read on to find out how to create violin plots and customise your plots!"
  },
  {
    "objectID": "tutorials/tutorial-w04.html#violin-plots",
    "href": "tutorials/tutorial-w04.html#violin-plots",
    "title": "QML tutorial - Week 4",
    "section": "5 Violin plots",
    "text": "5 Violin plots\nAn efficient way of showing the distribution of continuous variables depending on discrete groups (like consonant) are the so-called violin plots.\nGo ahead and run the following code.\n\nalb_vot %&gt;%\n  ggplot(aes(consonant, vot)) +\n  geom_violin()\n\n\n\n\nThey are called violin plots because they look like violins! The geometry for violin plots is… geom_violin().\nUsually, you need the following aesthetics:\n\nx-axis: a categorical variable, like consonant.\ny-axis: a continuous numeric variable, like vot.\n\n\n5.1 Lay over the raw data\nYou can lay over raw data on top of the violins. To do so, you can use the jitter geometry.\ngeom_jitter() creates so-called strip charts. Let’s first see how these look like without violins.\n\nalb_vot %&gt;%\n  ggplot(aes(consonant, vot)) +\n  geom_jitter()\n\n\n\n\nThat doesn’t look great right? The points are spread too wide.\nWe can fix that by specifying the argument width in the jitter geometry to be something smaller than 0.5. Try a few values until you think it looks better.\n\nalb_vot %&gt;%\n  ggplot(aes(consonant, vot)) +\n  geom_jitter(width = ...)\n\nWhen you are satisfied, create a violin plot with overlaid strip charts.\n\nalb_vot %&gt;%\n  ggplot(aes(consonant, vot)) +\n  geom_violin() +\n  geom_jitter(width = ...)\n\nNote that the order between the violin and jitter geometries is important. If you add the jitter geometry before the violin geometry, the violins will just cover the jittered points (see why it is called “layered” grammar of graphics?).\nNow, try to create a violin and strip chart plot so that each speaker is plotted separately on the y-axis instead of consonant.\n\n\n5.2 If-else\nIt’s clear from this last plot where each speaker is plotted separately that there are two subgroups of consonants: there are voiced and voiceless consonants (and you have also noticed this in the density plot above).\nSo, what if we want to plot voiceless and voiced consonants separately?\nWe can use faceting, as you learnt in Week 3, or we can use the aesthetics fill and/or colour. However, first we need to create a new column that indicates if a consonant is voiceless or voiced!\nWe will use the mutate() function to create a new voicing column. But how do we get the values for this new column (without having to manually code each row as voiceless or voiced)?\nWe can use the ifelse() function!\nThe ifelse() function works a bit like the English sentence “If X is true, do Y, otherwise do Z”. The “X” has to be a logical statement, i.e. an expression using the logical operators you learnt above.\nLook at the following code.\n\nifelse(10 &gt; 9, \"correct!\", \"wrong...\")\n\nYou can read that as: If 10 &gt; 9 is TRUE, then return correct!, if it’s FALSE, then return wrong....\nWhat will that code return? Think about this and then try the code to see that you got it right.\nifelse() is vectorised, so you can even test multiple values, like so:\n\nifelse(c(1, 3, 10) &gt; 9, \"correct!\", \"wrong...\")\n\n[1] \"wrong...\" \"wrong...\" \"correct!\"\n\n\nWe get wrong... for 1 &gt; 9 and 3 &gt; 9, and correct! for 10 &gt; 9.\nYou can even use %in%!\n\nifelse(c(\"apple\", \"banana\", \"cherry\") %in% c(\"apple\", \"durian\", \"passion fruit\"), \"got it!\", \"need to by\")\n\n[1] \"got it!\"    \"need to by\" \"need to by\"\n\n\nWe will use ifelse() in conjunction with mutate() to create a new column voicing. But first, we need to get the list of consonants in the data set!\n(You could find these manually, but why do so when you can automate things with code?)\n\n\n5.3 Finding unique values\nSo, the data contains the following consonants:\n\ncons_ipa &lt;- alb_vot$consonant\n\nunique(cons_ipa)\n\n [1] \"ts\" \"b\"  \"ɡ\"  \"k\"  \"p\"  \"t\"  \"d\"  \"dz\" \"tʃ\" \"d̻ʒ̻\" \"t̻ʃ̻\" \"dʒ\"\n\n\nWait, what’s that $ in alb_vot$consonant there? That is a base R way of extracting a single column (in this case consonant) from a data frame (alb_vot).\n\n\n\n\n\n\nThe dollar sign `$`\n\n\n\nYou can use the dollar sign $ to extract a single column from a data frame as a vector.\n\n\nIf you check cons_ipa in the Environment tab, it will say that this is chr vector, i.e. a character vector. This makes sense, because consonant is a character column in alb_vot.\n\n\n\n\n\n\nFind unique values\n\n\n\nThe unique() function shows the unique values of a vector (it works with any type of vector, numeric, character, logical, …).\n\n\nThis is great, we now know all the consonants in the consonant column, but maybe it will be easier to type them later if we use the custom transcription in label rather than IPA.\n\ncons_lab &lt;- alb_vot$label\n\nunique(cons_lab)\n\n [1] \"ts\" \"b\"  \"g\"  \"k\"  \"p\"  \"t\"  \"d\"  \"dz\" \"tS\" \"J\"  \"c\"  \"dZ\"\n\n\nGreat! Now, to make things easier, let me tell you which consonants are voiceless and which are voiced:\n\nvoiceless: c(\"ts\", \"k\", \"p\", \"t\", \"tS\", \"c\").\nvoiced: c(\"b\", \"g\", \"d\", \"dz\", \"J\", \"dZ\").\n\nNow with this knowledge we can create a new column with voiceless and voiced values for each row in the data frame. Like so:\n\nalb_vot &lt;- alb_vot %&gt;%\n  mutate(\n    voicing = ifelse(\n      # IF this is TRUE\n      label %in% c(\"ts\", \"k\", \"p\", \"t\", \"tS\", \"c\"),\n      # Write\n      \"voiceless\",\n      # OR ELSE write\n      \"voiced\"\n    )\n  )\n\nThe ifelse() function is doing the following:\n\nIf the value in the column label is among (%in%) the values in the vector c(\"ts\", \"k\", \"p\", \"t\", \"tS\", \"c\").\nWrite voiceless in the voicing column.\nOtherwise write voiced.\n\nCheck out the voicing column in alb_vot.\nNow that we have the info we need to plot voiceless and voiced consonants separately, let’s do that!\n\n\n5.4 Plotting voiceless and voiced VOT\nGo ahead and fill in the code to create a plot with violins and overlayed strip chart with voicing on the x-axis.\n\nalb_vot %&gt;%\n  ggplot(...) +\n  ...\n\nYou should get the following:\n\n\n\n\n\n\n\n5.5 Let’s add some colour\nLet’s fill the violins with colour. For now, just colour the violins depending on voicing.\n\nalb_vot %&gt;%\n  ggplot(aes(voicing, vot, ...)) +\n  geom_violin() +\n  geom_jitter(width = 0.05)\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the fill aesthetic in aes().\n\n\n\nYou should get this:\n\n\n\n\n\nNot bad! But maybe we can improve by making the fill a bit transparent?\nYou can achieve that with the alpha argument in geom_violin(), which takes a number between 0 (completely transparent) and 1 (completely opaque).\nAdd it to the code above to make the fill transparent. Try different values until you are satisfied with the output. You could also add alpha to geom_jitter() to make the points a bit transparent.\n\nalb_vot %&gt;%\n  ggplot(aes(voicing, vot, fill = voicing)) +\n  geom_violin(...) +\n  geom_jitter(width = 0.05)\n\nWonderful! Now, what can you tell about VOT in voiceless and voiced consonants in Albanian based on this plot?\nNext week we will learn how to model variables depending on different groups!\nBut for now, let’s talk about legends and labels."
  },
  {
    "objectID": "tutorials/tutorial-w04.html#legends-and-labels-in-plots",
    "href": "tutorials/tutorial-w04.html#legends-and-labels-in-plots",
    "title": "QML tutorial - Week 4",
    "section": "6 Legends and labels in plots",
    "text": "6 Legends and labels in plots\n\n6.1 Plot titles and labels\nThe labs() function allows you to set the plot title, subtitle, axis labels and more.\nSince these are strings (i.e. text), they must be quoted with \"\". Replace the ... below with an appropriate title, like \"VOT of Albanian stops\".\n\nalb_vot %&gt;%\n  ggplot(aes(voicing, vot, fill = voicing)) +\n  geom_violin() +\n  geom_jitter(width = 0.05, alpha = 0.2) +\n  labs(\n    title = ...\n  )\n\n\n\n6.2 Axis labels\nTo change the the axis labels, you can specify a string for x and y in the labs() function.\nI have changed the x label here below. Go ahead and change y to \"VOT (ms)\". (Remember to also add the title).\n\nalb_vot %&gt;%\n  ggplot(aes(voicing, vot, fill = voicing)) +\n  geom_violin() +\n  geom_jitter(width = 0.05, alpha = 0.2) +\n  labs(\n    title = ...,\n    x = \"Consonant voicing\",\n    ...\n  )\n\n\n\n6.3 Positioning the legend\nThe position of the legend can be set with the legend.position argument in the theme() function.\n\nalb_vot %&gt;%\n  ggplot(aes(voicing, vot, fill = voicing)) +\n  geom_violin() +\n  geom_jitter(width = 0.05, alpha = 0.2) +\n  theme(legend.position = \"bottom\")\n\n\n\n\nSometimes you don’t want the legend at all. You can achieve that with \"none\" as the value to the legend.position argument.\n\nalb_vot %&gt;%\n  ggplot(aes(voicing, vot, fill = voicing)) +\n  geom_violin() +\n  geom_jitter(width = 0.05, alpha = 0.2) +\n  theme(legend.position = ...)"
  },
  {
    "objectID": "tutorials/tutorial-w04.html#summary",
    "href": "tutorials/tutorial-w04.html#summary",
    "title": "QML tutorial - Week 4",
    "section": "7 Summary",
    "text": "7 Summary\n\n\n\n\n\n\nTransforming data\n\nmutate() creates new columns or mutates existing ones.\nfilter() can filter data based on given values.\nWith ifelse(), you can output values depending on a logical statement.\nUse logical operators to create logical statements: ==, !=, &gt;, &lt;.\nunique() returns the unique values of a vector.\nThe %in% operator returns TRUE if the value on the operator’s left-hand side is one of the values in the vector on the right-hand side, and FALSE if not.\n\nPlotting\n\nUse geom_density() to create density plots.\nUse geom_violin() to create violin plots.\nUse geom_jitter() to create strip charts.\nlabs() allows you to specify title and axis labels.\ntheme(legend.position = …) lets you specify the position of the legend.\n\nModelling\n\nUse brm() with family = gaussian() to model the mean and standard deviation of Gaussian data.\nsummary() returns the summary of a brm model.\nWith tidy() from broom.mixed you can obtain a tidy table with the model summary."
  },
  {
    "objectID": "tutorials/tutorial-w02.html",
    "href": "tutorials/tutorial-w02.html",
    "title": "QML tutorial - Week 2",
    "section": "",
    "text": "In the tutorial last week you’ve been playing around with R and variables.\nBut what if you want to import data in R?\nEasy! You can use the read_*() functions to read your files into R. But before we dive in, let’s first talk about some computer basics. (You can skip this section if it’s too basic for you)\n\n\nFiles saved on your computer live in a specific place. For example, if you download a file from a browser (like Google Chrome, Safari or Firefox), the file is normally saved in the Download folder.\nBut where does the Download folder live? Usually, in your user folder! The user folder normally is the name of your account or a name you picked when you created your computer account. In my case, my user folder is simply called ste.\n\n\n\n\n\n\nUser folder\n\n\n\nThe user folder is the folder with the name of your account.\n\n\n\n\n\n\n\n\nHow to find your user folder name\n\n\n\n\n\nOn macOS\n\nOpen the Finder Preferences.\nGo to Sidebar.\nThe name next to the house icon is the name of your home folder.\n\nOn Windows\n\nRight-click an empty area on the navigation panel in File Explorer.\nFrom the context menu, select the ‘Show all folders’ and your user profile will be added as a location in the navigation bar.\n\n\n\n\nSo, let’s assume I download a file, let’s say big_data.csv, in the Download folder of my user folder.\nNow we can represent the location of the big_data.csv file like so:\nste/\n└── Downloads/\n    └── big_data.csv\nTo mark that ste and Downloads are folders, we add a final forward slash /. That simply means “hey! I am a folder!”. big_data.csv is a file, so it doesn’t have a final /.\nInstead, the file name big_data.csv has a file extension. The file extension is .csv. A file extension marks the type of file: in this the big_data file is a .csv file, a comma separated value file (we will see an example of what that looks like later).\nDifferent file type have different file extensions:\n\nExcel files: .xlsx.\nPlain text files: .txt.\nImages: .png, .jpg, .gif.\nAudio: .mp3, .wav.\nVideo: .mp4, .mov, .avi.\nEtc…\n\n\n\n\n\n\n\nFile extension\n\n\n\nA file extension is a sequence of letters that indicates the type of a file and it’s separated with a . from the file name.\n\n\n\n\n\nNow, we can use an alternative, more succinct way, to represent the location of the big_data.csv:\nste/Downloads/big_data.csv\nThis is called a file path! It’s the path through folders that lead you to the file. Folders are separated by / and the file is marked with the extension .csv.\n\n\n\n\n\n\nFile path\n\n\n\nA file path indicates the location of a file on a computer as a path through folders that lead you to the file.\n\n\nNow the million pound question: where does ste/ live on my computer???\nUser folders are located in different places depending on the operating system you are using:\n\nOn macOS: the user folder is in /Users/.\n\nYou will notice that there is a forward slash also before the name of the folder. That is because the /Users/ folder is a top folder, i.e. there are no folders further up in the hierarchy of folders.\nThis means that the full path for the big_data.csv file on a computer running macOS would be: /Users/ste/Downloads/big_data.csv.\n\nOn Windows: the user folder is in C:/Users/\n\nYou will notice that C is followed by a colon :. That is because C is a drive, which contains files and folders. C: is not contained by any other folder, i.e. there are no other folders above C: in the hierarchy of folders.\nThis means that the full path for the big_data.csv file on a Windows computer would be: C:/Users/ste/Downloads/big_data.csv.\n\n\nWhen a file path starts from a top-most folder, we call that path the absolute file path.\n\n\n\n\n\n\nAbsolute path\n\n\n\nAn absolute path is a file path that starts with a top-most folder.\n\n\nThere is another type of file paths, called relative paths. A relative path is a partial file path, relative to a specific folder. You will learn how to use relative paths below, when we will go through importing files in R using R scripts below.\nImporting files in R is very easy with the tidyverse packages. You just need to know the file type (very often the file extension helps) and the location of the file (i.e. the file path).\nThe next sections will teach you how to import data in R!\n\n\n\n\n\n\nQuiz 1\n\n\n\n\nWhich of the following paths is an impossible path?\n\n Downloads/courses/qml/data/ /Users/smruti/Downloads/data.csv/files/ sascha/Documents/files_pdf/paper.pdf"
  },
  {
    "objectID": "tutorials/tutorial-w02.html#some-basics",
    "href": "tutorials/tutorial-w02.html#some-basics",
    "title": "QML tutorial - Week 2",
    "section": "",
    "text": "In the tutorial last week you’ve been playing around with R and variables.\nBut what if you want to import data in R?\nEasy! You can use the read_*() functions to read your files into R. But before we dive in, let’s first talk about some computer basics. (You can skip this section if it’s too basic for you)\n\n\nFiles saved on your computer live in a specific place. For example, if you download a file from a browser (like Google Chrome, Safari or Firefox), the file is normally saved in the Download folder.\nBut where does the Download folder live? Usually, in your user folder! The user folder normally is the name of your account or a name you picked when you created your computer account. In my case, my user folder is simply called ste.\n\n\n\n\n\n\nUser folder\n\n\n\nThe user folder is the folder with the name of your account.\n\n\n\n\n\n\n\n\nHow to find your user folder name\n\n\n\n\n\nOn macOS\n\nOpen the Finder Preferences.\nGo to Sidebar.\nThe name next to the house icon is the name of your home folder.\n\nOn Windows\n\nRight-click an empty area on the navigation panel in File Explorer.\nFrom the context menu, select the ‘Show all folders’ and your user profile will be added as a location in the navigation bar.\n\n\n\n\nSo, let’s assume I download a file, let’s say big_data.csv, in the Download folder of my user folder.\nNow we can represent the location of the big_data.csv file like so:\nste/\n└── Downloads/\n    └── big_data.csv\nTo mark that ste and Downloads are folders, we add a final forward slash /. That simply means “hey! I am a folder!”. big_data.csv is a file, so it doesn’t have a final /.\nInstead, the file name big_data.csv has a file extension. The file extension is .csv. A file extension marks the type of file: in this the big_data file is a .csv file, a comma separated value file (we will see an example of what that looks like later).\nDifferent file type have different file extensions:\n\nExcel files: .xlsx.\nPlain text files: .txt.\nImages: .png, .jpg, .gif.\nAudio: .mp3, .wav.\nVideo: .mp4, .mov, .avi.\nEtc…\n\n\n\n\n\n\n\nFile extension\n\n\n\nA file extension is a sequence of letters that indicates the type of a file and it’s separated with a . from the file name.\n\n\n\n\n\nNow, we can use an alternative, more succinct way, to represent the location of the big_data.csv:\nste/Downloads/big_data.csv\nThis is called a file path! It’s the path through folders that lead you to the file. Folders are separated by / and the file is marked with the extension .csv.\n\n\n\n\n\n\nFile path\n\n\n\nA file path indicates the location of a file on a computer as a path through folders that lead you to the file.\n\n\nNow the million pound question: where does ste/ live on my computer???\nUser folders are located in different places depending on the operating system you are using:\n\nOn macOS: the user folder is in /Users/.\n\nYou will notice that there is a forward slash also before the name of the folder. That is because the /Users/ folder is a top folder, i.e. there are no folders further up in the hierarchy of folders.\nThis means that the full path for the big_data.csv file on a computer running macOS would be: /Users/ste/Downloads/big_data.csv.\n\nOn Windows: the user folder is in C:/Users/\n\nYou will notice that C is followed by a colon :. That is because C is a drive, which contains files and folders. C: is not contained by any other folder, i.e. there are no other folders above C: in the hierarchy of folders.\nThis means that the full path for the big_data.csv file on a Windows computer would be: C:/Users/ste/Downloads/big_data.csv.\n\n\nWhen a file path starts from a top-most folder, we call that path the absolute file path.\n\n\n\n\n\n\nAbsolute path\n\n\n\nAn absolute path is a file path that starts with a top-most folder.\n\n\nThere is another type of file paths, called relative paths. A relative path is a partial file path, relative to a specific folder. You will learn how to use relative paths below, when we will go through importing files in R using R scripts below.\nImporting files in R is very easy with the tidyverse packages. You just need to know the file type (very often the file extension helps) and the location of the file (i.e. the file path).\nThe next sections will teach you how to import data in R!\n\n\n\n\n\n\nQuiz 1\n\n\n\n\nWhich of the following paths is an impossible path?\n\n Downloads/courses/qml/data/ /Users/smruti/Downloads/data.csv/files/ sascha/Documents/files_pdf/paper.pdf"
  },
  {
    "objectID": "tutorials/tutorial-w02.html#the-data",
    "href": "tutorials/tutorial-w02.html#the-data",
    "title": "QML tutorial - Week 2",
    "section": "2 The data",
    "text": "2 The data\nFor this tutorial, we will use the data from this paper: Song et al. 2020. Second language users exhibit shallow morphological processing. DOI: 10.1017/S0272263120000170.\nThe study consisted of a lexical decision task in which participants where first shown a prime, followed by a target word for which they had to indicate whether it was a real word or a nonce word.\nThe prime word belonged to one of three possible groups (Relation_type in the data) each of which refers to the morphological relation of the prime and the target word:\n\nUnrelated: for example, prolong (assuming unkindness as target, [[un-kind]-ness]).\nConstituent: unkind.\nNonConstituent: kindness.\n\nThe expectations are that\n\nLexical decisions for native English participants should be facilitated in the Constituent condition, but not in the Unrelated and NonConstituent conditions.\nLexical decisions for non-native English participants should be facilitated both in the Constituent and NonCostituent condition, but not in the Unrelated condition.\n\nWe can interpret “facilitated lexical decisions” as higher accuracy and shorter reaction times.\nIn section 2-4 of this tutorial you will learn about:\n\nThe data file format (a .csv file).\nHow to read data with the read_csv() function from the tidyverse packages and what packages are.\nHow to view the imported data.\n\n\n2.1 Download the data file\nPlease, follow these instructions carefully.\n\nDownload the file with the data by right-clicking on the following link and download the file: shallow.csv. (Note that tutorial files are also linked in the Course content page)\nCreate a folder called data/ (the slash is there just to remind you that it’s a folder, but you don’t have to include it in the name) in the RStudio project of the course.\n\nTo create a folder, go to the Files tab of the bottom-right panel in RStudio.\nMake sure you are viewing the project’s main folder.\nClick on the New Folder button, enter “data” in the text box and click OK\n\nMove the downloaded file into the data/ folder.\n\nOpen a Finder or File Explorer window.\nNavigate to the folder where you have saved the downloaded file (it will very likely be the Downloads/ folder).\nCopy the file.\nIn Finder or File Explorer, navigate to the RStudio project folder, then the data/ folder, and paste the file in there.\n\n\nThe rest of the tutorial will assume that you have created a folder called data/ in the RStudio project folder and that shallow.csv is in that folder.\nI recommend you to start being very organised with your files from now, whether it’s for this course or your dissertation or else. I also suggest to avoid overly nested structures (for example, avoid having one folder for each week for this course. Rather, save all data files in the data/ folder).\n\n\n\n\n\n\nOrganising your files\n\n\n\n\n\nThe Open Science Framework has the following recommendations that apply very well to any type of research project.\n\nUse one folder per project. This will also be your RStudio project folder.\nSeparate raw data from derived data.\nSeparate code from data.\nMake raw data read-only.\n\nTo learn more about this, check the OSF page Organising files.\nIn brief, what these recommendations mean is that you want a folder for your research project/course/else, and inside the folder two folders: one for data and one for code.\nThe data/ folder could further contain raw/ for raw data (data that should not be lost or changed, for example collected data or annotations) and derived/ for data that derives from the raw data, for example through automated data processing.\nI usually also have a separate folder called figs/ or img/ where I save plots. Of course which folders you will have it’s ultimately up to you and needs will vary depending on the project and field!\n\n\n\n\n\n2.2 About shallow.csv\nshallow.csv is a .csv file. It looks like this (the following is just an excerpt from the file, specifically the first 5 lines):\nGroup,ID,List,Target,ACC,RT,logRT,Critical_Filler,Word_Nonword,Relation_type,Branching\nL1,L1_01,A,banoshment,1,423,6.0474,Filler,Nonword,Phonological,NA\nL1,L1_01,A,unawareness,1,603,6.4019,Critical,Word,Unrelated,Left\nL1,L1_01,A,unholiness,1,739,6.6053,Critical,Word,Constituent,Left\nL1,L1_01,A,bictimize,1,510,6.2344,Filler,Nonword,Phonological,NA\nA .csv file is basically a format to save tabular data (i.e. data that looks like a table). To separate each column, a .csv file uses a comma , (hence the name “comma separated values”).\nThe first line of the file indicates the names of the columns of the table:\nGroup,ID,List,Target,ACC,RT,logRT,Critical_Filler,Word_Nonword,Relation_type,Branching\nThere are 11 columns.\nThen, the rest of the file contains the other rows of the table, with values for each of the 11 columns. Of course, separated by commas.\nL1,L1_01,A,banoshment,1,423,6.0474,Filler,Nonword,Phonological,NA\nL1,L1_01,A,unawareness,1,603,6.4019,Critical,Word,Unrelated,Left\nL1,L1_01,A,unholiness,1,739,6.6053,Critical,Word,Constituent,Left\nL1,L1_01,A,bictimize,1,510,6.2344,Filler,Nonword,Phonological,NA\nThe shallow.csv file should be placed in the data/ folder of the course RStudio project.\nNow, let’s import the data!\nImporting .csv files is very easy. You can use the read_csv() function from a collection of R packages known as the tidyverse.\nBut what are R packages? Find out in the following section."
  },
  {
    "objectID": "tutorials/tutorial-w02.html#r-packages",
    "href": "tutorials/tutorial-w02.html#r-packages",
    "title": "QML tutorial - Week 2",
    "section": "3 R packages",
    "text": "3 R packages\nWhen you install R, a library of packages is also installed. Packages are like plug-ins, they provide R with extra functionalities. The default R library contains all of the base R packages.\nYou can check all of the currently installed packages in the bottom-right panel of RStudio, in the Packages tab.\nIf you want to find the path of the R library on your computer, type .libPaths() in the Console. The function returns (i.e. outputs) the path or paths where your R library is.\n\n\n\n\n\n\nR library and packages\n\n\n\n\nThe R library contains the base R packages and all user-installed packages.\nR packages provide R with extra functionalities and are installed into the R library.\n\n\n\n\n3.0.1 Install packages\nYou can install extra packages in the R library in two ways:\n\nYou can use the install.packages() function. This function takes the name of package you want to install as a string, for example install.packages(\"cowsay\")\nOr you can go the Packages tab in the bottom-right panel of RStudio and click on Install.\n\nGo ahead and try to install a package using the second method. Install the fortunes package (remember to quote the name of the package). After installing you will see that the package fortunes is listed in the Packages tab.\n\n\n\n\n\n\nWarning\n\n\n\nYou need to install a package ONLY ONCE! Once installed, it’s there for ever, saved in the R library. You will be able to use all of your installed packages in any RStudio project you create.\n\n\n\n\n3.0.2 Attach packages\nNow, to use a package you need to attach the package with the library() function.\n\n\n\n\n\n\nWarning\n\n\n\nYou need to attach the package you want to use once per R session.\n\n\nLet’s attach the fortunes package. Run the following code in the Console. Note that while install.packages(\"fortunes\") takes the name of the package as a string, library(fortunes) takes the name of the package without quotes.\n\nlibrary(fortunes)\n\nNow you can use the functions provided by the attached package. Try it out!\n\nfortune()\n\n\nTom Backer Johnsen: I have just started looking at R, and are getting more and\nmore irritated at myself for not having done that before. However, one of the\nthings I have not found in the documentation is some way of preparing output\nfrom R for convenient formatting into something like MS Word.\nBarry Rowlingson: Well whatever you do, don't start looking at LaTeX, because\nthat will get you even more irritated at yourself for not having done it before.\n   -- Tom Backer Johnsen and Barry Rowlingson\n      R-help (February 2006)\n\n\nTo learn what a function does, you can check its documentation by typing the function name preceded by a ? question mark. Type ?function in the Console and hit ENTER to see the function documentation.\n\n\n\n\n\n\nWarning\n\n\n\nRemember, you need to install a package only once but you need to attach it with library() every time you start R.\nThink of install.packages() as mounting a light bulb (installing the package) and library() as the light switch (attaching the package).\n\n\n\n\n\n\n\n\n\nQuiz 2\n\n\n\n\nWhich of the following statements is wrong?\n\n You attach libraries with library(). install.packages() does not load packages. The R library is a folder."
  },
  {
    "objectID": "tutorials/tutorial-w02.html#import-and-view-data",
    "href": "tutorials/tutorial-w02.html#import-and-view-data",
    "title": "QML tutorial - Week 2",
    "section": "4 Import and view data",
    "text": "4 Import and view data\n\n4.1 Import the data\nTo import data in R we will use the read_csv() function from the readr package, one of the tidyverse packages.\nThe read_csv() function only requires you to specify the file path as a string (remember, strings are quoted between \" \", for example \"year_data.txt\").\nOn my computer, the file path of shallow.csv is /Users/ste/qml/data/shallow.csv, but on your computer the file path will be different, of course.\nAlso, note that it is not enough to use the read_csv() function. You also must assign the output of the read_csv() function (i.e. the data we are reading) to a variable, using the assignment arrow &lt;-.\nAnd since the read_csv() is a function from the tidyverse, you first need to attach the tidyverse packages with library(tidyverse) (remember, you need to attach packages only once per session).\nThis will attach a set of the tidyverse packages that are commonly used in most data analysis applications, including readr. Of course, you can also attach the individual packages directly: library(readr). If you use library(tidyverse) there is no need to attach individual tidyverse packages.\nSo, putting all together, here’s the code. Remember to change the file path to match the path of the file on your computer. Run the code in the Console to read the data.\n\nlibrary(tidyverse)\n\n# CHANGE THE FILE PATH TO MATCH THE PATH ON YOUR COMPUTER\nshallow &lt;- read_csv(\"/Users/ste/qml/data/shallow.csv\")\n\nFantastic!\n\n\n4.2 View the data\nNow we can view the data.\nThe easiest way is to click on the name of the data listed in the Environment tab, in the top-right panel of RStudio.\nYou will see a nicely formatted table, as you would in a programme like Excel.\nData tables in R (i.e. spreadsheet-like data) are called data frames or tibbles.1\nThe shallow data frame contains 11 columns (called variables in the Environment tab). The 11 columns are the following:\n\nGroup: L1 vs L2 speakers of English.\nID: Subject unique ID.\nList: Word list (A to F).\nTarget: Target word in the lexical decision trial.\nACC: Lexical decision response accuracy (0 incorrect response, 1 correct response).\nRT: Reaction times of response in milliseconds.\nlogRT: Logged reaction times.\nCritical_Filler: Whether the trial was a filler or critical.\nWord_Nonword: Whether the Target was a real Word or a Nonword.\nRelation_type: The type of relation between prime and target word (Unrelated, NonCostituent, Constituent, Phonological).\nBranching: Constituent syntactic branching, Left and Right (shout out to Charlie Puth).\n\n\n\n\n\n\n\nQuiz 3\n\n\n\n\nHow many rows does shallow have?\n\n 11 650 6500\n\n\n\n\nGreat stuff! See how easy it was to import data?\n\n\n\n\n\n\nData formats\n\n\n\nMost of the data we will be using in this course will be in the .csv format.\nThe comma separated values format (.csv) is the best format to save data in because it is basically a plain text file, it’s quick to parse, and can be opened and edited with any software (plus, it’s not a proprietary format).\n\n\nHowever, you can easily import other file types as long as they are tabular or rectangular (i.e. spreadsheet-like), like Excel files.\n\n\n\n\n\n\nHow to import other file types\n\n\n\n\n\nSometimes though you might have to read other file types, for example Excel spreadsheets.\nIt’s easy to read other file type using the other read_*() functions:\n\nFor Excel files (i.e. files with a .xls or .xlsx extension), you need to attach the readxl package with library(readxl) and use the read_excel() function.\nUse read_tsv() from the readr package (installed with the tidyverse) to read tab separated values files (these have either the .txt or .tsv extensions.\nThe flexible read_delim() function allows you to read a wide variety of delimited files.\n\n\n\n\n\n\n\n\n\n\nPractice 1\n\n\n\n\n\nDownload the following files into the data folder. Read them in R, making sure you use the right read_*() function.\n\ntakete_maluma.txt (a tab separated file)\nRelatives.xlsx\nGo to https://datashare.ed.ac.uk/handle/10283/4006 and download the file conflict_data_.xlsx. Read both sheets (“conflict_data2” and “demographics”). Any issues?"
  },
  {
    "objectID": "tutorials/tutorial-w02.html#r-scripts",
    "href": "tutorials/tutorial-w02.html#r-scripts",
    "title": "QML tutorial - Week 2",
    "section": "5 R scripts",
    "text": "5 R scripts\nSo far, you’ve been asked to write code in the Console and run it there.\nBut this is not very efficient. Every time, you need to write the code and execute it in the right order and it quickly becomes very difficult to keep track of everything when things start getting more involved.\nA solution is to use R scripts.\n\n\n\n\n\n\nR script\n\n\n\nAn R script is a file with the .R extension that contains R code.\n\n\nFor the rest of this tutorial, you will write all code in an R script.\n\n5.1 Create an R script\nFirst, create a folder called code in your RStudio project folder. This will be the folder where you will save all of your R scripts.\nNow, to create a new R script, look at the top-left corner of RStudio: the first button to the left looks like a white sheet with a green plus sign. This is the New file button. Click on that and you will see a few options to create a new file.\nClick on R Script. A new empty R script will be created and will open in the File Editor window of RStudio.\n\nNote that creating an R script does not automatically saves it on your computer. To do so, either use the keyboard short-cut CMD+S/CTRL+S or click on the floppy disk icon in the menu below the file tab.\n\nSave the file inside the code/ folder with the following name: tutorial-w02.R.\n\n\n\n\n\n\nWarning\n\n\n\nRemember that all the files of your RStudio project don’t live inside RStudio but on your computer.\nSo you can always access them from the Finder or File Explorer! However, do not open a file by double clicking on it from the Finder/File Explorer.\nRather, open the RStudio project by double clicking on the .Rproj file and then open files from RStudio to ensure you are working within the RStudio project and the working directory is set correctly. (See the tutorial from Week 1).\n\n\n\n\n5.2 Write code\nNow, let’s start filling up that script!\nGenerally, you start the script with calls to library() to load all the packages you need for the script.\nNow we only need one package, tidyverse, but in most cases you will need more than one! The best practice is to attach all of packages first, in the top of your script. Please, get in the habit of doing this from now, so that you can keep your scripts tidy and pretty!\nGo ahead, write the following code in the top of the tutorial-w02.R script.\n\nlibrary(tidyverse)\n\nshallow &lt;- read_csv(\"./data/shallow.csv\")\n\nWait, what is that \"./data/shallow.csv\"? That’s a relative path. We briefly mentioned relative paths above, but let’s understand the details now.\n\n\n\n\n\n\nWarning\n\n\n\nPlease, don’t include install.packages() in your R scripts!\nRemember, you only have to install a package once, and you can just type it in the Console.\nBut DO include library() in your scripts.\n\n\n\n\n5.3 Relative paths\n\n\n\n\n\n\nRelative path\n\n\n\nA relative path is a file path that is relative to a folder, which is represented by ./\n\n\nWhen you are using R scripts in RStudio projects, the ./ folder paths are relative to is the project folder! This is true whichever the name of the folder/project and whichever it’s location on your computer.\nFor example, if your project it’s called awesome_proj and it’s in Downloads/stuff/, then if you write ./data/results.csv you really mean Downloads/stuff/awesome_proj/data/results.csv!\nHow does R know the path is relative to the project folder?\nThat is because when working with RStudio projects, all relative paths are relative to the project folder (i.e. the folder with the .Rproj file)!\nThe folder which relative paths are relative to is called the working directory (directory is just another way of saying folder).\n\n\n\n\n\n\nWorking directory\n\n\n\nThe working directory is the folder which relative paths are relative to.\nWhen using RStudio projects, the working directory is the project folder.\n\n\nThe code read_csv(\"./data/shallow.csv\") above will work because you are using an RStudio project and inside the project folder there is a folder called data/ and in it there’s the shallow.csv file.\nSo from now on I encourage you to use RStudio projects, R scripts and relative paths always!\nThe benefit of doing so is that, if you move your project or rename it, or if you share the project with somebody, all the paths will just work because they are relative!\n\n\n\n\n\n\nGet the working directory\n\n\n\n\n\nYou can get the current working directory with the getwd() command.\nRun it now in the Console! Is the returned path the project folder path?\nIf not, it might be that you are not working from the RStudio project. Check the top-right corner of RStudio: is the project name in there or do you see Project (none)?\nIf it’s the latter, you are not in an RStudio project, but you are running R from somewhere else (meaning, the working directory is somewhere else).\n\n\n\n\n\n5.4 Run the script!\nFinally, the time has come to run the script.\nThere are several ways of doing this. The most straightforward is to click on the Run button. You can find this in the top-right corner of the script window.\n\nAn alternative way is to place the text cursor on the line of code you want to run and then press CMD+ENTER/CTRL+ENTER. This will run the line of code and move the text cursor to the next line of code.\nYou can even select multiple lines of code (as you would select text) and press CMD+ENTER/CTRL+ENTER to run multiple lines of code!\nNow that you know how to use R scripts and run code in them, I will assume that you will keep writing new code from this tutorial in your script and run it from there!\nJust another tip: sometimes we might want to add a few lines of text in our script, for example to take notes.\nYou can add so-called comments in R scripts, simply by starting a line with #.\nFor example:\n\n# This is a comment. Let's add 6 + 3.\n6 + 3\n\n[1] 9\n\n\n\n\n\n\n\n\nQuiz 4\n\n\n\n\nIs the following a valid line of R code? TRUEFALSE\n\nsum(x + 2) # x = 4"
  },
  {
    "objectID": "tutorials/tutorial-w02.html#data-transformation",
    "href": "tutorials/tutorial-w02.html#data-transformation",
    "title": "QML tutorial - Week 2",
    "section": "6 Data transformation",
    "text": "6 Data transformation\nData transformation is a fundamental aspect of data analysis.\nAfter the data you need to use is imported into R, you will have to filter rows, create new columns, or join data frames, among many other transformation operations.\nIn this tutorial we will learn how to obtain summary measures and how to count occurrences using the summarise() and count() functions.\n\n6.1 Summary measures\nDuring the lecture, we have learnt two types of measures.\n\n\n\n\n\n\nSummary measures\n\n\n\nMeasures of central tendency (mean, median, mode) indicate the typical or central value of a sample.\nMeasures of dispersion (min-max, range, standard deviation) indicate the dispersion of the sample values around the central tendency value.\n\n\nWhen you work with data, you always want to get summary measures for most of the variables in the data.\nData reports usually include summary measures. It is also important to understand which summary measure is appropriate for which type of variable.\nWe have covered this in the lecture, so we won’t go over it again here. Instead, you will learn how to obtain summary measures using the summarise() function from the dplyr tidyverse package.\nsummarise() takes at least two arguments:\n\nThe data frame to summarise.\nOne or more summary functions.\n\nFor example, let’s get the mean the reaction time column RT. Easy!\n\nsummarise(shallow, RT_mean = mean(RT))\n\n\n\n  \n\n\n\nGreat! The mean reaction times of the entire sample is 867 ms.\nWhat if we want also the standard deviation? Easy!\n\nsummarise(shallow, RT_mean = mean(RT), RT_sd = sd(RT))\n\n\n\n  \n\n\n\n\n# This is a comment. Let's add 6 + 3.\n6 + 3\n\n[1] 9\n\n\nNow we know that reaction times are on average 867 ms long and have a standard deviation of about 293 ms (rounded to the nearest integer).\nLet’s go all the way and also get the minimum and maximum RT values.\n\nsummarise(\n  shallow,\n  RT_mean = mean(RT), RT_sd = sd(RT),\n  RT_min = min(RT), RT_max = max(RT)\n)\n\n\n\n  \n\n\n\nFab! When writing a data report, you could write something like this.\n\nReaction times are on average 867 ms long (SD = 293 ms), with values ranging from 0 to 1994 ms.\n\nWe will learn more about standard deviations from Week 4 on, but for now just think of this as a relative measure of how dispersed the data are around the mean: the higher the SD, the greater the dispersion around the mean, i.e. the greater the variability in the data.\nWhen required, you can use the median() function to calculate the median, instead of the mean(). Go ahead and calculate the median reaction times in the data. Is it similar to the mean?\n\n\n\n\n\n\nQuiz 5\n\n\n\n\n\nWhat does the na.rm argument of mean() do?\n\n It changes NAs to FALSE. It converts NAs to 0s. It removes NAs before taking the mean.\n\nWhich is the mean of c(4, 23, NA, 5) when na.rm has the default value?\n\n NA. 0. 10.66.\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nCheck the documentation of ?mean.\n\n\n\n\n\nNote that R has a mode() function, but alas this is not the statistical mode. To get the mode of a categorical variable you can just count the occurrences of the values of that variable and the value that occurs the most is the mode!\nKeep reading to learn how to count occurrences.\n\n\n6.2 Count occurrences\nWe can use the count() function from the dplyr tidyverse package to count the number of occurrences for each value of a specific column. Let’s count how many trials are correct, i.e. let’s count occurrences in the ACC column.\nAccuracy has been coded with 0 for incorrect and 1 for correct. We will see how this is not an ideal, although very common way, of coding binary variables. For now let’s keep it as is.\nThe function count() takes the name of tibble and the name of column you want to count values in.\n\ncount(shallow, ACC)\n\nHow many correct responses are there in the shallow tibble?\nNote that you can add multiple column names, separated by commas, to get counts for the combinations of values of each column.\nTry to get counts of the combination of ACC and Group (L1 vs L2 participants). Replace ... with the right code.\n\ncount(shallow, ...)\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nIn count(), include the names of the two columns you want to get counts of, separated by commas.\n\n\n\nAre there differences in accuracy between the L1 and L2 group?\n\n\n6.3 Grouping data\nSometimes you might want to get summary measures for one variable depending on different values of another variable.\nYou can use the group_by() function from the dplyr tidyverse package, together with summarise() to achieve that. Let’s see how it works.\n\ngroup_by(shallow, Group) %&gt;%\n  summarise(\n    RT_mean = mean(RT),\n    RT_sd = sd(RT)\n  )\n\n\n\n  \n\n\n\nThe group_by() function takes at least two arguments:\n\nThe name of the tibble to group.\nThe name of the columns to group the tibble by, separated by commas.\n\nHere we are grouping shallow by Group.\nIf you are baffled by that %&gt;%, keep on reading.\n\n\n6.4 What the pipe?!\nWait, what is that thing, %&gt;%?\nIt’s called a pipe. Think of a pipe as a teleporter.\nIn the code above we are chaining two functions together using the pipe opeartor %&gt;%. The output of the first function (group_by()) is “teleported” into the second function summarise().\nThe pipe %&gt;% teleports the output of the preceding function as the first argument of the following function. The output of group_by is a (grouped) tibble, and summarise() needs a tibble as its first argument.\nThat’s why the code above works!\nIn fact, you can even use a pipe for the tibble of group_by(), like so:\n\nshallow %&gt;%\n  group_by(Group) %&gt;%\n    summarise(\n      RT_mean = mean(RT),\n      RT_sd = sd(RT)\n    )\n\n\n\n  \n\n\n\nYou see that the output of the code is the same here as it is above.\nFor comparison, this is what the code would look like without the pipe.\n\ngrouped_shallow &lt;- group_by(shallow, Group)\n\nsummarise(\n  grouped_shallow,\n  RT_mean = mean(RT),\n  RT_sd = sd(RT)\n)\n\n\n\n  \n\n\n\nDon’t worry too much if the concept of the pipe is not clear yet. It should become clearer later.\n\n\n\n\n\n\nQuiz 6\n\n\n\n\n\nWhich of the following returns the number of words in shallow?\n\n count(shallow, Target). shallow %&gt;% distinct(Target) %&gt;% count(). shallow %&gt;% count(Target).\n\nThe output of summarise() always drops the grouping. TRUEFALSE\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nFor 6b., check the documentation of ?summarise.\n\n\n\n\n\n\n\n\n\n\n\nPractice 2\n\n\n\n\n\n\nGo to https://github.com/stefanocoretta/infant-gestures and download the gestures.csv file in the data/ folder. This is data from the paper [A Cross-Cultural Analysis of Early Prelinguistic Gesture Development and Its Relationship to Language Development](https://doi.org/10.1111/cdev.13406).\nRead the file in R.\nCalculate the following:\n\nMeasure of central tendency and dispersion for the count column (it contains the number of gestures performed by each child in different tasks).\nMeasure of central tendency and dispersion for the countcolumn grouped by month (the child’s age).\nTotal number of gestures by children (dyad).\nNumber of children by background.\n\nWrite a paragraph where you report the measures.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nTo calculate the total number of gestures by children, you need the sum() function.\nTo calculate the number of children by background, you need the distinct() function.\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nHave you tried doing the exercise and couldn’t work it out?\nThe you can check the code solution here…\n\n\n\n\n\n\nCode\n\n\n\n\n\ngestures %&gt;%\n  summarise(\n    count_med = median(count, na.rm = TRUE),\n    count_min = min(count, na.rm = TRUE),\n    count_max = max(count, na.rm = TRUE),\n    count_range = count_max - count_min\n  )\n\ngestures %&gt;%\n  group_by(months) %&gt;%\n  summarise(\n    count_med = median(count, na.rm = TRUE),\n    count_min = min(count, na.rm = TRUE),\n    count_max = max(count, na.rm = TRUE),\n    count_range = count_max - count_min\n  )\n\ngestures %&gt;%\n  group_by(dyad) %&gt;%\n  summarise(\n    count_tot = sum(count)\n  )\n\ngestures %&gt;%\n  distinct(background, dyad) %&gt;%\n  count(background)"
  },
  {
    "objectID": "tutorials/tutorial-w02.html#summary",
    "href": "tutorials/tutorial-w02.html#summary",
    "title": "QML tutorial - Week 2",
    "section": "7 Summary",
    "text": "7 Summary\nYou made it! You completed this week’s tutorial.\nHere’s a summary of the R functions you learnt.\n\n\n\n\n\n\nlibrary() attaches R packages.\nread_csv() reads .csv files in R.\nsummarise() allows you to calculate measures of central tendency and dispersion (with mean(), median(), min() and max()).\ncount() lets you count the number of occurrences of levels in a categorical variable.\ngroup_by() allows you to group a tibble according to one or more variables.\nThe pipe %&gt;% can be used to chain multiple functions."
  },
  {
    "objectID": "tutorials/tutorial-w02.html#footnotes",
    "href": "tutorials/tutorial-w02.html#footnotes",
    "title": "QML tutorial - Week 2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA tibble is a special data frame. We will learn more about tibbles in the following weeks.↩︎"
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Set-up instructions",
    "section": "",
    "text": "Please, follow these instructions to get ready before the first workshop. It will take up to an hour to set everything up and you might encounter errors, so please do this asap."
  },
  {
    "objectID": "setup.html#r-and-rstudio",
    "href": "setup.html#r-and-rstudio",
    "title": "Set-up instructions",
    "section": "1 R and RStudio",
    "text": "1 R and RStudio\nBefore installing the necessary software, make sure you have installed or updated the following software.\n\nThe latest version of R (https://cloud.r-project.org).\nThe latest version of RStudio (https://www.rstudio.com/products/rstudio/download/#download).\nIdeally, your operating system is up-to-date."
  },
  {
    "objectID": "setup.html#r-packages",
    "href": "setup.html#r-packages",
    "title": "Set-up instructions",
    "section": "2 R packages",
    "text": "2 R packages\n\n\n\n\n\n\nImportant\n\n\n\nIf you have previously installed the C++ toolkit, CmdStan and/or cmdstanr, or if you have recently updated your OS, please follow these instructions to reinstall them.\n\n\nNow you will need to install a few packages and extra software.\nHere is an overview of what you will install:\n\nC++ toolchain.\nR packages: tidyverse, remotes, cmdstanr, brms.\nCmdStan (including Stan).\n\n\n2.1 Install the C++ toolchain\nSome of the software (CmdStan) used in the workshops require a working C++ toolchain for compilation.\nYou can find information on how to set up the C++ toolchain in Section 1.2.1 of the CmdStan User’s Guide.\nMake sure to follow the instructions for your operating system.\n\n\n2.2 Install the R packages\nAfter you have installed the C++ toolchain, you need to install the following R packages. Run the following in the Console.\ninstall.packages(\"remotes\")\nremotes::install_github(\"stan-dev/cmdstanr\")\ninstall.packages(c(\"brms\"))\ninstall.packages(c(\"tidybayes\", \"extraDistr\"))\nThe cmdstanr package is an interface between R and CmdStan (see below), while brms is the package you will use to run Bayesian linear models (think of it as the Bayesian equivalent of lme4).\nIt will take several minutes to install the packages, depending on your system and configuration.\nIf at any point you get asked about installing extra packages or software or update existing packages or software, please do so. In all cases, carefully read the message in the Console or in a pop-up window: they always tell you what to do or the options you have.\n\n\n2.3 Install CmdStan\nNow that you have installed the R packages, you need to install CmdStan.\nCmdStan is a shell interface to the programming language Stan.\nStan is what runs the Bayesian models, but you don’t have to know Stan, because you will use the R packages to run the models (those packages will communicate with Stan in your stead).\nTo install CmdStan, run the following command in the R console:\ncmdstanr::install_cmdstan(cores = parallel::detectCores(), overwrite = TRUE)\nIt will take several minutes and you will see a lot of text flashing in the Console.\n\n\n2.4 Check your installation\nRun the following in the RStudio Console.\nlibrary(cmdstanr)\nfile &lt;- file.path(cmdstan_path(), \"examples\", \"bernoulli\", \"bernoulli.stan\")\nmod &lt;- cmdstan_model(file, force_recompile = TRUE)\ndata_list &lt;- list(N = 10, y = c(0,1,0,0,0,0,0,0,0,1))\nfit &lt;- mod$sample(\n  data = data_list, \n  seed = 123, \n  chains = 4, \n  parallel_chains = 4,\n  refresh = 500 # print update every 500 iters\n)\nIf everything goes well you should see Compiling Stan program… and then Running MCMC with 4 parallel chains… followed by information on chains and iterations."
  },
  {
    "objectID": "setup.html#troubleshoot",
    "href": "setup.html#troubleshoot",
    "title": "Set-up instructions",
    "section": "3 Troubleshoot",
    "text": "3 Troubleshoot\nIf you have issues with any of these steps, please get in touch on Piazza. I won’t have time to help you during the workshop."
  },
  {
    "objectID": "content.html",
    "href": "content.html",
    "title": "QML",
    "section": "",
    "text": "Week\nTopic\n\n\n\n\n\n\n\n\n1\nQuantitative methods and uncertainty\nSlides\nTutorial\n\n\n\n\n2\nData wrangling\nSlides\nTutorial\n\n\n\n\n3\nData visualisation\nSlides\nTutorial\n\n\n\n\n4\nStatistical modeling basics\nSlides\nTutorial\n\n\n\n\n5\nCategorical predictors\nSlides\nTutorial\nF1\n\n\n\n6 Catch up\nNo classes\n\n\n\n\n\n\n7\nBinary outcomes\nSlides\nTutorial\n\n\n\n\n8\nMultiple predictors and interactions\nSlides\nTutorial\n\nS1\n\n\n9\nContinuous predictors\nSlides\nTutorial\n\n\n\n\n10\nBayesian inference\nSlides\nTutorial\nF2\n\n\n\n11\nObtaining p-values (optional)\nSlides\nTutorial\n\n\n\n\n12\n\n\n\n\nS2"
  },
  {
    "objectID": "content.html#schedule-overview",
    "href": "content.html#schedule-overview",
    "title": "QML",
    "section": "",
    "text": "Week\nTopic\n\n\n\n\n\n\n\n\n1\nQuantitative methods and uncertainty\nSlides\nTutorial\n\n\n\n\n2\nData wrangling\nSlides\nTutorial\n\n\n\n\n3\nData visualisation\nSlides\nTutorial\n\n\n\n\n4\nStatistical modeling basics\nSlides\nTutorial\n\n\n\n\n5\nCategorical predictors\nSlides\nTutorial\nF1\n\n\n\n6 Catch up\nNo classes\n\n\n\n\n\n\n7\nBinary outcomes\nSlides\nTutorial\n\n\n\n\n8\nMultiple predictors and interactions\nSlides\nTutorial\n\nS1\n\n\n9\nContinuous predictors\nSlides\nTutorial\n\n\n\n\n10\nBayesian inference\nSlides\nTutorial\nF2\n\n\n\n11\nObtaining p-values (optional)\nSlides\nTutorial\n\n\n\n\n12\n\n\n\n\nS2"
  },
  {
    "objectID": "content.html#weekly-schedule",
    "href": "content.html#weekly-schedule",
    "title": "QML",
    "section": "2 Weekly schedule",
    "text": "2 Weekly schedule\n\n2.1 Week 1: Quantitative methods and uncertainty\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nQuestions\n\nWhat is quantitative data analysis?\nWhat is the inference process?\nHow can we talk about uncertainty and variability?\nWhich are the limits of quantitative methods?\n\nSkills\n\nThink critically about statistics, uncertainty and variability.\nUse R to perform simple calculations.\nMaster the basics of the programming language R.\nUse RStudio.\n\n\n\n\n\n\n\n\n\n\nHomework\n\n\n\n\n\nCourse website\n\nCarefully read the homepage.\nFamiliarise yourself with this Course content page (note that the materials will be updated throughout the course).\n\nIntake form\n\nYou must complete the intake form before coming to the Tuesday lecture.\nThe link to the form can be found on the Learn website.\n\nInstall R and RStudio\n\nFor this course, you need to install both R and RStudio.\nNOTE: If you have installed either R or RStudio prior to January 2023, please make sure you delete both R and RStudio from your laptop.\nPlease, follow the instructions in the Setup page.\n\n\n\n\n\n\n\n\n\n\nMaterials\n\n\n\n\n\n\nLecture slides.\nWorkshop tutorial.\n\n\n\n\n\n\n\n\n\n\nSuggested readings\n\n\n\n\n\nMain textbooks\n\nStatistics for Linguists with R, by Bodo Winter (S4LR) Ch. 1. [via library]\nR for Data Science (R4DS) Ch. 1, Ch. 2. [online book]\nStatistical (Re)thinking, by Richard McElreath (SReT), Ch. 1. [via library]\n\nFrom the lecture\n\nEllis and Levy 2008. Framework of Problem-Based Research: A Guide for Novice Researchers on the Development of a Research-Worthy Problem\nSilberzahn et al. 2018. Many Analysts, One Data Set: Making Transparent How Variations in Analytic Choices Affect Results\nCoretta et al. 2023. Multidimensional signals and analytic flexibility: Estimating degrees of freedom in human speech analyses\nCumming 2014. The New Statistics: Why and How\nKurschke and Liddell 2018. The Bayesian New Statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective\n\nOther\n\nMethods as theory.\nMolnar 2022. Modeling Mindsets: The many cultures of learning from data.\nDarwin Holmes 2020. Researcher Positionality - A Consideration of Its Influence and Place in Qualitative Research - A New Researcher Guide\nJafar 2018. What is positionality and should it be expressed in quantitative studies?\n\n\n\n\n\n\n2.2 Week 2: Data wrangling\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nQuestions\n\nWhat are the types of statistical variables?\nWhich summary measures are appropriate for which types of variables?\nWhat are common measures central tendency?\nWhat are common measures of dispersion?\n\nSkills\n\nOrganise files efficiently.\nImport tabular data in R.\nObtain mean, median, mode, range and standard deviation.\nUse R scripts to save and reuse code.\n\n\n\n\n\n\n\n\n\n\nMaterials\n\n\n\n\n\n\nLecture slides.\nWorkshop tutorial.\nWorkshop files (right-click and download):\n\nshallow.csv\n\n\n\n\n\n\n\n\n\n\n\nSuggested readings\n\n\n\n\n\nMain textbooks\n\nS4LR Ch. 3. [via library]\nR4DS Ch. 3 and Ch. 4. [online book]\n\n\n\n\n\n\n2.3 Week 3: Data visualisation\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nQuestions\n\nWhat are the principles of good data visualisation?\nWhich are the main components of a plot?\nWhich are the appropriate plots for different types of data?\nHow can we visualise uncertainty?\n\nSkills\n\nCreate common types of plots with ggplot2.\nUse colour and shape to effectively convey meaning.\nDescribe a plot in writing and comment on observable patterns.\nCreate styled HTML reports.\n\n\n\n\n\n\n\n\n\n\nMaterials\n\n\n\n\n\n\nLecture slides.\nWorkshop tutorial.\nWorkshop files (right-click and download):\n\npolite.csv\nglot_status.rds\n\n\n\n\n\n\n\n\n\n\n\nSuggested readings\n\n\n\n\n\nMain textbooks\n\nR4DS Ch. 2. [online book]\nggplot2 documentation.\n\nFrom the lecture\n\nSpiegelhalter 2020. The Art of Statistics: Learning from Data.\n\nOther\n\nGabry et al 2019. Visualization in Bayesian workflow.\nPolitzer-Ahles and Piccini. On visualizing phonetic data from repeated measures experiments with multiple random effects.\nFundamentals of Data Visualisation.\nData viz catalogues\n\nDirectory of visualisations\nData viz catalogue\nData Viz project\nTop 50\nData Viz\n\nTutorials\n\nRaincloud plots\nLabels\nGraphic design\n\nColour\n\nColorBrewer2.\nMetBrewer\nUse colour wisely.\n\nCaveats\n\nSame stats different data.\nBehind bars.\nIssues with error bars.\nI’ve stopped using boxplots.\n\n\n\n\n\n\n\n2.4 Week 4: Statistical modeling basics\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nQuestions\n\nWhat are probability distributions?\nHow can we describe probability distributions with statistical parameters?\nWhat are the frequentist and Bayesian view of statistical parameters?\nHow can we estimate parameters using statistical models?\n\nSkills\n\nTransform data by creating new columns (mutate) and filtering based on specific values (filter).\nUse logical operators to transform data.\nFit a statistical model to estimate the mean and standard deviation of a Gaussian variable with brm().\nInterpret the summary of the model and understand the meaning of the reported estimates.\n\n\n\n\n\n\n\n\n\n\nMaterials\n\n\n\n\n\n\nLecture slides.\nWorkshop tutorial.\nWorkshop files (right-click and download):\n\nalb_vot.csv\n\n\n\n\n\n\n\n\n\n\n\nSuggested readings\n\n\n\n\n\nMain textbooks\n\nR4DS Ch. 2. [online book]\nggplot2 documentation.\nS4LR Ch 3. [via library]\nSReT Ch 2, sparingly (we have not covered everything in the chapter yet). [via library]\n\nOther\nThe following resources will be helpful throughout the course. Note they cover aspects that we have not yet discussed (some will be in the following weeks, others won’t be due to time), but do bookmark these because they will be valuable when you will be working on your dissertation.\n\nLinear Models and Mixed Models with R tutorials (1 and 2) by Bodo Winter (author of S4LR) for a general overview of the type of models we focus on in this course.\nOne Thousand and One names: table with naming conventions for different types of linear models.\nLinear Models: A cheat-sheet: use this to find out which building blocks you need for your linear model.\n\n\n\n\n\n\n2.5 Week 5: Categorical predictors\n\n\n\n\n\n\nFormative assessment 1\n\n\n\n\n\n\nDUE on Thu 19 October at noon.\nFormative assessment 1 requires you to complete a few guided exercises of the type that will be included in Summative 1.\nFind instructions and data here: https://github.com/uoelel/qml-f1\n\n\n\n\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nQuestions\n\nHow do we model variables using categorical predictors?\nWhich are the most common coding systems for categorical predictors?\nHow do we interpret the model output when there are categorical predictors?\nHow can we quickly check model goodness?\n\nSkills\n\nMaster contrast coding in R for categorical predictors.\nUnderstand treatment coding.\nFit, interpret and plot models with a categorical predictor.\nReporting of model specification and results.\n\n\n\n\n\n\n\n\n\n\nMaterials\n\n\n\n\n\n\nLecture slides.\nWorkshop tutorial.\nWorkshop files (right-click and download):\n\nsenses_valence.csv\n\n\n\n\n\n\n\n\n\n\n\nSuggested readings\n\n\n\n\n\nMain textbooks\n\nR4DS Ch. 17. [online book]\nS4LR Ch 7. [via library]\nSReT Sec 5.3. [via library]\n\nOther\n\nFactors, coding and contrasts: blog post on factors in linear models. It also discusses interactions, which we will cover in Weeks 8-9.\n\n\n\n\n\n\n2.6 Week 6: Catch-up Week\n\n\n\n\n\n\nHomework\n\n\n\n\n\nThere is no homework as such, so take the time to revise the materials and/or catch up with the previous weeks’ materials.\nThere will be no classes.\n\n\n\n\n\n2.7 Week 7: Binary outcomes\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nQuestions\n\nHow can we visualise proportions of binary outcomes (yes/no, correct/incorrect, …)?\nWhich distribution do binary outcomes follow?\nWhat is the relationship between probabilities and log-odds?\nHow do we interpret log-odds and odds?\n\nSkills\n\nPlot binary data as proportions in ggplot2.\nPivot data from wide to long with tidyr.\nFit, interpret and plot linear models with binary outcome variables, using the Bernoulli distribution family.\nConvert between log-odds, odds and probabilities.\n\n\n\n\n\n\n\n\n\n\nMaterials\n\n\n\n\n\n\nLecture slides.\nWorkshop tutorial.\nWorkshop files (right-click and download):\n\ntakete_maluma.txt.\n\n\n\n\n\n\n\n2.8 Week 8: Multiple predictors and interactions\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nQuestions\n\nWhat is a factorial design?\nHow do we estimate and interpret the effects of multiple predictors?\nHow do we deal with situations when one predictor’s effect is different, depending on the value of the other predictor?\nHow can such interactions between predictors be built into our models?\nHow do we interpret model estimates of interactions?\n\nSkills\n\nRun and interpret models with multiple predictors.\nInterpret interactions between two predictors.\nPlot posterior and conditional probabilities from models with interactions.\nPractice transforming and back-transforming variables.\n\n\n\n\n\n\n\n\n\n\nMaterials\n\n\n\n\n\n\nLecture slides.\nWorkshop tutorial.\nWorkshop files (right-click and download):\n\nshallow.csv.\ndur-ita-pol.csv.\n\n\n\n\n\n\n\n2.9 Week 9: Continuous predictors and interactions\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nQuestions\n\nHow do we model predictors that aren’t categorical, but continuous?\nHow do we interpret model estimates for continuous predictors?\nHow do we fit and interpret interactions involving continuous predictors?\n\nSkills\n\nCentre continuous predictors.\nRun and interpret models with continuous predictors.\nInterpret interactions that are categorical * continuous (in the lecture) and continuous * continuous (in the tutorial).\n\n\n\n\n\n\n\n\n\n\nMaterials\n\n\n\n\n\n\nLecture slides.\nWorkshop tutorial.\nWorkshop files (right-click and download):\n\nsi.csv.\n\n\n\n\n\n\n\n2.10 Week 10: Bayesian inference\n\n\n\n\n\n\nFormative assessment 2\n\n\n\n\n\n\nDUE on Thursday 23 November at noon.\nF2 requires you to read, plot and model data. Summative 2 will have the same format.\n\n\n\n\n\n\n2.11 Week 11: Obtaining p-values (Optional)"
  },
  {
    "objectID": "assessments.html",
    "href": "assessments.html",
    "title": "QML",
    "section": "",
    "text": "This course will be assessed on the basis of a set of online tests, 2 formative assessments and 2 summative assessments.\nFormative assessments are mock assessments that mirror the summative assessments. These will give you a chance to keep on track with the course contents and get interim feedback that will better prepare you for the summative assessment. See below for info.\nYou will have to submit two summative assessments. They will weight 50% and 50% of the final mark.\nSee below for details."
  },
  {
    "objectID": "assessments.html#assessment-overview",
    "href": "assessments.html#assessment-overview",
    "title": "QML",
    "section": "",
    "text": "This course will be assessed on the basis of a set of online tests, 2 formative assessments and 2 summative assessments.\nFormative assessments are mock assessments that mirror the summative assessments. These will give you a chance to keep on track with the course contents and get interim feedback that will better prepare you for the summative assessment. See below for info.\nYou will have to submit two summative assessments. They will weight 50% and 50% of the final mark.\nSee below for details."
  },
  {
    "objectID": "assessments.html#formative-assessments",
    "href": "assessments.html#formative-assessments",
    "title": "QML",
    "section": "2 Formative assessments",
    "text": "2 Formative assessments\n\n\n\n\n\n\nFormative 1: Week 5 (Thu 19 October at noon)\n\n\n\nData summary and visualisation\nYou can find the instructions and data for the first formative here: https://github.com/uoelel/qml-f1/.\n\n\n\n\n\n\n\n\nFormative 2: Week 10 (Thu 23 November at noon)\n\n\n\nStatistical modelling"
  },
  {
    "objectID": "assessments.html#summative-assessments",
    "href": "assessments.html#summative-assessments",
    "title": "QML",
    "section": "3 Summative assessments",
    "text": "3 Summative assessments\n\n\n\n\n\n\nSummative 1: Week 8 (Thu 9 November at noon)\n\n\n\nDue on Thursday 9 November at noon\nThe first summative contains a series of guided exercises.\n\n\n\n\n\n\n\n\nSummative 2: Week 12 (Thu 7 December at noon)\n\n\n\nDue on Thursday 7 December at noon\nIn the second summative assessment, you will:\n\nSelect a dataset from a list and its associated research questions.\nAnalyse the data using one linear model.\nWrite a report about the data, the model, and your findings."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative Methods for LEL",
    "section": "",
    "text": "Welcome to the main site of the course Quantitative Methods for Linguistics and English Language (Semester 1).\nThis website is your go-to place throughout the semester for any info related to the course.\n\n\n\n\n\n\nCourse description\n\n\n\nThis course is an introduction to quantitative data analysis (including data wrangling, visualisation and modelling) as commonly employed in linguistics, using the R software.\nWe will cover the following topics:\n\nThe basics of quantitative data analysis.\nData preparation.\nData summaries.\nPrinciples of data visualisation.\nStatistical modelling with linear models.\nStatistical inference using Bayesian inference (posterior probability distributions and credible intervals) and frequentist inference (p-values and confidence intervals).\n\nAt completion of the course you will have gained the following skills:\n\nImport common data formats, tidy and transform data.\nChoosing and reporting appropriate summary measures.\nUsing compelling visualisations to communicate a specific message about patterns in the data.\nMaster linear models for different types of data (continuous measures and binary outcomes).\nUsing Bayesian inference to answer research questions and avoid common interpretation pitfalls of frequentist techniques.\n\nExamples from different branches of linguistics will be used to provide you with hands-on experience in quantitative data analysis and Open Research practices.\n\n\n\n\n\n\n\n\nCourse rationale\n\n\n\nThis course is designed to help you develop the necessary skills for conducting and interpreting analyses of data as commonly employed in linguistics.\nThe content and objectives of the course are in response to recent advances in our understanding of the theory behind research methods.\nRecent meta-scientific research has identified three important aspects of research: the reproducibility, replicability and generalisability.\n\nA result is reproducible when the same analysis steps performed on the same dataset consistently produces the same answer.\nA result is replicable when the same analysis performed on different datasets produces qualitatively similar answers.\nA result is generalisable when a different analysis workflow performed on different data sets produces qualitatively similar answers.\n\nSee Definitions for a more detailed explanation.\nHowever, based on surveys from different disciplines, we are currently facing the three research crises (reproducibility, replicability and generalisability crises) by which most results are neither reproducible, nor replicable, nor generalisable (Munafò et al. 2017, Simmons et al. 2011, Ioannidis 2005, Yarkoni 2022).\nThe Open Research movement (also known as Open Science or Open Scholarship, Crüwell et al. 2019) was developed with the aim of improving our understanding of these crises and with the objective of providing researchers with guidelines and tools to produce reproducible, replicable and generalisable research.\nThe statistical philosophy adopted in the course is that of the New Statistics (Cumming 2014, Kruschke and Liddell 2018. The main goal of the New Statistics is to shift the attention from statistical significance to estimation with quantification of uncertainty.\n\n\n\n\n\n\n\n\nAsk for help\n\n\n\nIf at any point during the course you don’t feel comfortable with any aspect of the course, you are unsure about anything that has been covered in class or in your own time, you are struggling to keep up with the course workload, you are experiencing mental of physical distress due to a pre-existing or new illness, medical condition or disability, or you find yourself unable to access basic needs like food or housing, please do get in touch with me and/or the PPLS support (go to the PPLS UG or MSc Hub on SharePoint &gt; Support for students &gt; Health & Wellbeing).\nWe are humans first and the rapidly-changing new world we are living in now can put us under pressure. What is most important to me is that you are first and foremost healthy and able to participate to the course, and that you succeed and get the most out of the course.\nIt is OK not to be OK, and remember that you are not alone. Other people, teachers and students, might be struggling right now or have struggled before and might have gone through what you are going through now. Remember that support exists for you, so please do reach out. If you see somebody close to you struggling, please let them know about the available support network and encourage them to reach out.\n\n\n\n\n\n\n\n\nContacts\n\n\n\nYou can reach me (Stefano) at s.coretta@ed.ac.uk or on Teams.\nIf you want to book office hours with me or the tutors, you can do so here: https://bit.ly/33BH84L. Location depends on whom you are seeing and which day, so check the confirmation email for that info!"
  },
  {
    "objectID": "tutorials/tutorial-w01.html",
    "href": "tutorials/tutorial-w01.html",
    "title": "QML tutorial - Week 1",
    "section": "",
    "text": "R can be used to analyse all sorts of data, from tabular data (also known as “spreadsheets”), textual data, geographic data and even images.\n\nThis course will focus on the analysis of tabular data, since all of the techniques relevant to this type of data also apply to the other types.\n\nThe R community is a very inclusive community and it’s easy to find help. There are several groups that promote R in minority/minoritised groups, like R-Ladies, Africa R, and Rainbow R just to mention a few.\nMoreover, R is open source and free!"
  },
  {
    "objectID": "tutorials/tutorial-w01.html#why-r",
    "href": "tutorials/tutorial-w01.html#why-r",
    "title": "QML tutorial - Week 1",
    "section": "",
    "text": "R can be used to analyse all sorts of data, from tabular data (also known as “spreadsheets”), textual data, geographic data and even images.\n\nThis course will focus on the analysis of tabular data, since all of the techniques relevant to this type of data also apply to the other types.\n\nThe R community is a very inclusive community and it’s easy to find help. There are several groups that promote R in minority/minoritised groups, like R-Ladies, Africa R, and Rainbow R just to mention a few.\nMoreover, R is open source and free!"
  },
  {
    "objectID": "tutorials/tutorial-w01.html#r-vs-rstudio",
    "href": "tutorials/tutorial-w01.html#r-vs-rstudio",
    "title": "QML tutorial - Week 1",
    "section": "2 R vs RStudio",
    "text": "2 R vs RStudio\nBeginners usually have trouble understanding the difference between R and RStudio.\nLet’s use a car analogy.\nWhat makes the car go is the engine and you can control the engine through the dashboard.\nYou can think of R as an engine and RStudio as the dashboard.\n\n\n\n\n\n\n\nR\n\n\n\n\nR is a programming language.\nWe use programming languages to interact with computers.\nYou run commands written in a console and the related task is executed.\n\n\n\n\n\n\n\n\n\nRStudio\n\n\n\n\nRStudio is an Integrated Development Environment or IDE.\nIt helps you using R more efficiently.\nIt has a graphical user interface or GUI.\n\n\n\nThe next section will give you a tour of RStudio."
  },
  {
    "objectID": "tutorials/tutorial-w01.html#rstudio-1",
    "href": "tutorials/tutorial-w01.html#rstudio-1",
    "title": "QML tutorial - Week 1",
    "section": "3 RStudio",
    "text": "3 RStudio\nWhen you open RStudio, you can see the window is divided into 3 panels:\n\nBlue (left): the Console.\nGreen (top-right): the Environment tab.\nPurple (bottom-right): the Files tab.\n\n\nThe Console is where R commands can be executed. Think of this as the interface to R.\nThe Environment tab lists the objects created with R, while in the Files tab you can navigate folders on your computer to get to files and open them in the file Editor.\n\n3.1 RStudio projects\nRStudio is an IDE (see above) which allows you to work efficiently with R, all in one place.\nNote that files and data live in folders on your computer, outside of RStudio: do not think of RStudio as an app where you can save files in.\nAll the files that you see in the Files tab are files on your computer and you can access them from the Finder or File Explorer as you would with any other file.\nIn principle, you can open RStudio and then navigate to any folder or file on your computer.\nHowever, there is a more efficient way of working with RStudio: RStudio Projects.\n\n\n\n\n\n\nRStudio Projects\n\n\n\nAn RStudio Project is a folder on your computer that has an .Rproj file.\n\n\nYou can create as many RStudio Projects as you wish, and I recommend to create one per project (your dissertation, a research project, a course, etc…).\nWe will create an RStudio Project for this course (meaning, you will create a folder for the course which will be the RStudio Project). You will have to use this project/folder throughout the semester.\nTo create a new RStudio Project, click on the button that looks like a transparent light blue box with a plus, in the top-left corner of RStudio. A window like the one below will pop up.\n\nClick on New Directory then New Project.\n\nNow, this will create a new folder (aka directory) on your computer and will make that an RStudio Project (meaning, it will add a file with the .Rproj extension to the folder; the name of the file will be the name of the project/folder).\nGive a name to your new project, something like the name of the course and year (e.g. qml-2023).\nThen you need to specify where to create this new folder/Project. Click on Browse… and navigate to the folder you want to create the new folder/Project in.\nWhen done, click on Create Project. RStudio will automatically open your new project.\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen working through these tutorials, always make sure you are in the course RStudio Project you just created.\nYou know you are in an RStudio Project because you can see the name of the Project in the top-right corner of RStudio, next to the light blue cube icon.\nIf your see Project (none) in the top-right corner, that means your are not in an RStudio Project.\n\n\nThere are several ways of opening an RStudio Project:\n\nYou can go to the RStudio Project folder in Finder or File Explorer and double click on the .Rproj file.\nYou can click on File &gt; Open Project in the RStudio menu.\nYou can click on the project name in the top-right corner of RStudio, which will bring up a list of projects. Click on the desired project to open it.\n\n\n\n3.2 A few important settings\nBefore moving on, there are a few important settings that you need to change.\n\n\nOpen the RStudio preferences (Tools &gt; Global options...).\nUn-tick Restore .RData into workspace at startup.\n\nThis mean that every time you start RStudio you are working with a clean Environment. Not restoring the workspace ensures that the code you write is fully reproducible.\n\nSelect Never in Save workspace to .RData on exit.\n\nSince we are not restoring the workspace at startup, we don’t need to save it. Remember that as long as you save the code, you will not lose any of your work! You will learn how to save code from next week.\n\nClick OK to confirm the changes.\n\n\n\n\n\n\n\nQuiz 1\n\n\n\n\nTrue or false?\n\nRStudio executes the code. TRUEFALSE\nR is a programming language. TRUEFALSE\nAn IDE is necessary to run R. TRUEFALSE\nRStudio projects are folders with an .Rproj file. TRUEFALSE\nThe project name is shown in the top-right corner of RStudio. TRUEFALSE"
  },
  {
    "objectID": "tutorials/tutorial-w01.html#r-basics",
    "href": "tutorials/tutorial-w01.html#r-basics",
    "title": "QML tutorial - Week 1",
    "section": "4 R basics",
    "text": "4 R basics\nIn this part of the tutorial you will learn the very basics of R.\nIf you have prior experience with programming, you should find all this familiar. If not, not to worry! Make sure you understand the concept highlighted in the green boxes and practice the related skills.\nFor this tutorial, you will just run code directly in the R Console in RStudio, i.e. you will type code in the Console and press ENTER to run it.\nIn future tutorials, you will learn how to save your code in a script file, so that you can keep track of which code you have run and make your work reproducible.\n\n4.1 R as a calculator\nWrite this code 1 + 2 in the Console, then press ENTER/RETURN to run the code.\nFantastic! You should see that the answer of the addition has been printed in the Console, like this:\n[1] 3\n(Never mind the [1] part for now).\nNow, try some more operations (write each of the following in the Console and press ENTER). Feel free to add your own operations to the mix!\n\n67 - 13\n2 * 4\n268 / 43\n\nYou can also chain multiple operations.\n\n6 + 4 - 1 + 2\n4 * 2 + 3 * 2\n\n\n\n\n\n\n\nQuiz 2\n\n\n\n\nAre the following pairs of operations equivalent?\n\n3 * 2 / 4 = 3 * (2 / 4) TRUEFALSE\n10 * 2 + 5 * 0.2 = (10 * 2 + 5) * 0.2 TRUEFALSE\n\n\n\n\n\n\n\n\n\n\nExtra: Arithmetics\n\n\n\n\n\nIf you need a maths refresher, I recommend checking the following pages:\n\nhttps://www.mathsisfun.com/definitions/order-of-operations.html\nhttps://www.mathsisfun.com/algebra/introduction.html\n\n\n\n\n\n\n4.2 Variables\n\nForget-me-not.\n\nMost times, we want to store a certain value so that we can use it again later.\nWe can achieve this by creating variables.\n\n\n\n\n\n\nVariable\n\n\n\nA variable holds one or more values and it’s stored in the computer memory for later use.\n\n\nYou can create a variable by using the assignment operator &lt;-.\nLet’s assign the value 156 to the variable my_num.\n\nmy_num &lt;- 156\n\nNow, check the list of variables in the Environment tab of the top-right panel of RStudio. You should see the my_num variable and its value there.\nNow, you can just call the variable back when you need it! Write the following in the Console and press ENTER.\n\nmy_num\n\n[1] 156\n\n\nA variable like my_num is also called a numeric vector: i.e. a vector that contains a number (hence numeric).\n\n\n\n\n\n\nVector\n\n\n\nA vector is an R object that contains one or more values of the same type.\n\n\nA vector is a type of variable and a numeric vector is a type of vector. However, it’s fine in most cases to use the word variable to mean vector (just note that a variable can also be something else than a vector; you will learn about other R objects from next week).\nLet’s now try some operations using variables.\n\nincome &lt;- 1200\nexpenses &lt;- 500\nincome - expenses\n\n[1] 700\n\n\nSee? You can use operations with variables too!\nAnd you can also go all the way with variables.\n\nsavings &lt;- income - expenses\n\nAnd check the value…\n\nsavings\n\n[1] 700\n\n\nVectors can hold more than one item or value.\nJust use the combine c() function to create a vector containing multiple values.\nThe following are all numeric vectors.\n\none_i &lt;- 6\n# Vector with 2 values\ntwo_i &lt;- c(6, 8)\n# Vector with 3 values\nthree_i &lt;- c(6, 8, 42)\n\nCheck the list of variables in the Environment tab. You will see now that before the values of two_i and three_i you get the vector type num for numeric. (If the vector has only one value, you don’t see the type in the Enviroment list but it is still of a specific type).\n\n\n\n\n\n\nNumeric vector\n\n\n\nA numeric vector is a vector that holds one or more numeric values.\n\n\nNote that the following are the same:\n\none_i &lt;- 6\none_i\n\n[1] 6\n\none_ii &lt;- c(6)\none_ii\n\n[1] 6\n\n\nAnother important aspect of variables is that they are… variable! Meaning that once you assign a value to one variable, you can overwrite the value by assigning a new one to the same variable.\n\nmy_num &lt;- 88\nmy_num &lt;- 63\nmy_num\n\n[1] 63\n\n\n\n\n\n\n\n\nQuiz 3\n\n\n\n\nTrue or false?\n\nA vector is a type of variable. TRUEFALSE\nNot all variables are vectors. TRUEFALSE\nA numeric vector can only hold numeric values. TRUEFALSE\n\n\n\n\n\n\n4.3 Functions\n\nR cannot function without… functions.\n\n\n\n\n\n\n\nFunction\n\n\n\nA function usually runs an operation on one or more specified arguments.\n\n\nA function in R has the form function() where:\n\nfunction is the name of the function, like sum.\n() are round parentheses, inside of which you write arguments, separated by commas.\n\nLet’s see an example:\n\nsum(3, 5)\n\n[1] 8\n\n\nThe sum() function sums the number listed as arguments. Above, the arguments are 3 and 5.\nAnd of course arguments can be vectors!\n\nmy_nums &lt;- c(3, 5, 7)\n\nsum(my_nums)\n\n[1] 15\n\nmean(my_nums)\n\n[1] 5\n\n\n\n\n\n\n\n\nQuiz 4\n\n\n\n\nTrue or false?\n\nFunctions can take other functions as arguments. TRUEFALSE\nAll function arguments must be specified. TRUEFALSE\nAll functions need at least one argument. TRUEFALSE\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe Sys.Date() function and other functions like it don’t take any arguments.\n\n\n\n\n\n\n\n\n\nExtra: R vs Python\n\n\n\n\n\nIf you are familiar with Python, you will soon realise that R and Python, although they share many concepts and types of objects, they can differ substantially. This is because R is a functional programming language (based on functions) while Python is an Object Oriented programming language (based on methods applied on objects).\nGenerally speaking, functions look like print(x) while methods look like x.print()\n\n\n\n\n\n4.4 String and logical vectors\n\nNot just numbers.\n\nWe have seen that variables can hold numeric vectors. But vectors are not restricted to being numeric. They can also store strings.\nA string is basically a set of characters (a word, a sentence, a full text).\nIn R, strings have to be quoted using double quotes \" \".\nChange the following strings to your name and surname. Remember to keep the double quotes\n\nname &lt;- \"Stefano\"\nsurname &lt;- \"Coretta\"\n\nname\n\n[1] \"Stefano\"\n\n\nStrings can be used as arguments in functions, like numbers can.\n\ncat(\"My name is\", name, surname)\n\nMy name is Stefano Coretta\n\n\nRemember that you can reuse the same variable name to override the variable value.\n\nname &lt;- \"Raj\"\n\ncat(\"My name is\", name, surname)\n\nMy name is Raj Coretta\n\n\nYou can combine multiple strings into a character vector, using c().\n\n\n\n\n\n\nCharacter vector\n\n\n\nA character vector is a vector that holds one or more strings.\n\n\n\nfruit &lt;- c(\"apple\", \"oranges\", \"bananas\")\nfruit\n\n[1] \"apple\"   \"oranges\" \"bananas\"\n\n\nCheck the Environment tab. Character vectors have chr before the values.\nAnother type of vector is one that contains either TRUE or FALSE. Vectors of this type are called logical vectors and they are listed as logi in the Environment tab.\n\n\n\n\n\n\nLogical vector\n\n\n\nA logical vector is a vector that holds one or more TRUE or FALSE values.\n\n\n\ngroceries &lt;- c(\"apple\", \"flour\", \"margarine\", \"sugar\")\nin_pantry &lt;- c(TRUE, TRUE, FALSE, TRUE)\n\ndata.frame(groceries, in_pantry)\n\n\n\n  \n\n\n\nTRUE and FALSE values must be written in all capitals and without double quotes (they are not strings!).\n(We will talk about data frames, another type of object in R, in the following weeks.)\n\n\n\n\n\n\nQuiz 5\n\n\n\n\n\nWhich of the following is not a character vector.\n\n c(1, 2, \"43\") \"s\" c(apple) (assuming apple &lt;- 45) c(letters)\n\nWhich of the following is not a logical vector.\n\n c(T, T, F) TRUE \"FALSE\" c(FALSE)\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou can use the class() function to check the type (“class”) of a vector.\n\nclass(FALSE)\n\n[1] \"logical\"\n\nclass(c(1, 45))\n\n[1] \"numeric\"\n\nclass(c(\"a\", \"b\"))\n\n[1] \"character\"\n\n\n\n\n\n\n\n\n\n\n\nExplanation\n\n\n\n\n\n5a\n\nc(1, 2, \"43\") is a character vector because the last number \"43\" is a string (it’s between double quotes!). A vector cannot have a mix of types of elements: they have to be all numbers or all strings or else, but not some numbers and some strings. Numbers are special in that if you include a number in a character vector without quoting it, it is automatically converted into a string. Try the following:\n\n\nchar &lt;- c(\"a\", \"b\", \"c\")\nchar &lt;- c(char, 1)\nchar\nclass(char)\n\n\nc(letters) is a character vector because letters contains the letters of the alphabet as strings (this vector comes with base R).\nc(apple) is not a character vector because the variable apple holds a number, 45!\n\n5b\n\n\"FALSE\" is not a logical vector because FALSE has been quoted (anything that is quoted is a string!).\n\n\n\n\n\n\n\n\n\n\n\n\nExtra: For-loops and if-else statements\n\n\n\n\n\nThis course does not cover programming in R in the strict sense, but if you are curious here’s a short primer on for-loops and if-else statements in R.\nFor-loops\n\nfruits &lt;- c(\"apples\", \"mangos\", \"durians\")\n\nfor (fruit in fruits) {\n  cat(\"I like\", fruit, \"\\n\")\n}\n\nI like apples \nI like mangos \nI like durians \n\n\nIf-else\n\nfor (fruit in fruits) {\n  if (grepl(\"n\", fruit)) {\n    cat(fruit, \"has an 'n'\", \"\\n\")\n  } else {\n    cat(fruit, \"does not have an 'n'\", \"\\n\")\n  }\n}\n\napples does not have an 'n' \nmangos has an 'n' \ndurians has an 'n'"
  },
  {
    "objectID": "tutorials/tutorial-w01.html#summary",
    "href": "tutorials/tutorial-w01.html#summary",
    "title": "QML tutorial - Week 1",
    "section": "5 Summary",
    "text": "5 Summary\nYou made it! You completed this week’s tutorial.\nHere’s a summary of what you learnt.\n\n\n\n\n\n\n\nR is a programming language while RStudio is an IDE.\nRStudio projects are folders with an .Rproj file (you can see the name of the project you are currently in in the top-right corner of RStudio).\nYou can perform mathematical operations with +, -, *, /.\nYou can store values in variables.\nA typical object to be stored in a variable is a vector: there are different type of vectors, like numeric, character and logical.\nFunctions are used to perform an operation on its arguments: sum() sums it’s arguments, mean() calculates the mean and cat() prints the arguments.\n\n\n\n\n\n\n\n\n\n\nExtra: Programming in R\n\n\n\n\n\nIf you are interested in learning about programming in R, I recommend you go through Chapters 26-28 of the R4DS book and the Advanced R book.\nNote that these topics are not covered in the course, nor will be assessed."
  },
  {
    "objectID": "tutorials/tutorial-w03.html",
    "href": "tutorials/tutorial-w03.html",
    "title": "QML tutorial - Week 3",
    "section": "",
    "text": "Last week, you learnt how to use R scripts to save your code.\nKeeping track of the code you use for data analysis is a very important aspect of research project managing: not only the code is there if you need to rerun it later, but it allows your data analysis to be reproducible (i.e., it can be reproduced by you or other people in such a way that starting with the same data and code you get to the same results).\n\n\n\n\n\n\nReproducible research\n\n\n\nResearch is reproducible when the same data and same code return the same results.\n\n\nR scripts are great for writing code, and you can even document the code (add explanations or notes) with comments (i.e. lines that start with #).\nBut for longer text or complex data analysis reports, R scripts can be a bit cumbersome.\nA solution to this is using Rmarkdown files (they have the .Rmd extension).\n\n\nRmarkdown is a file format that allows you to mix code and formatted text in the same file.\nThis means that you can write dynamic reports using Rmd files: dynamic reports are just like analysis reports (i.e. they include formatted text, plots, tables, code output, code, etc…) but they are dynamic in the sense that if, for example, data or code changes, you can just rerun the report Rmd file and all code output (plots, tables, etc…) is updated accordingly!\nYou can watch this short video for a quick tour.\n\n\n\n\n\n\n\nDynamic reports in R Markdown\n\n\n\nR Markdown is a file type with extension .Rmd in which you can write formatted text and code together.\nR Markdown can be used to generate dynamic reports: these are files that are generated automatically from the R Markdown source, ensuring data and results in the report are always up to date.\n\n\n\n\n\nR comments in R scripts cannot be formatted (for example, you can’t make bold or italic texts).\nText in Rmd files can be fully formatted using a simple but powerful mark-up language called markdown.\nYou don’t have to learn markdown all in one go, so I encourage you to just learn it bit by bit, in your time. You can look at the the Markdown Guide for an in-depth intro and/or dive in the Markdown Tutorial for a hands-on approach.\nA few quick pointers (you can test them in the Markdown Live Preview):\n\nText can be made italics by enclosing it between single stars: *this text is in italics*.\nYou can make text bold with two stars: **this text is bold!**.\nHeadings are created with #: # This is a level-1 heading. ## This is a level-2 heading.\n\n\n\n\n\n\n\nMark-up, Markdown\n\n\n\nA mark-up language is a text-formatting system consisting of symbols or keywords that control the structure, formatting or relationships of textual elements. The most common mark-up languages are HTML, XML and TeX.\nMarkdown is a simple yet powerful mark-up language.\n\n\n\n\n\nTo create a new Rmd file, just click on the New file button, then R Markdown.... (If you are asked to install/update packages, do so.)\n\nA window will open. Add a title of your choice and your name, then click OK.\n\nA new Rmd file will be created and will open in the File Editor panel in RStudio.\nNote that creating an Rmd file does not automatically saves it on your computer. To do so, either use the keyboard short-cut CMD+S/CTRL+S or click on the floppy disk icon in the menu below the file tab.\n\nSave the file inside the code/ folder with the following name: tutorial-w03.Rmd.\nRemember that all the files of your RStudio project don’t live inside RStudio but on your computer. So you can always access them from the Finder or File Explorer!\n\n\n\nAn Rmd file usually has three main parts:\n\nThe preamble (green).\nCode chunks (orange).\nText (blue).\n\n\nEach Rmd file has to start with a preamble, but you can include as many code chunks and as much text as you wish, in any order.\n\n\n\n\n\n\nR Markdown: Preamble\n\n\n\nThe preamble of an .Rmd file contains a list of key: value pairs, used to specify settings or document info like the title and author.\nPreambles start and end with three dashes ---.\n\n\n\n\n\n\n\n\nR Markdown: Code chunks\n\n\n\nCode chunks start and end with three back-ticks ``` and they contain code.\nSettings can be specified between curly brackets {} on the first ``` line: for example {r chunk-name, eval=FALSE}. r indicates that the code is R code.\n\n\n\n\n\nYou can render an Rmd file into a nicely formatted HTML file.\nTo render an Rmd file, just click on the Knit button and an HTML file will be created and saved in the same location of the Rmd file.\n\nIt will also be shown in the Viewer pane, in the bottom-right panel of RStudio.\n\nRendering Rmd files is not restricted to HTML, but also PDFs and even Word documents!\nThis is very handy when you are writing an analysis report you need to share with others.\n\n\n\n\n\n\nR Markdown: Rendering\n\n\n\nRmd files can be rendered into other formats, like HTML, PDF and Word documents.\n\n\n\n\n\nThe first code chunk in an Rmd file is a special one: it’s called setup and it’s where you attach all the packages needed for running the code in the file. (You can find the name of a code chunk—or define one yourself!—in the first line of a code block: ```{r chunk-name}). The first code chunk of an Rmd file should always be named setup.\nThe setup code chunk also has special code from the knitr package. The knitr package is what allows you to run R code from Rmd files.\nThe line knitr::opts_chunk$set(echo = TRUE) sets the option echo to TRUE. The echo option let’s you decide whether you want the R code printed (echoed) in the rendered Rmd file. When echo = TRUE the code is printed, when echo = FALSE the code is not printed.\nNow go ahead and add library(tidyverse) in the line next to the knitr::opts_chunk line.\n\n\n\n\n\n\nRunning code\n\n\n\nTo run the code of a code chunk you can use different approaches:\n\nClick on the green arrow to the right of the code chunk to run the entire code chunk.\nAlternatively, place the text cursor inside the code chunk and press CMD/CTRL + SHIFT + ENTER.\nYou can also run line by line as you do in an R script by placing the text cursor on the line you want to run and press CMD/CTRL + ENTER.\n\n\n\nNow delete everything after the setup code chunk and write a new level-2 heading after the chunk with the name “Plotting basics”. (Remember to leave an empty line between the chunk and the heading.)\n\nYou will use this Rmd file to write text and code for this tutorial. To insert a new code chunk, you can click on the Insert a new code chunk button and then select R, or you can press OPT/ALT + CMD/CTRL + I.\n\nA new R code chunk will be inserted at the text cursor position."
  },
  {
    "objectID": "tutorials/tutorial-w03.html#rmarkdown",
    "href": "tutorials/tutorial-w03.html#rmarkdown",
    "title": "QML tutorial - Week 3",
    "section": "",
    "text": "Last week, you learnt how to use R scripts to save your code.\nKeeping track of the code you use for data analysis is a very important aspect of research project managing: not only the code is there if you need to rerun it later, but it allows your data analysis to be reproducible (i.e., it can be reproduced by you or other people in such a way that starting with the same data and code you get to the same results).\n\n\n\n\n\n\nReproducible research\n\n\n\nResearch is reproducible when the same data and same code return the same results.\n\n\nR scripts are great for writing code, and you can even document the code (add explanations or notes) with comments (i.e. lines that start with #).\nBut for longer text or complex data analysis reports, R scripts can be a bit cumbersome.\nA solution to this is using Rmarkdown files (they have the .Rmd extension).\n\n\nRmarkdown is a file format that allows you to mix code and formatted text in the same file.\nThis means that you can write dynamic reports using Rmd files: dynamic reports are just like analysis reports (i.e. they include formatted text, plots, tables, code output, code, etc…) but they are dynamic in the sense that if, for example, data or code changes, you can just rerun the report Rmd file and all code output (plots, tables, etc…) is updated accordingly!\nYou can watch this short video for a quick tour.\n\n\n\n\n\n\n\nDynamic reports in R Markdown\n\n\n\nR Markdown is a file type with extension .Rmd in which you can write formatted text and code together.\nR Markdown can be used to generate dynamic reports: these are files that are generated automatically from the R Markdown source, ensuring data and results in the report are always up to date.\n\n\n\n\n\nR comments in R scripts cannot be formatted (for example, you can’t make bold or italic texts).\nText in Rmd files can be fully formatted using a simple but powerful mark-up language called markdown.\nYou don’t have to learn markdown all in one go, so I encourage you to just learn it bit by bit, in your time. You can look at the the Markdown Guide for an in-depth intro and/or dive in the Markdown Tutorial for a hands-on approach.\nA few quick pointers (you can test them in the Markdown Live Preview):\n\nText can be made italics by enclosing it between single stars: *this text is in italics*.\nYou can make text bold with two stars: **this text is bold!**.\nHeadings are created with #: # This is a level-1 heading. ## This is a level-2 heading.\n\n\n\n\n\n\n\nMark-up, Markdown\n\n\n\nA mark-up language is a text-formatting system consisting of symbols or keywords that control the structure, formatting or relationships of textual elements. The most common mark-up languages are HTML, XML and TeX.\nMarkdown is a simple yet powerful mark-up language.\n\n\n\n\n\nTo create a new Rmd file, just click on the New file button, then R Markdown.... (If you are asked to install/update packages, do so.)\n\nA window will open. Add a title of your choice and your name, then click OK.\n\nA new Rmd file will be created and will open in the File Editor panel in RStudio.\nNote that creating an Rmd file does not automatically saves it on your computer. To do so, either use the keyboard short-cut CMD+S/CTRL+S or click on the floppy disk icon in the menu below the file tab.\n\nSave the file inside the code/ folder with the following name: tutorial-w03.Rmd.\nRemember that all the files of your RStudio project don’t live inside RStudio but on your computer. So you can always access them from the Finder or File Explorer!\n\n\n\nAn Rmd file usually has three main parts:\n\nThe preamble (green).\nCode chunks (orange).\nText (blue).\n\n\nEach Rmd file has to start with a preamble, but you can include as many code chunks and as much text as you wish, in any order.\n\n\n\n\n\n\nR Markdown: Preamble\n\n\n\nThe preamble of an .Rmd file contains a list of key: value pairs, used to specify settings or document info like the title and author.\nPreambles start and end with three dashes ---.\n\n\n\n\n\n\n\n\nR Markdown: Code chunks\n\n\n\nCode chunks start and end with three back-ticks ``` and they contain code.\nSettings can be specified between curly brackets {} on the first ``` line: for example {r chunk-name, eval=FALSE}. r indicates that the code is R code.\n\n\n\n\n\nYou can render an Rmd file into a nicely formatted HTML file.\nTo render an Rmd file, just click on the Knit button and an HTML file will be created and saved in the same location of the Rmd file.\n\nIt will also be shown in the Viewer pane, in the bottom-right panel of RStudio.\n\nRendering Rmd files is not restricted to HTML, but also PDFs and even Word documents!\nThis is very handy when you are writing an analysis report you need to share with others.\n\n\n\n\n\n\nR Markdown: Rendering\n\n\n\nRmd files can be rendered into other formats, like HTML, PDF and Word documents.\n\n\n\n\n\nThe first code chunk in an Rmd file is a special one: it’s called setup and it’s where you attach all the packages needed for running the code in the file. (You can find the name of a code chunk—or define one yourself!—in the first line of a code block: ```{r chunk-name}). The first code chunk of an Rmd file should always be named setup.\nThe setup code chunk also has special code from the knitr package. The knitr package is what allows you to run R code from Rmd files.\nThe line knitr::opts_chunk$set(echo = TRUE) sets the option echo to TRUE. The echo option let’s you decide whether you want the R code printed (echoed) in the rendered Rmd file. When echo = TRUE the code is printed, when echo = FALSE the code is not printed.\nNow go ahead and add library(tidyverse) in the line next to the knitr::opts_chunk line.\n\n\n\n\n\n\nRunning code\n\n\n\nTo run the code of a code chunk you can use different approaches:\n\nClick on the green arrow to the right of the code chunk to run the entire code chunk.\nAlternatively, place the text cursor inside the code chunk and press CMD/CTRL + SHIFT + ENTER.\nYou can also run line by line as you do in an R script by placing the text cursor on the line you want to run and press CMD/CTRL + ENTER.\n\n\n\nNow delete everything after the setup code chunk and write a new level-2 heading after the chunk with the name “Plotting basics”. (Remember to leave an empty line between the chunk and the heading.)\n\nYou will use this Rmd file to write text and code for this tutorial. To insert a new code chunk, you can click on the Insert a new code chunk button and then select R, or you can press OPT/ALT + CMD/CTRL + I.\n\nA new R code chunk will be inserted at the text cursor position."
  },
  {
    "objectID": "tutorials/tutorial-w03.html#plotting-basics",
    "href": "tutorials/tutorial-w03.html#plotting-basics",
    "title": "QML tutorial - Week 3",
    "section": "2 Plotting basics",
    "text": "2 Plotting basics\nPlotting data in R is easy once you understand the basics.\n\n2.1 Graphic systems\nIn R, you can create plots using different systems.\n\nBase R.\nlattice.\nggplot2.\nmore…\n\nIn this course you will learn how to use the ggplot2 system, but before we dive in, let’s have a look at the base R plotting system too.\n\n\n2.2 Base R plotting function\nLet’s create two vectors, x and y and plot them. For now, run these in the Console (not in the code chunk we just created).\n\nx &lt;- 1:10\ny &lt;- x^3\n\nplot(x, y)\n\n\n\n\nEasy!\nNow let’s add a few more things.\n\nplot(x, y, type = \"l\", col = \"purple\", lwd = 3, lty = \"dashed\")\n\n\n\n\nWith plots as simple as this one, the base R plotting system is sufficient, but to create more complex plots (which is virtually always the case), base R gets incredibly complicated.\nInstead we can use the tidyverse package ggplot2. ggplot2 works well with the other tidyverse packages and it follows the same principles, so it is convenient to use it for data visualisation instead of base R!"
  },
  {
    "objectID": "tutorials/tutorial-w03.html#your-first-ggplot2-plot",
    "href": "tutorials/tutorial-w03.html#your-first-ggplot2-plot",
    "title": "QML tutorial - Week 3",
    "section": "3 Your first ggplot2 plot",
    "text": "3 Your first ggplot2 plot\nThe tidyverse package ggplot2 provides users with a consistent set of functions to create captivating graphics.\n\n\n\n\n\n\nWarning\n\n\n\nTo be able to use the functions in a package, you first need to attach the package. We have already attached the library(tidyverse) packages, among which there is ggplot2, so you don’t need to do anything else.\n\n\n\n3.1 The polite data\nWe will use the polite data to learn the basics of plotting using ggplot.\nThe data comes from the paper by Winter and Grawunder, 2020. The phonetic profile of Korean formal and informal speech registers. DOI: 10.1016/j.wocn.2012.08.006. Read the abstract to familiarise yourself with the study.\nTo download the file with the data right-click on the following link and download the file: polite.csv. (Note that tutorial files are also linked in the Syllabus). Remember to save the file in data/ in the course project folder.\nWith Rmd files, we can use relative paths to read data. For relative paths to work relative to the project working directory, you need to install the here package (remember how to install packages?) and add the following line in the setup chunk, just after the first line of code.\nknitr::opts_knit$set(root.dir = here::here())\n(Note that it’s knitr::opts_knit rather than knitr::opts_chunk).\nNow rerun the setup code chunk to set the new option.\nThis will set the working directory for the Rmd file to the project working directory.\n\n\n\n\n\n\nWarning\n\n\n\nMake sure that you always include that knitr::opts_knit() line in the setup block, or R might get confused about where the working directory is and not be able to find the data.\n\n\nFinally, create a new code chunk, add the following line of code and then run the code chunk to read the data.\n\npolite &lt;- read_csv(\"data/polite.csv\")\n\nIn this tutorial we will use the following columns:\n\nf0mn: mean f0 (fundamental frequency).\nH1H2: difference between H2 and H1 (second and first harmonic).\ngender: speaker’s gender.\nmusicstudent: whether the speaker is a music student (yes) or not (no).\n\n\n\n\n\n\n\nQuiz 1\n\n\n\n\nWhat does here::here() return?\n\n The current working directory. The project directory. The data directory.\n\n\n\n\n\n\n\n\nExplanation\n\n\n\n\n\nThe answer The current working directory. is wrong because if you manually set the working directory with setwd(), here::here() will still return the project directory!\n\n\n\n\n\n\n\n3.2 A basic plot\nThese are the minimum constituents of a ggplot2 plot.\n\n\n\n\n\n\nggplot basics\n\n\n\n\nThe data: you have to specify the data frame with the data you want to plot.\nThe mapping: the mapping tells ggplot how to map data columns to parts of the plot like the axes or groupings within the data. These parts are called aesthetics, or aes for short.\n\n\n\nYou can specify the data and mapping with the data and mapping arguments of the ggplot() function.\nNote that the mapping argument is always specified with aes(): mapping = aes(…).\nIn the following bare plot, we are just mapping the mean f0 (f0) to the x-axis and the H2-H1 difference to the y-axis, from the polite data frame.\nCreate a new code chunk, copy the following code and run it. From this point on I will assume you’ll create a new code chunk and run the code yourself, without explicit instructions.\n\nggplot(\n  data = polite,\n  mapping = aes(x = f0mn, y = H1H2)\n)\n\n\n\n\nNot much to see here: just two axes!\n\n\n\n\n\n\nQuiz 2\n\n\n\n\nIs the following code correct? Justify your answer. TRUEFALSE\n\nggplot(\n  data = polite,\n  mapping = c(x = total_duration, y = articulation_rate)\n)\n\n\n\n\n3.3 Let’s add geometries\nNice, but we are missing the most important part: showing the data!\nData is represented with geometries, or geoms for short. geoms are added to the base ggplot with functions whose names all start with geom_.\n\n\n\n\n\n\nGeometries\n\n\n\nGeometries are plot elements that show the data through geometric shapes.\nDifferent geometries are added to a ggplot using one of the geom_*() functions.\n\n\nFor this plot, you want to use geom_point(). This geom simply adds point to the plot based on the data in the polite data frame.\nTo add geoms to a plot, you write a + at the end of the ggplot() command and include the geom on the next line. For example:\n\nggplot(\n  data = polite,\n  mapping = aes(x = f0mn, y = H1H2)\n) +\n  geom_point()\n\nWarning: Removed 12 rows containing missing values (`geom_point()`).\n\n\n\n\n\nThis type of plot, with two continuous axes and data represented by points, is called a scatter plot.\n\n\n\n\n\n\nScatter plot\n\n\n\nA scatter plot is a plot with two numeric axes and points indicating the data. It is used when you want to show the relationship between two numeric variables.\nTo create a scatter plot, use the geom_point() geometry.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNote that using the + is a quirk of ggplot(). The idea behind it is that you start from a bare plot and you add (+) layers of data on top of it. This is because of the philosophy behind the package, called the Layered Grammar of Graphics.\n\n\n\n\n3.4 Function arguments\nNote that the data and mapping arguments don’t have to be named explicitly (with data = and mapping =) in the ggplot() function, since they are obligatory and they are specified in that order.\n\nggplot(\n  polite,\n  aes(x = f0mn, y = H1H2)\n) +\n  geom_point()\n\n\n\n\nIn fact, you can also leave out x = and y =.\n\nggplot(\n  polite,\n  aes(f0mn, H1H2)\n) +\n  geom_point()\n\nWarning: Removed 12 rows containing missing values (`geom_point()`).\n\n\n\n\n\nTry running ?ggplot in the Console to see the arguments of the function and the order they appear in.\n\n\n\n\n\n\nQuiz 3\n\n\n\nWhich of the following will produce the same plot as the one above? Reason through it first without running the code, then run all of these to check whether they look the way you expected.\n\nWhich of the following will produce the same plot as the one above? Reason through it first without running the code, then run all of these to check whether they look the way you expected.\n\n ggplot(polite, aes(H1H2, f0mn)) + geom_point() ggplot(polite, aes(y = H1H2, x = f0mn)) + geom_point() ggplot(polite, aes(y = f0mn, x = H1H2)) + geom_point()\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nWhen specifying arguments, the order matters when not using the argument names.\nSo this aes(a, b) is different from aes(b, a).\nBut this aes(y = b, x = a) is the same as aes(a, b)."
  },
  {
    "objectID": "tutorials/tutorial-w03.html#bar-charts",
    "href": "tutorials/tutorial-w03.html#bar-charts",
    "title": "QML tutorial - Week 3",
    "section": "4 Bar charts",
    "text": "4 Bar charts\nAnother common type of plot is the bar chart.\n\n\n\n\n\n\nBar charts\n\n\n\nBar charts are useful when you are counting things. For example:\n\nNumber of verbs vs nouns vs adjectives in a corpus.\nNumber of languages by geographic area.\nNumber of correct vs incorrect responses.\n\nThe bar chart geometry is geom_bar().\n\n\nIn the following example, we will be counting the number of languages by their endangerment status.\n\n4.1 Do you understand me?\nThere are thousands of languages in the world, but most of them are losing speakers, and some are already no longer spoken. The endangerment status of a language in the data is on a scale from not endangered (languages with large populations of speakers) through threatened, shifting and nearly extinct, to extinct (languages that have no living speakers left).\nThe glot_status data frame contains the endangerment status for 7,845 languages from Glottolog. To download the file with the data right-click on the following link and download the file: glot_status.rds. (Note that tutorial files are also linked in the Syllabus). Remember to save the file in data/ in the course project folder.\nThis time the file is not a .csv file but an .rds file. .rds files are files that can save any R object. They are commonly used to save data frames, like in this case.\n\n\n\n\n\n\nRds files\n\n\n\n.Rds files are a type of R file which can store any R object and save it on disk.\nR objects can be saved to an .Rds file with the saveRDS() function and they can be read with the readRDS() function.\n\n\nTo read an .rds file you need the readRDS() function.\n\nglot_status &lt;- readRDS(\"data/glot_status.rds\")\n\nHere’s what the data frame looks like.\n\nglot_status\n\n\n\n  \n\n\n\nFor us today, the relevant columns are:\n\nName: the language name.\nstatus: the endangerment status of the language.\nMacroarea: the geographic macro-area the language is spoken in.\n\n\n\n4.2 Create a bar chart\nTo create a bar chart, use the geom_bar() geometry.\n\n\n\n\n\n\nBar chart axes\n\n\n\nIn a simple bar chart, you only need to specify one axis, the x-axis, in the aesthetics aes().\nThis is because the counts that are placed on the y-axis are calculated by the geom_bar() function under the hood.\nThis quirk is something that confuses many new learners, so make sure you internalise this.\n\n\nGo ahead and complete the following code to create a bar chart.\n\nglot_status %&gt;%\n  ggplot(aes(x = status)) +\n  ...\n\nNote how we’re using %&gt;% to pipe the glot_status data frame into the ggplot() function. This works because ggplot()’s first argument is the data, and piping is a different way of providing the first argument to a function.\nAs mentioned above, the counting for the y-axis is done automatically. R looks in the status column and counts how many times each value in the column occurs in the data frame.\n\n\n4.3 Stacked bar charts\nA special type of bar charts are the so-called stacked bar charts.\n\n\n\n\n\n\nStacked bar chart\n\n\n\nA stacked bar chart is a bar chart in which each contains a “stack” of shorter bars, each indicating the counts of some sub-groups.\nThis type of plot is useful to show how counts of something vary depending on some other grouping (in other words, when you want to count the occurrences of a categorical variable based on another categorical variable). For example:\n\nNumber of languages by endangerment status, grouped by geographic area.\nNumber of infants by head-turning preference, grouped by first language.\nNumber of past vs non-past verbs, grouped by verb class.\n\n\n\nTo create a stacked bar chart, you just need to add a new aesthetic mapping to aes(): fill. The fill aesthetic lets you fill bars or areas with different colours depending on the values of a specified column.\nLet’s make a plot on language endangerment by macro-area.\nComplete the following code by specifying that fill should be based on status.\n\nggplot(\n  glot_status,\n  aes(x = Macroarea, ...)\n) +\n  geom_bar()\n\n\n\n\n\n\n\nQuiz 4\n\n\n\nWhat is wrong in the following code?\ngestures %&gt;%\n  ggplot(aes(x = status), fill = Macroarea) +\n  geom_bar()"
  },
  {
    "objectID": "tutorials/tutorial-w03.html#faceting-and-panels",
    "href": "tutorials/tutorial-w03.html#faceting-and-panels",
    "title": "QML tutorial - Week 3",
    "section": "5 Faceting and panels",
    "text": "5 Faceting and panels\nSometimes we might want to separate the data into separate panels.\nWe can achieve that easily using faceting.\n\n5.1 Polite again\nLet’s reproduce one of the plots from above, but this time let’s use the colour aesthetic to colour the points by gender.\n\npolite %&gt;%\n  ggplot(aes(f0mn, H1H2, colour = gender)) +\n  geom_point()\n\n\n\n\n\n\n5.2 Does being a music student matter?\nThat looks great, but we want to know if being a music student has an effect on the relationship of f0mn and H1H2.\nIn the plot above, the aesthetics mappings are the following:\n\nf0mn on the x-axis.\nH1H2 on the y-axis.\ngender as colour.\n\nHow can we separate data further depending on whether the participant is a music student or not (musicstudent)?\nWe can create panels using facet_grid(). This function takes lists of variables to specify panels in rows and/or columns.\n\n\n\n\n\n\nFaceting\n\n\n\nFaceting a plot allows to split the plot into multiple panels, arranged in rows and columns, based on one or more variables.\nTo facet a plot, use the facet_grid() function.\n\n\nThe syntax is a bit strange. Check the documentation with ?facet_grid() (especially the examples) to see how it works and then complete the code below and run it to produce a plot with two column panels for musicstudent.\n\npolite %&gt;%\n  ggplot(aes(f0mn, H1H2, colour = gender)) +\n  geom_point() +\n  facet_grid(...)"
  },
  {
    "objectID": "tutorials/tutorial-w03.html#render-your-rmd-file",
    "href": "tutorials/tutorial-w03.html#render-your-rmd-file",
    "title": "QML tutorial - Week 3",
    "section": "6 Render your Rmd file!",
    "text": "6 Render your Rmd file!\nNow that you have done all of this hard work, why don’t you try and render the Rmd file you’ve been working on to an HTML file?\nGo ahead, click on the “Knitr” button and if everything works fine you should see a rendered HTML file in a second!\nNote that you will be asked to render your Rmd files for the assessments, so I recommend you try this out now."
  },
  {
    "objectID": "tutorials/tutorial-w03.html#summary",
    "href": "tutorials/tutorial-w03.html#summary",
    "title": "QML tutorial - Week 3",
    "section": "7 Summary",
    "text": "7 Summary\nThat’s all for this week!\n\n\n\n\n\n\nR Markdown\n\nR Markdown can be used to create dynamic and reproducible reports.\nMark-up languages are text-formatting systems that specify text formatting and structure using symbols or keywords. Markdown is the mark-up language R Markdown is based on.\nThe main parts of an .Rmd file are the preamble, text and code chunks.\n\nPlotting\n\nggplot2 is a plotting package from the tidyverse.\n\nTo create a basic plot, you use the ggplot() function and specify data and mapping.\nThe aes() function allows you to specify aesthetics (like axes, colours, …) in the mapping argument.\nGeometries map data values onto shapes in the plot. All geometry functions are of the type geom_*().\n\nScatter plots are created with geom_point() and can be used with two numeric variables.\nBar charts are created with geom_bar() and can be used to show the counts of different levels of a categorical variable.\nStacked bar charts are created by specifying the fill aesthetic and can be used to show counts grouped by a second categorical variable.\nFaceting allows you to split the plot in different panels, based on other variables in the data. Faceting is achieved with facet_grid().\n\n\n\n\nNext week, we will start learning about statistical modelling and while doing that you will also learn how to create density and violin plots and how to customise legends, titles and labels!"
  },
  {
    "objectID": "tutorials/tutorial-w05.html",
    "href": "tutorials/tutorial-w05.html",
    "title": "QML tutorial - Week 5",
    "section": "",
    "text": "We will model data from Winter, 2016. Taste and smell words form an affectively loaded part of the English lexicon. DOI: 10.1080/23273798.2016.1193619. The study looked at the emotional valence of sense words in English, in the domains of taste and smell.\nTo download the file with the data right-click on the following link and download the file: senses_valence.csv. (Note that tutorial files are also linked in the Syllabus). Remember to save the file in data/ in the course project folder.\nCreate a new .Rmd file first, save it in code/ and name it tutorial-w05 (the extension .Rmd is added automatically!).\nI leave to you creating title headings in your file as you please. Remember to to add knitr::opts_knit$set(root.dir = here::here()) in the setup chunk and to attach the tidyverse.\nLet’s read the data. The original data also include words from other senses, so for this tutorial we will filter the data to include only smell and taste words.\n\nsenses_valence &lt;- read_csv(\"data/senses_valence.csv\") %&gt;%\n  filter(\n    Modality %in% c(\"Smell\", \"Taste\")\n  )\n\nRows: 405 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Word, Modality\ndbl (1): Val\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThere are three columns:\n\nWord: the English word [categorical].\nModality: Taste or Smell [categorical].\nVal: emotional valence [numeric, continuous]. The higher the number the more positive the valence.\n\nLet’s quickly check the range of values in Val.\n\nrange(senses_valence$Val)\n\n[1] 4.630476 6.436000\n\n\nThis is what the density plot of Val looks like.\n\nsenses_valence %&gt;%\n  ggplot(aes(Val)) +\n  geom_density(fill = \"gray\", alpha = 0.5) +\n  geom_rug()\n\n\n\n\nAnd if we separate by Modality (by using the fill parameter), we can see that the density of taste words is shifted towards higher values than that of smell words.\n\nsenses_valence %&gt;%\n  ggplot(aes(Val, fill = Modality)) +\n  geom_density(alpha = 0.5) +\n  geom_rug()\n\n\n\n\nThe plot thus suggests that, in this sample of 72 words, taste words are generally associated with a somewhat higher emotional valence than smell words.\n\n\nNow that we got an overview of what the data looks like, let’s move onto modelling it with brm()!\nLet’s assume, as we did with the VOT values, that the emotional valence values come from a Gaussian distribution.1 In notation:\n\\[\n\\text{val} \\sim Gaussian(\\mu, \\sigma)\n\\]\nNow, we want to estimate \\(\\mu\\) so that we take into consideration whether the modality is “smell” or “taste”. In other words, we want to model valence as a function of modality, in R code Val ~ Modality.\nModality is a categorical variable (a chr character column in sense_valence) so it is coded using the default treatment coding system (this is done under the hood by R for you!). Since Modality has 2 levels, R will create for us a N-1 = 1 dummy variable (for illustration, we can imagine that it’s called modality_taste).\nThis is what the coding would look like:\n\n\n\n\nmodality_taste\n\n\n\n\nmodality = smell\n0\n\n\nmodality = taste\n1\n\n\n\n(R assigns the reference level, i.e., 0, to “smell”, because “s” comes before “t” in the alphabet.)\nNow we allow \\(\\mu\\) to vary depending on modality.\n\n\\[\n\\mu = \\beta_0 + \\beta_1 \\cdot modality_{Taste}\n\\]\nLet’s unpack that. There’s a bit of algebra in what follows, so take your time if this is a bit out of your comfort zone. But it’s worth it—comfort with basic algebra will also help you become more comfortable working with linear models.\n\n\\(\\beta_0\\) is the mean valence \\(\\mu\\) when [modality = smell]. That’s because the variable \\(modality_{Taste}\\) takes on the value 0 when [modality = smell] (as we can see from the table above). Multiplying by 0 means that \\(\\beta_1\\) vanishes from the equation, and all that’s left is \\(\\beta_0\\).\n\n\\[\n\\begin{aligned}\n\\mu &= \\beta_0 + \\beta_1 \\cdot modality_{Taste}\\\\\n\\mu_{mod = smell} &= \\beta_0 + \\beta_1 \\cdot 0 \\\\\n\\mu_{mod = smell} &= \\beta_0 \\\\\n\\end{aligned}\n\\]\n\n\\(\\beta_1\\) is the difference in mean valence \\(\\mu\\) between the mean valence when [modality = smell] and the mean valence when [modality = taste]. That’s because \\(modality_{Taste}\\) takes on the value 1 when [modality = taste], and multiplying \\(\\beta_1\\) by 1 leaves it unchanged in the equation.\n\n\\[\n\\begin{aligned}\n\\mu &= \\beta_0 + \\beta_1 \\cdot modality_{Taste}\\\\\n\\mu_{mod = taste} &= \\beta_0 + \\beta_1 \\cdot 1 \\\\\n\\mu_{mod = taste} &= \\beta_0 + \\beta_1 \\\\\n\\end{aligned}\n\\]\n\nHow do we know that \\(\\beta_1\\) really represents the difference between the mean valence when [modality = smell] and the mean valence when [modality = taste]? Remember that \\(\\mu_{mod = smell} = \\beta_0\\), as we said in the first point above. We can substitute this into the equation from the last point, and then isolate \\(\\beta_1\\) step by step as follows:\n\n\\[\n\\begin{aligned}\n\\mu_{mod = taste} &= \\beta_0 + \\beta_1 \\\\\n\\mu_{mod = taste} &= \\mu_{mod=smell} + \\beta_1 \\\\\n\\mu_{mod = taste} - \\mu_{mod=smell} &= \\beta_1\n\\end{aligned}\n\\]\nSo, \\(\\beta_1\\) really is the difference between the means of these two levels of Modality.\nNow we can define the probability distributions of \\(\\beta_0\\) and \\(\\beta_1\\).\n\\[\n\\beta_0 \\sim Gaussian(\\mu_0, \\sigma_0)\n\\]\n\\[\n\\beta_1 \\sim Gaussian(\\mu_1, \\sigma_1)\n\\]\nAnd the usual for \\(\\sigma\\).\n\\[\n\\sigma \\sim TruncGaussian(\\mu_2, \\sigma_2)\n\\]\nAll together, we need to estimate the following parameters: \\(\\mu_0, \\sigma_0, \\mu_1, \\sigma_1, \\mu_2, \\sigma_2\\).\n\n\n\nNow we know what the model will do, we can run it.\nBefore that though, create a folder called cache/ in the data/ folder of the RStudio project of the course. We will use this folder to save the output of model fitting so that you don’t have to refit the model every time. (This is useful because as models get more and more complex, they can take quite a while to fit.)\nAfter you have created the folder, run the following code.\n\nval_bm &lt;- brm(\n  Val ~ Modality,\n  family = gaussian(),\n  data = senses_valence,\n  backend = \"cmdstanr\",\n  # Save model output to file\n  file = \"data/cache/val_bm\"\n)\n\nThe model will be fitted and saved in data/cache/ with the file name val_bm.rds. If you now re-run the same code again, you will notice that brm() does not fit the model again, but rather reads it from the file (no output is shown, but trust me, it works! Check the contents of data/cache/ to see for yourself.).\n\n\n\nLet’s inspect the model summary.\n\nsummary(val_bm)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Val ~ Modality \n   Data: senses_valence (Number of observations: 72) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept         5.47      0.06     5.35     5.59 1.00     3800     3149\nModalityTaste     0.34      0.08     0.18     0.50 1.00     3866     2984\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.32      0.03     0.27     0.38 1.00     3136     2573\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nLook at the Population-Level Effects part of the summary.\n\n\nPopulation-Level Effects: \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept         5.47      0.06     5.35     5.59 1.00     3800     3149\nModalityTaste     0.34      0.08     0.18     0.50 1.00     3866     2984\n\n\nWe get two “effects” or coefficients:\n\nIntercept: this is our \\(\\beta_0\\).\nModalityTaste: this is our \\(\\beta_1\\).\n\nEach coefficient has an Estimate and an Est.Error (estimate error). As we have seen last week, these are the mean and SD of the probability distribution of the coefficients.\nIn notation:\n\\[\n\\beta_0 = Gaussian(5.47, 0.06)\n\\]\n\\[\n\\beta_1 = Gaussian(0.34, 0.08)\n\\]\nThis means that:\n\nThe probability distribution of the mean valence when [modality = smell] is a Gaussian distribution with mean = 5.47 and SD = 0.06.\nThe probability distribution of the difference between the mean valence of [modality = taste] and that of [modality = smell] is a Gaussian distribution with mean = 0.34 and SD = 0.08.\n\nNow look at the Credible Intervals (CrIs, or CI in the model summary) of the coefficients. Based on their CrIs, we can say that:\n\nWe can be 95% confident that (or, there is 95% probability that), based on the data and the model, the mean emotional valence of smell words is between 5.35 and 5.59.\nThere is a 95% probability that the difference in mean emotional valence between taste and smell words is between 0.18 and 0.5. In other words, the emotional valence increases by 0.18 to 0.5 in taste words relative to smell words."
  },
  {
    "objectID": "tutorials/tutorial-w05.html#emotional-valence-of-sense-words",
    "href": "tutorials/tutorial-w05.html#emotional-valence-of-sense-words",
    "title": "QML tutorial - Week 5",
    "section": "",
    "text": "We will model data from Winter, 2016. Taste and smell words form an affectively loaded part of the English lexicon. DOI: 10.1080/23273798.2016.1193619. The study looked at the emotional valence of sense words in English, in the domains of taste and smell.\nTo download the file with the data right-click on the following link and download the file: senses_valence.csv. (Note that tutorial files are also linked in the Syllabus). Remember to save the file in data/ in the course project folder.\nCreate a new .Rmd file first, save it in code/ and name it tutorial-w05 (the extension .Rmd is added automatically!).\nI leave to you creating title headings in your file as you please. Remember to to add knitr::opts_knit$set(root.dir = here::here()) in the setup chunk and to attach the tidyverse.\nLet’s read the data. The original data also include words from other senses, so for this tutorial we will filter the data to include only smell and taste words.\n\nsenses_valence &lt;- read_csv(\"data/senses_valence.csv\") %&gt;%\n  filter(\n    Modality %in% c(\"Smell\", \"Taste\")\n  )\n\nRows: 405 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Word, Modality\ndbl (1): Val\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThere are three columns:\n\nWord: the English word [categorical].\nModality: Taste or Smell [categorical].\nVal: emotional valence [numeric, continuous]. The higher the number the more positive the valence.\n\nLet’s quickly check the range of values in Val.\n\nrange(senses_valence$Val)\n\n[1] 4.630476 6.436000\n\n\nThis is what the density plot of Val looks like.\n\nsenses_valence %&gt;%\n  ggplot(aes(Val)) +\n  geom_density(fill = \"gray\", alpha = 0.5) +\n  geom_rug()\n\n\n\n\nAnd if we separate by Modality (by using the fill parameter), we can see that the density of taste words is shifted towards higher values than that of smell words.\n\nsenses_valence %&gt;%\n  ggplot(aes(Val, fill = Modality)) +\n  geom_density(alpha = 0.5) +\n  geom_rug()\n\n\n\n\nThe plot thus suggests that, in this sample of 72 words, taste words are generally associated with a somewhat higher emotional valence than smell words.\n\n\nNow that we got an overview of what the data looks like, let’s move onto modelling it with brm()!\nLet’s assume, as we did with the VOT values, that the emotional valence values come from a Gaussian distribution.1 In notation:\n\\[\n\\text{val} \\sim Gaussian(\\mu, \\sigma)\n\\]\nNow, we want to estimate \\(\\mu\\) so that we take into consideration whether the modality is “smell” or “taste”. In other words, we want to model valence as a function of modality, in R code Val ~ Modality.\nModality is a categorical variable (a chr character column in sense_valence) so it is coded using the default treatment coding system (this is done under the hood by R for you!). Since Modality has 2 levels, R will create for us a N-1 = 1 dummy variable (for illustration, we can imagine that it’s called modality_taste).\nThis is what the coding would look like:\n\n\n\n\nmodality_taste\n\n\n\n\nmodality = smell\n0\n\n\nmodality = taste\n1\n\n\n\n(R assigns the reference level, i.e., 0, to “smell”, because “s” comes before “t” in the alphabet.)\nNow we allow \\(\\mu\\) to vary depending on modality.\n\n\\[\n\\mu = \\beta_0 + \\beta_1 \\cdot modality_{Taste}\n\\]\nLet’s unpack that. There’s a bit of algebra in what follows, so take your time if this is a bit out of your comfort zone. But it’s worth it—comfort with basic algebra will also help you become more comfortable working with linear models.\n\n\\(\\beta_0\\) is the mean valence \\(\\mu\\) when [modality = smell]. That’s because the variable \\(modality_{Taste}\\) takes on the value 0 when [modality = smell] (as we can see from the table above). Multiplying by 0 means that \\(\\beta_1\\) vanishes from the equation, and all that’s left is \\(\\beta_0\\).\n\n\\[\n\\begin{aligned}\n\\mu &= \\beta_0 + \\beta_1 \\cdot modality_{Taste}\\\\\n\\mu_{mod = smell} &= \\beta_0 + \\beta_1 \\cdot 0 \\\\\n\\mu_{mod = smell} &= \\beta_0 \\\\\n\\end{aligned}\n\\]\n\n\\(\\beta_1\\) is the difference in mean valence \\(\\mu\\) between the mean valence when [modality = smell] and the mean valence when [modality = taste]. That’s because \\(modality_{Taste}\\) takes on the value 1 when [modality = taste], and multiplying \\(\\beta_1\\) by 1 leaves it unchanged in the equation.\n\n\\[\n\\begin{aligned}\n\\mu &= \\beta_0 + \\beta_1 \\cdot modality_{Taste}\\\\\n\\mu_{mod = taste} &= \\beta_0 + \\beta_1 \\cdot 1 \\\\\n\\mu_{mod = taste} &= \\beta_0 + \\beta_1 \\\\\n\\end{aligned}\n\\]\n\nHow do we know that \\(\\beta_1\\) really represents the difference between the mean valence when [modality = smell] and the mean valence when [modality = taste]? Remember that \\(\\mu_{mod = smell} = \\beta_0\\), as we said in the first point above. We can substitute this into the equation from the last point, and then isolate \\(\\beta_1\\) step by step as follows:\n\n\\[\n\\begin{aligned}\n\\mu_{mod = taste} &= \\beta_0 + \\beta_1 \\\\\n\\mu_{mod = taste} &= \\mu_{mod=smell} + \\beta_1 \\\\\n\\mu_{mod = taste} - \\mu_{mod=smell} &= \\beta_1\n\\end{aligned}\n\\]\nSo, \\(\\beta_1\\) really is the difference between the means of these two levels of Modality.\nNow we can define the probability distributions of \\(\\beta_0\\) and \\(\\beta_1\\).\n\\[\n\\beta_0 \\sim Gaussian(\\mu_0, \\sigma_0)\n\\]\n\\[\n\\beta_1 \\sim Gaussian(\\mu_1, \\sigma_1)\n\\]\nAnd the usual for \\(\\sigma\\).\n\\[\n\\sigma \\sim TruncGaussian(\\mu_2, \\sigma_2)\n\\]\nAll together, we need to estimate the following parameters: \\(\\mu_0, \\sigma_0, \\mu_1, \\sigma_1, \\mu_2, \\sigma_2\\).\n\n\n\nNow we know what the model will do, we can run it.\nBefore that though, create a folder called cache/ in the data/ folder of the RStudio project of the course. We will use this folder to save the output of model fitting so that you don’t have to refit the model every time. (This is useful because as models get more and more complex, they can take quite a while to fit.)\nAfter you have created the folder, run the following code.\n\nval_bm &lt;- brm(\n  Val ~ Modality,\n  family = gaussian(),\n  data = senses_valence,\n  backend = \"cmdstanr\",\n  # Save model output to file\n  file = \"data/cache/val_bm\"\n)\n\nThe model will be fitted and saved in data/cache/ with the file name val_bm.rds. If you now re-run the same code again, you will notice that brm() does not fit the model again, but rather reads it from the file (no output is shown, but trust me, it works! Check the contents of data/cache/ to see for yourself.).\n\n\n\nLet’s inspect the model summary.\n\nsummary(val_bm)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Val ~ Modality \n   Data: senses_valence (Number of observations: 72) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept         5.47      0.06     5.35     5.59 1.00     3800     3149\nModalityTaste     0.34      0.08     0.18     0.50 1.00     3866     2984\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.32      0.03     0.27     0.38 1.00     3136     2573\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nLook at the Population-Level Effects part of the summary.\n\n\nPopulation-Level Effects: \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept         5.47      0.06     5.35     5.59 1.00     3800     3149\nModalityTaste     0.34      0.08     0.18     0.50 1.00     3866     2984\n\n\nWe get two “effects” or coefficients:\n\nIntercept: this is our \\(\\beta_0\\).\nModalityTaste: this is our \\(\\beta_1\\).\n\nEach coefficient has an Estimate and an Est.Error (estimate error). As we have seen last week, these are the mean and SD of the probability distribution of the coefficients.\nIn notation:\n\\[\n\\beta_0 = Gaussian(5.47, 0.06)\n\\]\n\\[\n\\beta_1 = Gaussian(0.34, 0.08)\n\\]\nThis means that:\n\nThe probability distribution of the mean valence when [modality = smell] is a Gaussian distribution with mean = 5.47 and SD = 0.06.\nThe probability distribution of the difference between the mean valence of [modality = taste] and that of [modality = smell] is a Gaussian distribution with mean = 0.34 and SD = 0.08.\n\nNow look at the Credible Intervals (CrIs, or CI in the model summary) of the coefficients. Based on their CrIs, we can say that:\n\nWe can be 95% confident that (or, there is 95% probability that), based on the data and the model, the mean emotional valence of smell words is between 5.35 and 5.59.\nThere is a 95% probability that the difference in mean emotional valence between taste and smell words is between 0.18 and 0.5. In other words, the emotional valence increases by 0.18 to 0.5 in taste words relative to smell words."
  },
  {
    "objectID": "tutorials/tutorial-w05.html#plotting-probability-distributions",
    "href": "tutorials/tutorial-w05.html#plotting-probability-distributions",
    "title": "QML tutorial - Week 5",
    "section": "2 Plotting probability distributions",
    "text": "2 Plotting probability distributions\nThey say a plot is better than a thousand words, so why don’t we plot the probability distributions of the Intercept (\\(\\beta_0\\)) and ModalityTaste (\\(\\beta_1\\)) coefficients?\nBefore we do, just a quick terminological clarification.\n\n2.1 Posterior probability distributions\n\n\n\n\n\n\nPosterior probability distribution\n\n\n\nThe probability distributions of model coefficients obtained through model fitting with brm() are called posterior probability distributions.\n\n\nWe will see why they are called that and what prior probability distributions are, but for the time being, just remember that when we talk about posterior probability distributions we are talking about the probability distributions of the model coefficients.\nSo, how do we plot posterior probability distributions?\nThere are different methods, each of which has its own advantages and disadvantages. Some methods require extra packages, while others work with the packages you already know of.\nTo simply things, we will use a method that works out of the box with just ggplot2. The first step to plot posterior distributions is to extract the values to be plotted from the model output.\n\n\n2.2 Posterior draws\nWhich values, you might ask? The coefficient values obtained with the MCMC draws. (Remember we talked about Markov Chain Monte Carlo iterations last week?). In order to estimate the coefficients, brm() runs a number of MCMC iterations and at each iteration a value for each coefficient in the model is proposed (this is a simplification, if you want to know more about the inner working of MCMCs, check the Statistical (Re)Thinking book).\nAt the end, what you get is a list of values for each coefficient in the model. These are called the posterior draws. The probability distribution of the posterior draws of a coefficient is the posterior probability distribution of that coefficient.\nAnd, if you take the mean and standard deviation of the posterior draws of a coefficient, you get the Estimate and Est.Error values in the model output!\nYou can easily extract the posterior draws of all the model’s coefficients with the as_draws_df() function from the brms package.\n\nval_bm_draws &lt;- as_draws_df(val_bm)\nval_bm_draws\n\n\n\n  \n\n\n\nCheck the first three columns of the tibble (don’t worry about the other columns for now—those are technical things that are important for brms, but not so important for us). The first three columns are our \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma\\) from the model formulae above!\n\nb_Intercept: \\(\\beta_0\\).\nb_ModalityTaste: \\(\\beta_1\\).\nsigma: \\(\\sigma\\).\n\n(Why is there a b at the beginning of b_Intercept and b_ModalityTaste? Well, “b” is for “beta”, and it’s brms’s way of telling us what kind of coefficient we’re working with.)\nFantastic! Now that we have the posterior draws as a nice tibble, we can use geom_density() from ggplot2 to plot the probability density, a.k.a., the posterior probability distribution, for each coefficient.\n\n\n2.3 Plotting posteriors\nLet’s start with the posterior probability, or posterior for short, of b_Intercept.\n\nval_bm_draws %&gt;%\n  ggplot(aes(b_Intercept)) +\n  geom_density(fill = \"gray\", alpha = 0.5) +\n  labs(\n    title = \"Posterior probability distribution of Intercept\"\n  )\n\n\n\n\nNice, huh?\nNow with b_ModalityTaste.\n\nval_bm_draws %&gt;%\n  ggplot(aes(b_ModalityTaste)) +\n  geom_density(fill = \"gray\", alpha = 0.5) +\n  labs(\n    title = \"Posterior probability distribution of ModalityTaste\"\n  )\n\n\n\n\nIf you feel like it, go ahead and plot the posterior distribution of sigma as well."
  },
  {
    "objectID": "tutorials/tutorial-w05.html#plotting-conditional-probability-distributions",
    "href": "tutorials/tutorial-w05.html#plotting-conditional-probability-distributions",
    "title": "QML tutorial - Week 5",
    "section": "3 Plotting conditional probability distributions",
    "text": "3 Plotting conditional probability distributions\nThis is all great, but what if you wanted to know the probability distribution of mean emotional valence in either smell or taste words? So far, we only know the probability distribution of mean valence for smell words (\\(\\beta_0\\)), and how that valence is changed for taste words (\\(\\beta_1\\)). We don’t yet know the probability distribution of mean emotional valence for taste words. How do we find that distribution?\nThis is where conditional posterior probability distributions (or conditional probabilities for short) come in!\nThey are posterior probability distributions like the ones we have plotted above, but they are conditional on specific values of each predictor in the model.\nIn the val_bm model we had only included one predictor (we will see how to include more in the coming weeks): Modality, which has two levels Smell and Taste.\nHow do we obtain the conditional probabilities of emotional valence for smell and taste words respectively?\nThe formula of \\(\\mu\\) above can help us solve the mystery.\n\\[\n\\mu = \\beta_0 + \\beta_1 \\cdot modality_{Taste}\n\\]\nThe conditional probability of mean emotional valence in smell words is just \\(\\beta_0\\). Why? Remember that when [modality = smell], \\(modality_{Taste}\\) is 0, so:\n\\[\n\\beta_0 + \\beta_1 \\cdot 0 = \\beta_0\n\\]\nIn other words, the conditional probability of mean emotional valence in smell words is equal to the posterior probability of b_Intercept. This is so because Modality is coded using the treatment coding system (nothing mysterious here).\nBut what about taste words? Here’s where we need to do some more maths/data processing.\nWhen [modality = taste], \\(modality_{Taste}\\) is 1, so:\n\\[\n\\beta_0 + \\beta_1 \\cdot 1 = \\beta_0 + \\beta_1\n\\]\nSo to get the conditional posterior probability of mean emotional valence for taste words, we need to sum the posterior draws of b_Intercept (\\(\\beta_0\\)) and b_ModalityTaste (\\(\\beta_1\\)): \\(\\beta_0 + \\beta_1\\).\nIt couldn’t be easier than using the mutate() function from dplyr. Remember that mutate() creates a new column (here, called taste) based on the values of other columns. Here, we’re just adding together the values in b_Intercept and those in b_ModalityTaste (a.k.a., the posterior draws for each of those coefficients).\n\nval_bm_draws &lt;- val_bm_draws %&gt;%\n  mutate(\n    taste = b_Intercept + b_ModalityTaste\n  )\n\nNow, we can just plot the probability density of taste to get the conditional posterior probability distribution of mean emotional valence for taste words.\n\nval_bm_draws %&gt;%\n  ggplot(aes(taste)) +\n  geom_density(fill = \"gray\", alpha = 0.5) +\n  labs(\n    title = \"Conditional distribution of emotional valence in taste words\"\n  )\n\n\n\n\nWhat if you want to show both the conditional probability of emotional valence in smell and taste words? Easy!2\n\nval_bm_draws %&gt;%\n  ggplot() +\n  geom_density(aes(b_Intercept), fill = \"red\", alpha = 0.5) +\n  geom_density(aes(taste), fill = \"blue\", alpha = 0.5) +\n  labs(\n    title = \"Conditional distributions of mean emotional valence in smell vs taste words\"\n  )\n\n\n\n\nBased on this plot, we can say that the model suggests a different mean emotional valence for smell vs. taste words, and that taste words (blue) have a more positive emotional valence than smell words (red).\nTo reiterate from above, based on the 95% CrI of b_ModalityTaste, there is a 95% probability that the mean emotional valence of taste words is 0.18 to 0.5 greater than that of smell words.\nYou might ask now: Is this difference relevant? Unfortunately, statistics cannot help you to answer that question (remember, we imbue numbers with meaning). Only conceptual theories of lexical emotional valence can (or cannot)!"
  },
  {
    "objectID": "tutorials/tutorial-w05.html#morphological-processing-and-reaction-times",
    "href": "tutorials/tutorial-w05.html#morphological-processing-and-reaction-times",
    "title": "QML tutorial - Week 5",
    "section": "4 Morphological processing and reaction times",
    "text": "4 Morphological processing and reaction times\nSo far, we’ve seen how to work with treatment-coded variables with only two levels. In this last section, we’ll take a look at how to use dummy coding for a variable with three levels.\nYou might remember the shallow.csv data we used in Week 2, from Song et al. 2020. Second language users exhibit shallow morphological processing. DOI: 10.1017/S0272263120000170.\nThe study compared results of English L1 vs L2 participants and of left- vs right-branching words, but for this tutorial we will only be looking at the L1 and left-branching data.\nThe data file also contains data from the filler items, which we filter out.\n\nshallow &lt;- read_csv(\"data/shallow.csv\") %&gt;%\n  filter(\n    Group == \"L1\",\n    Branching == \"Left\",\n    Critical_Filler == \"Critical\",\n    RT &gt; 0\n  )\n\nRows: 6500 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Group, ID, List, Target, Critical_Filler, Word_Nonword, Relation_ty...\ndbl (3): ACC, RT, logRT\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nshallow\n\n\n\n  \n\n\n\nThe study consisted of a lexical decision task in which participants where first shown a prime, followed by a target word for which they had to indicate whether it was a real word or a nonce word.\nThe prime word belonged to one of three possible groups (Releation_type in the data) each of which refers to the morphological relation of the prime and the target word:\n\nUnrelated: for example, prolong (assuming unkindness as target, [[un-kind]-ness]).\nConstituent: unkind.\nNonConstituent: kindness.\n\nThe expectation is that lexical decision for native English participants should be facilitated in the Constituent condition, but not in the Unrelated and NonConstituent conditions (if you are curious as to why that is the expectation, I refer you to the paper).\nLet’s interpret that as:\n\nThe Constituent condition should elicit shorter reaction times than the other two conditions.\n\nBefore moving on, let’s visualise the reaction times (RT) values which are in milliseconds.\n\nshallow %&gt;%\n  ggplot(aes(RT)) +\n  geom_density() +\n  geom_rug()\n\n\n\n\nYou will notice that RT can only be positive: reaction time is a numeric continuous variable, bounded to positive numbers only!\nRemember how we talked above about choosing an appropriate distribution to model your data? Variables that are bounded to positive numbers are known not to be distributed according to a Gaussian distribution. Each case might be a bit different, but when a variable is bounded (e.g., by zero), it is very safe to assume that the values of the variable do not follow a Gaussian distribution.3\n\n4.1 Log-transformation\nBut we have a trick in our pocket: just calculate the logarithm of the values and they will conform to a Gaussian distribution! This is a common trick in psycholinguistics for modelling reaction time data. You can calculate the logarithm (or log) of a number in R using the log() function. Calculating the logs of a variable is known as a log-transformation.\nI will illustrate what this looks like with the first five values of RT in shallow.\n\nRT &lt;- c(603, 739, 370, 821, 1035)\nRT\n\n[1]  603  739  370  821 1035\n\nlog(RT)\n\n[1] 6.401917 6.605298 5.913503 6.710523 6.942157\n\n\nSo the log of 603 is 6.4, the log of 739 is 6.6 and so on.\nThe shallow data table already has a column with the log-transformed RTs: logRT. So let’s plot that now.\n\nshallow %&gt;%\n  ggplot(aes(logRT)) +\n  geom_density() +\n  geom_rug()\n\nWarning: Removed 4 rows containing non-finite values (`stat_density()`).\n\n\n\n\n\nWarning: it is usually not possible to rely on the shape of the probability density to determine if the probability is Gaussian or not! Rather, ask yourself the following question: are the values bounded to positive numbers only? If the answer is “yes”, then the variable is not Gaussian and you know you can log-transform it.\n\n\n\n\n\n\nLog-normal distribution\n\n\n\nVariables that are Gaussian (normal) when they are log-transformed follow the log-normal distribution.\n\n\nNow let’s look at a violin and strip plot with Relation_type on the x-axis and logRT on the y-axis.\n\nshallow %&gt;%\n  ggplot(aes(Relation_type, logRT, fill = Relation_type)) +\n  geom_violin() +\n  geom_jitter(width = 0.1, alpha = 0.2)\n\nWarning: Removed 4 rows containing non-finite values (`stat_ydensity()`).\n\n\nWarning: Removed 4 rows containing missing values (`geom_point()`).\n\n\n\n\n\nBased on this, we can see that the bulk of the distribution (the place where the violin is wider) falls somewhat lower on the log-RT scale for the Constituent condition relative to the other conditions.\n\n\n4.2 Ordering levels of a categorical predictor\nBy default, the levels in Relation_type follow the order: Constituent, NonConstituent, Unrelated (this is the default alphabetical order).\nThere is nothing bad with this per se, but maybe we can reorder them so that they are in this order: Unrelated, NonConstituent, Constituent.\nThis might make a bit more sense from a conceptual point of view (you can think of the unrelated condition as the “baseline” condition).\nTo order levels in R, we can use a factor variable/column.\n\nshallow &lt;- shallow %&gt;%\n  mutate(\n    Relation_type = factor(Relation_type, levels = c(\"Unrelated\", \"NonConstituent\", \"Constituent\"))\n  )\n\nThis overwrites the Relation_type column so that the levels are in the specified order. You can check this in the Environment tab: click on the blue arrow next to shallow to see the column list and now Relation_type is a Factor column with 3 levels.\n\n\n4.3 Model RTs by relation type\nNow it’s your turn! With the following guidance, go ahead and run a model with brm() to investigate the effect of relation type on log-RT. Inspect the model summary and plot the posterior probabilities.\nRelation_type is a categorical variable with three levels, so here’s what treatment coding looks like (there will be N-1 = 3-1 = 2 dummy variables).\n\n\n\n\nrelation_ncons\nrelation_cons\n\n\n\n\nrelation = unrelated\n0\n0\n\n\nrelation = non-constituent\n1\n0\n\n\nrelation = constituent\n0\n1\n\n\n\nHere are a few formulae that describe in mathematical notation the model that we’ll need to fit. Try to go through them and unpack them based on what you’ve learned above and in the lecture.\n\\[\n\\begin{aligned}\n\\text{logRT} &\\sim Gaussian(\\mu, \\sigma) \\\\\n\\mu &= \\beta_0 + \\beta_1 \\cdot relation_{ncons} + \\beta_2 \\cdot relation_{cons} \\\\\n\\beta_0 &\\sim Gaussian(\\mu_0, \\sigma_0)\\\\\n\\beta_1 &\\sim Gaussian(\\mu_1, \\sigma_1)\\\\\n\\beta_2 &\\sim Gaussian(\\mu_2, \\sigma_2)\\\\\n\\sigma &\\sim TruncGaussian(\\mu_3, \\sigma_3)\n\\end{aligned}\n\\]\nFocus especially on \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\beta_2\\). What do they correspond to?\nFit the model with brm(). What does the R formula for this model look like?\nCheck the model summary. What can you say based on the estimates and the CrIs?\nPlot the posterior probability distributions of \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\beta_2\\) and the conditional probability distributions of mean log-RTs for each relation type.\nA hint for the conditional probability distributions: to plot these you need to sum the posterior draws of the beta coefficients depending on the level of relation type. Use the formula of \\(\\mu\\) to guide you: the process is the same as with the emotional valence data above, but now there are 3 levels instead of two."
  },
  {
    "objectID": "tutorials/tutorial-w05.html#summary",
    "href": "tutorials/tutorial-w05.html#summary",
    "title": "QML tutorial - Week 5",
    "section": "5 Summary",
    "text": "5 Summary\n\n\n\n\n\n\nTreatment coding\n\nCategorical predictors need to be coded as numbers.\nN-1 dummy variables, where N is number of levels in the predictor.\nLevel ordering is alphabetical but you can specify your own.\nNOTE: you don’t have to apply treatment coding yourself! It’s done under the hood by R.\n\nInterpreting coefficients\n\nThe Intercept \\(\\beta_0\\) is the mean of the reference level.\nThe other \\(\\beta\\)’s are the difference of the other levels relative to the reference level.\n\nConditional posterior probability distributions\n\nTo calculate the conditional posterior probability distributions (conditional probabilities for short) you need to extract the draws of the model with as_draws_df().\n\nLog-normal distribution\n\nContinuous variables that are Gaussian when transformed to logs follow the log-normal distribution.\nYou can fit a Gaussian model to log-transformed values if the variable is log-normal."
  },
  {
    "objectID": "tutorials/tutorial-w05.html#footnotes",
    "href": "tutorials/tutorial-w05.html#footnotes",
    "title": "QML tutorial - Week 5",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that, in real-life research contexts, you should decide which distribution to use for different outcome variables by drawing on previous research that has assessed the possible distribution families for those variables, on domain-specific expertise or common-sense. You will see that in most cases with linguistic phenomena we know very little about the nature of the variables we work with, hence it can be difficult to know which family of distributions to use.↩︎\nThe following code is a bit of a hack. There is a better way to do this using pivot_longer() from the tidyr package. We will learn how to use this and other functions from that package later in the course.↩︎\nIn fact, we have assumed above that emotional valence is distributed according to a Gaussian distribution, and we got lucky that this assumption in principle holds for the values in the data sample, because emotional valence as measured in the study is actually bounded between 1 and 8. In most cases you won’t be lucky, so always carefully think about the nature of your variables. Again, only conceptual theories of the phenomena you are investigating will be able to help.↩︎"
  },
  {
    "objectID": "tutorials/tutorial-w08.html",
    "href": "tutorials/tutorial-w08.html",
    "title": "qml tutorial – Week 8",
    "section": "",
    "text": "This tutorial has two sections, and each will guide you through one example of an interaction between two binary variables. The first section revisits the morphological processing data from the lecture. The second section focuses on a different data set, this time with a continuous outcome. This analysis will require you to adapt the code from earlier in the tutorial, as well as recall from earlier weeks how to fit and interpret a Gaussian model.\nAs usual, though, first things first:"
  },
  {
    "objectID": "tutorials/tutorial-w08.html#lexical-decision-accuracy",
    "href": "tutorials/tutorial-w08.html#lexical-decision-accuracy",
    "title": "qml tutorial – Week 8",
    "section": "1 Lexical decision accuracy",
    "text": "1 Lexical decision accuracy\n\n1.1 Read and process the data\nWe’ve played with the data in shallow.csv before, way back in week 2 (ah, the good old days!). This means you might already have it in your data/ folder, but if not, you can download it again from here: shallow.csv.\nUse the usual read_csv() function to read in the dataset, and assign it to a variable called shallow.\n\nshallow &lt;- ...\n\nRun the following code to filter this data frame for only the data we need for this example.\n\nshallow &lt;- shallow %&gt;% \n  filter(\n    Group == \"L1\",\n    Critical_Filler == \"Critical\",\n    RT &gt; 0,\n    Relation_type != \"NonConstituent\"\n  )\n\nNext, we’ll prepare our variables for analysis.\nThe outcome variable is ACC, accuracy (whether the response was correct or not). This variable is binary and it is now coded using 0 (for “incorrect”) and 1 (for “correct”). This type of coding (using 0/1) is quite common. However, it makes it a bit more difficult to keep track of what 0 and 1 mean, especially when you have a bunch of binary variables.\nAs a general recommendation, it is best to code binary variables using meaningful labels. In the case of ACC, better, more transparent labels would be incorrect and correct. We will create a new column based on ACC with these new labels below.\nThe data also contains the two predictor variables we want:\n\nRelation_type, with two levels: Unrelated and Constituent.\nBranching, with two levels: Left and Right.\n\nCan you see how the code below will ensure that each variable has the desired ordering for their levels? (Remember that by default the ordering is alphabetical!)\n\nshallow &lt;- shallow %&gt;% \n  mutate(\n    # Create Accuracy column based on ACC with desired order\n    Accuracy      = ifelse(ACC == 1, \"correct\", \"incorrect\"), \n    # Order factor levels\n    Accuracy      = factor(Accuracy, levels = c(\"incorrect\", \"correct\")),\n    Relation_type = factor(Relation_type, levels = c(\"Unrelated\", \"Constituent\")),\n    Branching     = factor(Branching, levels = c(\"Left\", \"Right\"))\n  )\n\n\n\n1.2 Visualise the data\nUse shallow to create a bar plot that looks like this one. Show the proportion of correct/incorrect responses by Relation_type, faceting by Branching.\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nA couple hints:\n\nTo make geom_bar() display proportions instead of counts, you can write geom_bar(position = \"fill\").\nTo include the text “Branching” in the facet labels, give the function facet_grid() the following arguments:\n\nFirst, ~ Branching, to tell it to facet by branching,\nand then labeller = labeller(Branching = label_both).\n\n\n\n\n\nNotice that this plot reveals, among other things, a difference of differences: The difference between Relation_type == Unrelated and Relation_Type == Constituent is bigger when Branching == Left, and smaller when Branching == Right. The only way to capture this difference of differences is to include an interaction between Relation_type and Branching.\n\n\n\n\n\n\nExtra: More on contrasts\n\n\n\n\n\nThe contrasts() function takes as its argument a factor, and returns the coding that R will use to represent this factor in a model.\n\n1.2.1 Accuracy\n\ncontrasts(shallow$Accuracy)\n\n          correct\nincorrect       0\ncorrect         1\n\n\nWhich level of Accuracy, our outcome variable, corresponds to a “success” in Bernoulli terms? How do we know? What does this mean for the estimates that the model will produce?\n\n\n1.2.2 Relation_Type\n\ncontrasts(shallow$Relation_type)\n\n            Constituent\nUnrelated             0\nConstituent           1\n\n\nBecause we are using 0/1 treatment coding, Relation_type’s \\(\\beta\\) coefficient tells us the effect of moving from the baseline level to the non-baseline level. In this case, the baseline is Unrelated, coded as 0. This means that the \\(\\beta\\) coefficient will represent the estimated effect of moving from Unrelated to Constituent.\nIn other words:\n\nRelation_type’s \\(\\beta\\) coefficient’s 95% CrI will be mostly or entirely positive if greater accuracy is associated with Relation_type == Constituent than with Relation_type == Unrelated.\nIts \\(\\beta\\) coefficient’s 95% CrI will be mostly or entirely negative if lower accuracy is associated with Relation_type == Constituent than with Relation_type == Unrelated.\nIts \\(\\beta\\) coefficient’s 95% CrI will be centered around zero if there is no clear association between accuracy and relation type.\n\nBased on this information and on the bar plot above, do you expect that the coefficient’s posterior distribution will be largely positive, negative, or around zero?\n\n\n1.2.3 Branching\n\ncontrasts(shallow$Branching)\n\n      Right\nLeft      0\nRight     1\n\n\nHere, the baseline is Left, so the \\(\\beta\\) for Branching will represent the estimated effect of moving from Left to Right.\nIn other words:\n\nBranching’s \\(\\beta\\) coefficient’s 95% CrI will be mostly or entirely positive if greater accuracy is associated with Branching == Right than with Branching == Left.\nIts \\(\\beta\\) coefficient’s 95% CrI will be mostly or entirely negative if lower accuracy is associated with Branching == Right than with Branching == Left.\nIts \\(\\beta\\) coefficient’s 95% CrI will be centered around zero if there is no clear association between accuracy and branching.\n\nAgain, based on this, what kind of expectations do you have about this coefficient’s posterior distribution?\n\n\n\n\n\n\n1.3 Fit the model\nFrom the lecture slides, here is the mathematical specification of the model we want to fit.\n\\[\n\\begin{aligned}\n\\text{acc} & \\sim Bernoulli(p) \\\\\nlogit(p) & = \\beta_0 + (\\beta_1 \\times relation) + (\\beta_2 \\times branch) + (\\beta_3 \\times relation \\times branch)\\\\\n\\beta_0 & \\sim Gaussian(\\mu_0, \\sigma_0) \\\\\n\\beta_1 & \\sim Gaussian(\\mu_1, \\sigma_1) \\\\\n\\beta_2 & \\sim Gaussian(\\mu_2, \\sigma_2) \\\\\n\\beta_3 & \\sim Gaussian(\\mu_3, \\sigma_3) \\\\\n\\end{aligned}\n\\]\nTry to understand what each line mean before moving on.\nWe’ll use brm() to fit a model called acc_inter_bm.\n\nThe model formula should specify that we’re predicting Accuracy as a function of Relation_type, Branching, and their interaction.\nSpecify the correct family for the model as well as the data we’ll draw on.\nInclude backend = \"cmdstanr\".\nFinish off by saving the model file using brm()’s file argument.\n\n\nacc_inter_bm &lt;- brm(\n  ...\n)\n\nNow run the model.\nThe summary should look something like this (but don’t worry if your numbers are slightly different from these! That’ll happen due to the randomness inherent in the MCMC sampling process).\n\nsummary(acc_inter_bm)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: Accuracy ~ Relation_type + Branching + Relation_type:Branching \n   Data: shallow (Number of observations: 692) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                                        Estimate Est.Error l-95% CI u-95% CI\nIntercept                                   1.02      0.17     0.69     1.38\nRelation_typeConstituent                    0.85      0.29     0.28     1.43\nBranchingRight                              1.69      0.36     1.04     2.42\nRelation_typeConstituent:BranchingRight    -0.63      0.55    -1.72     0.42\n                                        Rhat Bulk_ESS Tail_ESS\nIntercept                               1.00     3445     3109\nRelation_typeConstituent                1.00     2527     2141\nBranchingRight                          1.00     2131     2325\nRelation_typeConstituent:BranchingRight 1.00     1860     2203\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n1.4 Interpreting the interaction\nFor predictors using treatment contrasts, a positive interaction coefficient represents a positive adjustment to one variable’s effect, when we go from the reference of other variable to its other level (or in other words, when we compare the second level to the reference level). And a negative interaction coefficient represents a negative adjustment.\nAs we observed in the lecture, the mean estimate of \\(-0.63\\) we obtained in the model above suggests that:\n\nThere is on average a smaller effect of Relation_type in Branching == Right when compared to Branching == Left (the reference level);\nOr, equivalently, there is on average a smaller effect of Branching in Relation_type == Constituent when compared to Relation_type == Unrelated (the reference level).\n\nHowever, the 95% CrI of the interaction is \\([-1.72, 0.42]\\) which includes both negative and positive values. In other words, while it seems that a negative adjustment is more probable (the range of negative values in the interval is greater than the positive range), a positive adjustment is also possible. This suggests that based on the model, the direction of the interaction is quite uncertain.\n\n\n1.5 Conditional posterior probability distributions\nGetting the conditional posterior probability distributions when so many predictors are at play is a bit of a process, but we’ll approach it step by step.\nThe first thing to do is to get the posterior draws from our model, using as_draws_df().\n\nacc_draws &lt;- as_draws_df(acc_inter_bm)\nacc_draws\n\n\n\n  \n\n\n\nEach column that begins with b_ contains draws from the posterior probability distribution of each of our model’s \\(\\beta\\) parameters (see Extra: Draws to learn more). Can you identify which column corresponds to which model coefficient?\n\n\n\n\n\n\nExtra: Draws\n\n\n\n\n\nThe values in each column of the output of as_draws_df() are the values draws (Or, more accurately, each column contains a number of draws from the posterior distribution, as obtained by the MCMC sampling procedure.) that summary() summarises when it shows us the model’s estimates.\n\n\n\nTo know how to combine these columns to get the correct conditional posterior probability distributions, we have to reason about how the 0/1 treatment coding affects the presence or absence of the \\(\\beta\\)s in the model equation, shown below:\n\\[\nlogit(p) = \\beta_0 + (\\beta_1 \\times relation) + (\\beta_2 \\times branch) + (\\beta_3 \\times relation \\times branch)\n\\]\nOur goal is to figure out the conditional posterior probability distribution of \\(logit(p)\\) (i.e., the log-odds of answering correctly) in each of our four combinations of levels. To do this, we substitute \\(relation\\) and \\(branch\\) in the equation with 0 or 1, depending on the wanted levels (Remember that 0 is the reference level and 1 is the second level).\n\\[\n\\begin{aligned}\n\\text{Unrelated, Left:}     && \\beta_0 &+ (\\beta_1 \\times 0) + (\\beta_2 \\times 0) + (\\beta_3 \\times 0) &&= \\beta_0  &\\\\\n\\text{Unrelated, Right:}    && \\beta_0 &+ (\\beta_1 \\times 0) + (\\beta_2 \\times 1) + (\\beta_3 \\times 0) &&= \\beta_0 + \\beta_2 &\\\\\n\\text{Constituent, Left:}   && \\beta_0 &+ (\\beta_1 \\times 1) + (\\beta_2 \\times 0) + (\\beta_3 \\times 0) &&= \\beta_0 + \\beta_1 &\\\\\n\\text{Constituent, Right:}  && \\beta_0 &+ (\\beta_1 \\times 1) + (\\beta_2 \\times 1) + (\\beta_3 \\times 1) &&= \\beta_0 + \\beta_1 + \\beta_2 + \\beta_3 &\\\\\n\\end{aligned}\n\\]\nMake sure you understand why each 0 and 1 appears where it does.\nNow we just need to reproduce this in R, using the code below. The output is a data frame where each column contains the conditional posterior draws of \\(logit(p)\\) of each of the four combination of levels.\n\nacc_draws_cond &lt;- acc_draws %&gt;% \n  mutate(\n    Unrelated_Left = b_Intercept,\n    Unrelated_Right = b_Intercept + b_BranchingRight,\n    Constituent_Left = b_Intercept + b_Relation_typeConstituent,\n    Constituent_Right = b_Intercept + b_BranchingRight + b_Relation_typeConstituent + `b_Relation_typeConstituent:BranchingRight`\n  ) %&gt;% \n  select(Unrelated_Left:Constituent_Right)\n\nacc_draws_cond\n\n\n\n  \n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that, in acc_draws, the column name that corresponds to the interaction’s \\(\\beta\\) contains :, which has a special meaning in R. (Try nums &lt;- 1:10 in the console). To avoid problems, we must wrap the column name using tick marks `b_Relation_typeConstituent:BranchingRight`.\n\n\nNow the conditional posterior draws are almost ready to work with! There’s just one more data-manipulation step left before we can compute summary statistics and create pretty plots.\nThat step is to transform this data frame into “long” format using pivot_longer(). The goal is to change the data into a data frame with the following columns:\n\npredictors, with the combinations of levels (Unrelated_Left, Unrelated_Rigth, …);\nsampled_logodds, with the posterior draws values.\n\n\nacc_draws_cond_long &lt;- acc_draws_cond %&gt;% \n  pivot_longer(\n    # The range of columns to pivot:\n    Unrelated_Left:Constituent_Right,\n    # Name the column that will contain the current headers:\n    names_to = \"predictors\",\n    # Name the column that will contain the current values:\n    values_to = \"sampled_logodds\"\n  )\nacc_draws_cond_long\n\n\n\n  \n\n\n\nCan you see how acc_draws_cond_long contains the same data as acc_draws_cond, but in a different layout?\n\n\n\n\n\n\nPivoting\n\n\n\nPivoting is the process of reshaping a data frame so that either the number of columns increases and that of rows decreases (pivot_wider()), or, vice versa, the number of columns decreases and that of rows increases (pivot_longer()).\nThese functions from the tidyverse package tidyr are very powerful and it’s very common to have to use them in data analyses.\nTo learn more about pivoting, check the Pivoting vignette.\n\n\nNow we have just a couple tiny things left:\n\nWe’re going to use separate() to “separate” the predictors column into two columns: one with the value for Relation_type, and another for Branching. By default, column values are split at any punctuation character, like _. Have a look at the documentation for separate() if you’re curious about the details. The reverse of separate() is unite().\nacc_draws_cond_long is a new data frame, so R will apply its defaults for factor ordering unless we again order the factor levels manually.\n\n\nacc_draws_cond_long &lt;- acc_draws_cond_long %&gt;% \n  # Separate `predictors` at \"_\" into two columns\n  separate(predictors, into = c(\"Relation_type\", \"Branching\")) %&gt;% \n  # Set factor levels.\n  mutate(\n    Relation_type = factor(Relation_type, levels = c(\"Unrelated\", \"Constituent\")),\n    Branching     = factor(Branching, levels = c(\"Left\", \"Right\"))\n  )\nacc_draws_cond_long\n\n\n\n  \n\n\n\nNice!\nWe are finally ready to summarise and plot these distributions.\n\n\n\n\n\n\nNote\n\n\n\nWhen computing the conditional posterior probability distributions when the model involves non-linear transformations like log() or logit(), it is very important to do any addition/subtraction of draws before transforming the values back to their original scale.\nIf you were to first back-transform each individual \\(\\beta\\) and then add them together, for example, this would give you incorrect estimates.\nTry the following\n# Incorrect\nplogis(0) + plogis(0)\n\n# Correct\nplogis(0 + 0)\n\n\n\n1.5.1 Compute means and 95% CrIs\nBefore moving on to plotting, we’ll use acc_draws_cond_long to compute summary statistics of the posteriors. We can do this using some now-familiar functions: group_by(), summarise(), mean(), and quantile2().\n\nacc_cond_summ &lt;- acc_draws_cond_long %&gt;% \n  group_by(Relation_type, Branching) %&gt;% \n  summarise(\n    # Calculate the lower and upper bounds of a 95% CrI + mean\n    mean_logodds = mean(sampled_logodds),\n    q95_lo = round(quantile2(sampled_logodds, probs = 0.025), 2),\n    q95_hi = round(quantile2(sampled_logodds, probs = 0.975), 2),\n    # Transform into probabilities\n    p_mean   = round(plogis(mean_logodds), 2),\n    p_q95_lo = round(plogis(q95_lo), 2),\n    p_q95_hi = round(plogis(q95_hi), 2)\n  )\nacc_cond_summ\n\n\n\n  \n\n\n\nA good sense check to make sure that we did this computation correctly is to compare it to the output of summary(). Because summary() also computes summary measures using precisely those same posterior draws, we can compare the numbers in the model summary and in acc_cond_summ.\nSpecifically, in the model summary, the estimates for Intercept should match the summary measures here for Relation_type == Unrelated and Branching == Left (since those are the two baseline levels).\nLet’s have a look:\n\n\nPopulation-Level Effects: \n                                        Estimate Est.Error l-95% CI u-95% CI\nIntercept                                   1.02      0.17     0.69     1.38\nRelation_typeConstituent                    0.85      0.29     0.28     1.43\nBranchingRight                              1.69      0.36     1.04     2.42\nRelation_typeConstituent:BranchingRight    -0.63      0.55    -1.72     0.42\n\n\nYes! The intercept’s Estimate matches the mean when Relation_type == Unrelated and Branching == Left, and the lower and upper bounds of the 95% CrI match as well.\nNow compute other CrI levels: 50%, 60, 75, and 85% (you have to work out which probabilities to use, given that for a 95% CrI we use 0.025 and 0.975).\n\n\n1.5.2 Plot multiple posterior densities\nFinally, we’ll use the conditional posterior draws to create a density plot.\nSee if you understand how the following code creates the plot we see below.\n\nacc_draws_cond_long %&gt;% \n  ggplot(aes(x = sampled_logodds, fill = Relation_type)) +\n  geom_density(alpha = 0.5) +\n  facet_wrap(~ Branching, nrow=2, labeller = labeller(Branching = label_both)) +\n  labs(x = \"Log-odds\",\n       y = \"Probability density\",\n       title = \"Conditional posterior probability of log-odds of 'correct' response\",\n       fill = \"Relation type\")\n\n\n\n\nHow would you create the same plot, but instead of the x-axis showing the data in log-odds, have the x axis showing the data back-transformed to probabilities? Something like this:\n\n\n\n\n\nGive it a shot. Creating this kind of plot is really worth practicing, because these are exactly the kinds of plots you’ll want to show in your reports and dissertations.\nOK, that’s it for our first model! Now we’ll turn to the second part of the tutorial: a more loosely-guided walkthrough of how to put a few different things you’ve learned so far into practice."
  },
  {
    "objectID": "tutorials/tutorial-w08.html#vowel-duration-in-italian-and-polish",
    "href": "tutorials/tutorial-w08.html#vowel-duration-in-italian-and-polish",
    "title": "qml tutorial – Week 8",
    "section": "2 Vowel duration in Italian and Polish",
    "text": "2 Vowel duration in Italian and Polish\nThe data we’ll use in this section comes from the paper An exploratory study of voicing-related differences in vowel duration as compensatory temporal adjustment in Italian and Polish (Coretta 2019, DOI: https://doi.org/10.5334/gjgl.869).\nRead the abstract to see what the study is about.\nThe dataset can be downloaded here: dur-ita-pol.csv.\n\n2.1 Read in the data\nSave dur-ita-pol.csv in your data/ folder, and read it in using read_csv().\n\ndur_ita_pol &lt;- ...\n\nThe data you see should look something like this (selecting only a handful of the many columns to view!).\n\n\n\n\n  \n\n\n\nThe variables of interest for us are:\n\nv1_duration: The duration of the first vowel in the word in milliseconds (ms).\nc2_phonation: Whether the word’s second consonant is voiced or voiceless.\nlanguage: The first language of the speaker (Italian or Polish).\n\nThe model we’ll build below will predict v1_duration as a function of c2_phonation, language, and their interaction.\n\n\n2.2 Wrangle and plot the data\nWhat kinds of transformations and level re-orderings, if any, will we need to do to prepare the data for analysis?\nAfter you have prepared the data, make sure to create a few plots to see what the data looks like.\n\n\n2.3 Some thinking\nBefore fitting the model in the next section, try to answer the following questions based on what you have seen in the plots and based on the contrast coding of the predictors.\n\nWhich level of each predictor corresponds to the reference level?\nBased on the plots above and on the contrast coding of c2_phonation, do you think the model’s estimate for the coefficient of c2_phonation will be positive, negative, or around zero?\nWhat about the estimate for language?\nWhat about the interaction between c2_phonation and language?\n\n\n\n2.4 Fit the model\n\ndur_bm &lt;- brm(\n  ...\n)\n\n\nThe model formula should specify that we’re predicting log vowel duration as a function of c2_phonation, language, and their interaction (do you understand why we need to include the interaction?).\nUse the appropriate model family.\nTell the model which dataset to draw from.\nUse cmdstanr as the backend.\nSave the model to a file.\n\nHere’s what your model summary should look like (with possible small differences due to MCMC sampling).\n\nsummary(dur_bm)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: log_v1_dur ~ c2_phonation * language \n   Data: dur_ita_pol (Number of observations: 1334) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                                     Estimate Est.Error l-95% CI u-95% CI Rhat\nIntercept                                4.78      0.01     4.75     4.80 1.00\nc2_phonationvoiceless                   -0.11      0.02    -0.15    -0.07 1.00\nlanguagePolish                          -0.39      0.02    -0.43    -0.34 1.00\nc2_phonationvoiceless:languagePolish     0.01      0.03    -0.05     0.07 1.00\n                                     Bulk_ESS Tail_ESS\nIntercept                                3203     2925\nc2_phonationvoiceless                    2911     2684\nlanguagePolish                           2707     3082\nc2_phonationvoiceless:languagePolish     2283     2884\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.27      0.01     0.26     0.28 1.00     3972     2723\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHow does this line up with your answers to the questions above?\n\n\n2.5 Interpreting the interaction\nHere, the interaction coefficient 95% CrI is estimated to be [-0.05, 0.07].\nThis CrI is basically centered around zero, with near-equal probability assigned to both negative and positive values. This means that we can’t be certain about the direction of the interaction’s effect, or even whether there is an interaction at all.\nAlso, note how small the values in the 95% CrI are compared to the other coefficient, another sign that perhaps the interaction is very small and practically 0.\nWhat does this mean for the cross-linguistic comparison of the effect of voicing on vowel duration?\n\n\n2.6 Conditional posterior probabilities\nNow apply the workflow we went through above to this new model dur_bm. This will involve:\n\nGetting the posterior draws from the model using as_draws_df().\nWork out which columns correspond to which \\(\\beta\\)s.\nAdd those columns together appropriately to cover all four combinations of the levels of c2_phonation and language.\nTransform the resulting data frame into “long” format using pivot_longer().\nSeparate the column headers into two columns, one of which corresponds to c2_phonation and the other, language.\n\nYour data frame should have three columns: c2_phonation, language, and whatever you’ve chosen to name the column that contains the values of the posterior draws (perhaps something like sampled_logms).\nUsing this data, compute the mean and 95% CrIs of the conditional posterior probability distributions for every combination of c2_phonation and language, and create density plots.\nThese conditional posterior probability distributions are going to be in logs, because we log-transformed the durations to make them modellable as Gaussian. So, like the way we back-transformed the log-odds into probabilities above using plogis(), you’ll also want to back-transform the log(ms) into ms by using the inverse of the log() function, which is exp()."
  },
  {
    "objectID": "tutorials/tutorial-w08.html#summary",
    "href": "tutorials/tutorial-w08.html#summary",
    "title": "qml tutorial – Week 8",
    "section": "3 Summary",
    "text": "3 Summary\n\n\n\n\n\n\nModelling\n\nA model that contains interactions is able to estimate a difference in the effect of one predictor between levels of another.\nFor treatment-coded variables A and B, a positive interaction coefficient represents a positive adjustment (i.e., an addition) to variable A’s effect, when we compare variable B’s reference level to its otther level. And a negative interaction coefficient represents a negative adjustment (i.e., a subtraction).\nNull results (when a coefficient is practically 0) are still interpretable and valid results.\n\nData processing and visualisation\n\nseparate() can split up a single column into multiple ones, as long as every value in that column contains the same separator, e.g., _.\nThe same way that we back-transform log-odds into probabilities using plogis(), we can back-transform log values into the original space (here, ms) using exp().\nAny operations that we use to create conditional posterior probability distributions must be done on the scale in which the model was fit (e.g., log-odds or log), not the scale of the original variable (e.g., probability or ms). Back-transformation into the original space should be the final step.\nTo create density plots of multiple distributions, give ggplot the data in “long” format and let the fill aesthetic separate the data into differently-coloured densities. This also automatically creates a legend for each colour."
  }
]