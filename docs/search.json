[
  {
    "objectID": "posts/plot-advanced.html",
    "href": "posts/plot-advanced.html",
    "title": "Advanced plotting",
    "section": "",
    "text": "Prerequisites\n\n\n\n\nBar charts\nDensity plots"
  },
  {
    "objectID": "posts/plot-advanced.html#overview",
    "href": "posts/plot-advanced.html#overview",
    "title": "Advanced plotting",
    "section": "1 Overview",
    "text": "1 Overview\nIn this tutorial, you can go through the different sections in any order you like and you can pick and choose which sections you want to work on. Note that you should complete Section 4 because it covers stat_summary() which you need for Exercise 1 of the first summative.\nThe tutors will be able to help you choose sections that might be useful based on your interests or your dissertation projects (whether you are working on it now or you will next year!).\nIn this tutorial, there will be less explicit instructions or explanations. Before starting, make sure you attach the necessary packages (like tidyverse).\nYou can find information on the data in the QM data website or by clicking the “Description” button in each section."
  },
  {
    "objectID": "posts/plot-advanced.html#formant-data",
    "href": "posts/plot-advanced.html#formant-data",
    "title": "Advanced plotting",
    "section": "2 Formant data",
    "text": "2 Formant data\nDescription\n\n\nIn this section we will plot formant data from Italian.\n\n# .rda are R data files and they can be read with `load()`\n# `load()` does not need to be assigned to avariable for the data to be added to the enviornment\nload(\"data/coretta2018a/formants.rda\")\n\nThe data contains F1 and F2 measurements at 3 points within each vowel. We want to plot the values at the mid-point of the vowel.\n\nformants |&gt; \n  ggplot(aes(f12, f22, colour = vowel)) +\n  geom_point(alpha = 0.5)\n\n\n\n\n\n\n\n\nThis doesn’t look right though… Shouldn’t /a/ be at the bottom, /i/ on the top-left and /u/ on the top-right?\nLet’s fix this! First we need to use f22 as the x-axis and f12 as the y-axis.\n\nformants |&gt; \n  ggplot(aes(f22, f12, colour = vowel)) +\n  geom_point(alpha = 0.5)\n\n\n\n\n\n\n\n\nBut now the plot is upside-down!\nTo fix this we need to reverse both axes with scale_*_reverse().\n\nformants |&gt; \n  ggplot(aes(f22, f12, colour = vowel)) +\n  geom_point(alpha = 0.5) +\n  scale_x_reverse() + scale_y_reverse()\n\n\n\n\n\n\n\n\nWe can do better! Let’s split the data for each participant and let’s plot ellipses (circles) for each vowel.\nWe can plot speakers separately with facet_wrap(): this works like facet_grid() but instead of specifying variables for rows and columns, you just specify one variable. The data of each value in that variable will be plotted in a separate panel. This is useful when you have many different values for a single variable, like speaker here.\nWe’re also adding in ellipses around each cluster using stat_ellipse(). If you see test that says Warning: Probable convergence failure, that’s because some ellipses might not fit the data super well. Don’t worry about it for now.\n(Note: the plot your code generates will look more squashed than to the one below. If you want to learn how to make your plot taller, like the one below, check out the Extra box below.)\n\nformants |&gt; \n  ggplot(aes(f22, f12, colour = vowel)) +\n  geom_point(alpha = 0.5) +\n  stat_ellipse() +\n  scale_x_reverse() + scale_y_reverse() +\n  facet_wrap(vars(speaker), ncol = 3)\n\n\n\n\n\n\n\n\nIn the plot below, the data is normalised within speakers: note how the numbers along the axis labels are different and the clusters are a bit less dispersed. If you want to learn how to normalise the data within speakers so that you can recreate the following plot, check Section 5 .\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtra: Setting the figure aspect ratio\n\n\n\n\n\nYou can set the aspect ratio of a figure in a Quarto document with the fig-asp Quarto option.\nThe result won’t look different when you just run the code chunk, but when you render the document to HTML, the figure will have the aspect ratio you specify.\n{r}\n#| label: tall-figure\n#| fig-asp: 1.5\n\n(your figure code)"
  },
  {
    "objectID": "posts/plot-advanced.html#reaction-times",
    "href": "posts/plot-advanced.html#reaction-times",
    "title": "Advanced plotting",
    "section": "3 Reaction times",
    "text": "3 Reaction times\nDescription\n\n\nYou have learnt how to use density plots, but an alternative way of plotting continuous data by categorical groups is to use strip charts and violin plots, alone or in combination!\nLet’s plot reaction times from the shallow data.\n\nshallow &lt;- read_csv(\"data/song2020/shallow.csv\")\n\nRows: 6500 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Group, ID, List, Target, Critical_Filler, Word_Nonword, Relation_ty...\ndbl (3): ACC, RT, logRT\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nYou can plot violin plots using geom_violin(). A violin is basically just a density plot, mirrored vertically. The wider parts of the violin indicate a greater density of data around the related y-axis values.\n\nshallow |&gt; \n  # we filter to include only the critical trials\n  filter(Critical_Filler == \"Critical\") |&gt; \n  ggplot(aes(Relation_type, RT)) +\n  geom_violin() +\n  facet_grid(cols = vars(Group))\n\n\n\n\n\n\n\n\nWe can overlay a strip chart, which is basically just the raw data, plotted as dots that are jittered horizontally.\n\nshallow |&gt; \n  # we filter to include only the critical trials\n  filter(Critical_Filler == \"Critical\") |&gt; \n  ggplot(aes(Relation_type, RT)) +\n  geom_violin() +\n  # the width argument in geom_jitter() specifies how wide the jitter should be, as\n  # a value between 0 and 1.\n  # the lower the number, the narrower the jitter.\n  geom_jitter(width = 0.1, alpha = 0.25) +\n  facet_grid(cols = vars(Group))\n\n\n\n\n\n\n\n\nCan you tell if there are differences in RTs between the different groups and relation types?"
  },
  {
    "objectID": "posts/plot-advanced.html#sec-accuracy",
    "href": "posts/plot-advanced.html#sec-accuracy",
    "title": "Advanced plotting",
    "section": "4 Accuracy and computing proportions",
    "text": "4 Accuracy and computing proportions\nDescription\n\n\nLet’s look at data from Koppensteiner et al, 2016. The study looked at the relationship between sound iconicity and body motion.\n\ntakete_maluma &lt;- read_delim(\"data/koppensteiner2016/takete_maluma.txt\")\n\nRows: 460 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (3): Tak_Mal_Stim, Answer, Rater\ndbl (2): Corr_1_Wrong_0, Female_0\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe study specifically analysed the accuracy of the responses (Answer) but in this tutorial we will look instead at the response itself (whether they selected a shaking motion or a flowing motion).\nAlas, this piece of information is not coded in a column in the data, but we can create a new column based on the available info.\n\nWhen the stimulus is Takete and the answer is CORRECT then the participant’s response was shaking.\nWhen the stimulus is Takete and the answer is WRONG then the participant’s response was flowing.\nWhen the stimulus is Maluma and the answer is CORRECT then the participant’s response was shaking.\nWhen the stimulus is Maluma and the answer is WRONG then the participant’s response was flowing.\n\nNow, go ahead and create a new column called Response using the mutate() and the case_when() function.\nWe have not encountered this function yet, so here’s a challenge for you: check out its documentation to learn how it works. You will also need to use the AND operator &: this allows you to put two statements together, like Tak_Mal_Stim == \"Takete\" & Answer == \"CORRECT\" for “if stimulus is Takete AND answer is CORRECT”.\n(If you are following the documentation but case_when() is still giving you mysterious errors, make sure that your version of dplyr is the most current one. To do this, run packageVersion(\"dplyr\") in the console. You want the output to be 1.1.0. If it’s not, you’ll need to update tidyverse by re-installing it.)\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ntakete_maluma &lt;- takete_maluma |&gt;\n  mutate(\n    Response = case_when(\n      Tak_Mal_Stim == \"Takete\" & Answer == \"CORRECT\" ~ \"shaking\",\n      Tak_Mal_Stim == \"Takete\" & Answer == \"WRONG\" ~ \"flowing\",\n      Tak_Mal_Stim == \"Maluma\" & Answer == \"CORRECT\" ~ \"flowing\",\n      Tak_Mal_Stim == \"Maluma\" & Answer == \"WRONG\" ~ \"shaking\"\n    )\n  )\n\n\n\n\nYou should now have something that looks like this:\n\ntakete_maluma\n\n\n  \n\n\n\nA common (but incorrect) way of plotting proportion/percentage data (like accuracy) is to calculate the mean accuracy of each participant and then produce a bar chart with error bars that indicate the mean accuracy and the dispersion around the mean accuracy.\nYou might have seen something like this in many papers.\n\n\n\n\n\n\n\n\n\nTHE HORROR. Alas, this is a very bad way of processing proportion data. Check this blog post if you want to learn why.\nThe alternative (robust) way to plot proportion data is to show the proportion for individual participants (Rater in the takete_maluma data frame). For example, what percentage of the time did a participant give a “shaking” response?\nTo do so we can use a combination of summarise(), geom_jitter() and stat_summary().\nFirst, we need to calculate the proportion of “shaking” responses by participant (Rater) with the simple formula: number of “shaking” responses / total number of responses. (Note that you could have well picked “flowing” instead of “shaking”, and that would give you an equivalent but opposite result: if somebody gave 70% “shaking” responses, they must have given 30% “flowing” responses. The most important thing is to be consistent throughout the entire computation.)\nTo calculate this proportion, we first need to create a new column with a numeric version of the Response column. Since we want to calculate the proportion of “shaking” responses, we use 1 for “shaking and 0 for”flowing”.\nWe can do that using the ifelse() function.\n\ntakete_maluma &lt;- takete_maluma |&gt;\n  mutate(\n    Response_num = ifelse(Response == \"shaking\", 1, 0)\n  )\ntakete_maluma\n\n\n  \n\n\n\nWe can use a special tidyverse function, n(), which returns the number of rows in the data frame, or if the data is grouped, the number of rows in each group. Since we will group the data frame by Rater and Tak_Mal_Stim, n() returns the number of rows for each participant and each stimulus!\n\nshaking_prop &lt;- takete_maluma |&gt;\n  group_by(Rater, Tak_Mal_Stim) |&gt;\n  summarise(\n    shaking_prop = sum(Response_num) / n(),\n    # The following drops the grouping created by group_by() which we don't\n    # need anymore.\n    .groups = \"drop\"\n  )\nshaking_prop\n\n\n  \n\n\n\nWhy does sum(Response_num) / n() give us the proportion of “shaking” responses? We coded every “shaking” response as 1, so if somebody gave 5 “shaking” responses and 10 “flowing” responses, we would calculate \\(5 / (5 + 10) = 5/15 = 33.3\\%\\).\nNow we can plot the proportions and add mean and confidence intervals using geom_jitter() and stat_summary().\nBefore proceeding, you need to install the Hmisc package. There is no need to attach it (it is used by stat_summary() under the hood). Remember not to include the code for installation in your document; you need to install the package only once.\n\nggplot() +\n  # Proportion of each participant\n  geom_jitter(\n    data = shaking_prop,\n    aes(x = Tak_Mal_Stim, y = shaking_prop),\n    width = 0.1, alpha = 0.5\n  ) +\n  # Mean proportion by stimulus with confidence interval\n  stat_summary(\n    data = takete_maluma,\n    aes(x = Tak_Mal_Stim, y = Response_num, colour = Tak_Mal_Stim),\n    fun.data = \"mean_cl_boot\", size = 1\n  ) +\n  labs(\n    title = \"Proportion of takete responses by participant and stimulus\",\n    caption = \"Mean proportion is represented by coloured points with 95% bootstrapped Confidence Intervals.\",\n    x = \"Stimulus\",\n    y = \"Proportion\"\n  ) +\n  # ylim(0, 1) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nDo you see how much more informative and interesting this plot is, compared to the bar-and-whisker plot above?\nYou will notice something new in the code: we have specified the data inside geom_jitter() and stat_summary() instead of inside ggplot(). This is because the two functions need different data: geom_jitter() needs the data with the proportion we calculated for each participant and stimulus; stat_summary() needs to calculate the mean and CIs from the overall data, rather than from the proportion data we created.\nWe are also specifying the aesthetics within each geom/stat function, because while x is the same, the y differs!\nIn stat_summary(), the fun.data argument lets you specify the function you want to use for the summary statistics to be added. Here we are using the mean_cl_boot function, which returns the mean proportion of Response_num and the 95% Confidence Intervals (CIs, there are frequentist confidence intervals) of that mean. The CIs are calculated using a bootstrapping procedure (if you are interested in learning what that is, check the documentation of smean.sd from the Hmisc package)."
  },
  {
    "objectID": "posts/plot-advanced.html#sec-f0",
    "href": "posts/plot-advanced.html#sec-f0",
    "title": "Advanced plotting",
    "section": "5 Fundamental frequency (f0) and normalisation",
    "text": "5 Fundamental frequency (f0) and normalisation\nDescription\n\n\n\n# .rda are R data files and they can be read with `load()`\nload(\"data/coretta2018a/formants.rda\")\n\nLet’s plot f0 for different vowels: We can use a violin plot with a strip chart (see previous section!).\n\nformants |&gt; \n  ggplot(aes(label, f0)) +\n  geom_violin() +\n  geom_jitter(width = 0.1, alpha = 0.1)\n\n\n\n\n\n\n\n\nYou might notice something weird going on… The violins are quite bumpy! This is because different people have different mean f0.\nWe want to normalise the data across speakers, so that individual differences in mean f0 are removed. This process is also called standardisation or z-scoring.\nZ-scores are a standardised unit that allows you to compare things that are on different scales. Our f0 values are on different scales because some speakers have higher mean f0, some have lower mean f0.\nZ-scores are calculated by removing the mean from each value and dividing it by the standard deviation. Since we are normalising within subject, we need to calculate the mean and standard deviation for each participant and apply those to each participant’s data separately. We achieve this by grouping the data by speaker, and then creating a new column that contains each speaker’s z-scores.\n\nformants &lt;- formants |&gt; \n  # group data by speaker so that normalisation is applied for each speaker\n  # separately\n  group_by(speaker) |&gt; \n  # calculate z-scores\n  mutate(\n    f0_z = (f0 - mean(f0)) / sd(f0)\n  )\n\nhead(formants$f0_z)\n\n[1]  3.4126740 -0.9289615  1.1254401 -0.5180812 -1.0111376  0.2488954\n\n\nNow we can recreate the plot above but using f0_z.\n\nformants |&gt; \n  ggplot(aes(label, f0_z)) +\n  geom_violin() +\n  geom_jitter(aes(colour = vowel), width = 0.1, alpha = 0.1)\n\n\n\n\n\n\n\n\nNow the distributions are a lot less lumpy!\nWhy is colour = vowel in geom_jitter(aes(...))? Because we want to colour only the jittered points. If we put colour = vowel in the main ggplot(aes(...)) , then the violin borders would also be coloured.\nIf you want to colour the areas of the violins instead, you can use the fill aesthetic. This will work both in the main ggplot(aes(...)) or in geom_violin(aes(...)) because the points of geom_jitter() can’t be given a colour fill, but only a colour."
  },
  {
    "objectID": "posts/plot-advanced.html#rating-likert-scales",
    "href": "posts/plot-advanced.html#rating-likert-scales",
    "title": "Advanced plotting",
    "section": "6 Rating (Likert) scales",
    "text": "6 Rating (Likert) scales\nDescription\n\n\nLikert scales are a kind of rating scales (see here for more on the relationship between them). Such scales produce very strange variables. They are categorical and ordinal.\nDespite being very common, the practice of treating Likert scales as numeric is quite inappropriate. For an explanation, check the first two pages of Verissimo (2021).\nA very nice type of plot that is very useful for Likert scale data is a so-called divergent stacked bar chart.\nThe HH package has a few handy functions to make creating divergent stacked bar charts very straightforward. You will have to install this package (remember not to include the code for installation in your document; you need to install the package only once).\nWe will create a plot of language attitudes towards Esperanto and Emilian (Gallo-Italian).\nParticipants were asked the following: “when you hear somebody speaking Emilian/Esperanto, you would think they are…” and they were presented with 9 adjectives, which they had to rate from strongly disagree (meaning they strongly disagreed that somebody speaking Emilian/Esperanto had that quality) to strongly agree (meaning they strongly agreed that the person had that quality). Esperanto speakers rated Esperanto while Emilian speakers rated Emilian.\nLet’s read the data (it’s an .rds file!).\n\nemilianto_attitude &lt;- readRDS(\"data/hampton2023/emilianto_attitude.rds\")\n\nInstall the HH package before proceeding (but you don’t need to attach it).\nTo be able to plot with the likert() function from the HH package, we first need to wrangle the data.\nBasically, we need a new tibble with counts of each value of the scale for each of the adjectives that participants had to rate. In other words, how many times did people say “strong disagree”, “disagree”, and so on, for each adjective.\nMake sure you understand each line of the following code.\n\nemilianto_lik &lt;- emilianto_attitude |&gt;\n  \n  # let's select only the columns we need\n  select(language, educated:familiar) |&gt;\n  \n  # let's pivot the data so that we get a longer tibble, with the columns\n  # language, adjective and score\n  pivot_longer(educated:familiar, names_to = \"adjective\", values_to = \"score\") |&gt;\n  \n  # and we count the numbers of each adjective and score combo\n  count(language, adjective, score) |&gt;\n  \n  # then we pivot again so that now the data is wider, with the columns,\n  # language, adjective, and one column for each of the five scores\n  pivot_wider(names_from = \"score\", values_from = n) %&gt;%\n  \n  # we rename the scores to the actual label that the participants saw\n  rename(\"strong disagree\" = `1`, \"disagree\" = `2`, \"neither\" = `3`, \"agree\" = `4`, \"strong agree\" = `5`)\nemilianto_lik\n\n\n  \n\n\n\nNow we can finally plot this using the likert() function. The function takes a formula that describes how to plot the data: here, we want a divergent stacked bar for each adjective and we want all the scores (adjective ~ .), but we also want to facet by language (| language).\n\nHH::likert(\n  # formula\n  adjective ~ . | language,\n  # data\n  emilianto_lik,\n  as.percent = TRUE,\n  # plot title\n  main = \"Language attitudes towards Emilian and Esperanto\"\n)\n\n\n\n\n\n\n\n\nWhat is that HH::likert()? That’s the R way of calling a function from a package directly, without the need to attach the full package with library(HH). This is useful, for example, for packages that override functions from other packages when attached or for packages that attach a lot of package dependencies.\nIn most cases you won’t have to worry, but if you ever think that function overwriting is causing issues, you know you can use the package::function() syntax.\nWhat can you tell about the attitude towards Emilian and Esperanto from the plot?"
  },
  {
    "objectID": "posts/regression-interactions.html",
    "href": "posts/regression-interactions.html",
    "title": "Regression models: interactions",
    "section": "",
    "text": "Prerequisites\n\n\n\n\nRegression: Indexing of categorical predictors."
  },
  {
    "objectID": "posts/regression-interactions.html#introduction",
    "href": "posts/regression-interactions.html#introduction",
    "title": "Regression models: interactions",
    "section": "1 Introduction",
    "text": "1 Introduction\nRegression models can include both numeric and categorical predictors. Numeric predictors can be included in regression models directly, while categorical predictors have to be coded numerically (regression models can only work with numeric stuff). Coding of categorical predictors is done automatically by R when you include categorical predictors. There are two main type of coding approaches: contrasts (the default) and indexing.\nIndexing of categorical predictors is helpful when including multiple predictors in a regression model. So far, we only had one predictor per model. In this tutorial you will learn how to include and interpret regression models with multiple predictors.\nWe will also explore the concept of predictor interactions. In brief, an interaction between two predictors allows the model to adjust the effect of one predictor depending on the value/level of another, and vice versa. To illustrate interactions, we will use the Massive Auditory Lexical Decision data, MALD."
  },
  {
    "objectID": "posts/regression-interactions.html#the-data-massive-auditory-lexical-decision-mald",
    "href": "posts/regression-interactions.html#the-data-massive-auditory-lexical-decision-mald",
    "title": "Regression models: interactions",
    "section": "2 The data: Massive Auditory Lexical Decision (MALD)",
    "text": "2 The data: Massive Auditory Lexical Decision (MALD)\nLet’s read the data. They are in an .rds file, so we need readRDS().\n\nmald &lt;- readRDS(\"data/tucker2019/mald_1_1.rds\")\n\nWe will focus on reaction times (RT) and the phonetic Levinstein distance (PhonLev). Specifically, we will investigate the effect of the phonetic distance on RTs depending on the lexical status of the the word heard by participants (i.e. where the word was a real word or a nonce word, IsWord).\n\nmald |&gt; \n  ggplot(aes(PhonLev, RT)) +\n  geom_point(alpha = 0.1) +\n  geom_smooth(aes(colour = IsWord, fill = IsWord), method = \"lm\", formula = \"y ~ x\")\n\n\n\n\n\n\n\n\nThe scatter plot above includes two regression lines: one for real words (IsWord = TRUE) and one for nonce words (IsWord = FALSE). Generally, increasing phonetic distance corresponds to increasing RTs (i.e. slower responses). This positive relationship also differs somewhat depending on the word lexical status. Try and find more patterns in the plot and make a mental record. You will be able to compare your intuitions with the regression results later."
  },
  {
    "objectID": "posts/regression-interactions.html#regression-with-two-predictors-isword-and-phonelev",
    "href": "posts/regression-interactions.html#regression-with-two-predictors-isword-and-phonelev",
    "title": "Regression models: interactions",
    "section": "3 Regression with two predictors: IsWord and PhoneLev",
    "text": "3 Regression with two predictors: IsWord and PhoneLev\nWe start by fitting a regression model with logged RTs as the outcome variable and a Gaussian distribution as the probability distribution of RTs. We include IsWord and PhonLev. IsWord will be coded with indexing (rather than the default contrasts).\n\\[\n\\begin{align}\nlog(RT)_i & \\sim Gaussian(\\mu_i, \\sigma)\\\\\n\\mu_i & = \\alpha_{\\text{W}[i]} + \\beta \\cdot \\text{PL}_i\n\\end{align}\n\\]\n\n\\(\\alpha_{\\text{W}[i]}\\) is the mean RT value depending on IsWord.\n\\(\\beta\\) is the change in RT for each unit increase of phonetic distance (PhonLev).\n\nThe mathematical formula corresponds to the R formula RT_log ~ 0 + IsWord + PhonLev. Let’s fit the regression model with this formula now.\n\nmald_bm_1 &lt;- brm(\n  RT_log ~ 0 + IsWord + PhonLev,\n  family = gaussian,\n  data = mald,\n  seed = 9284,\n  cores = 4,\n  file = \"data/cache/regression-interactions_mald_bm_1\"\n)\n\nLet’s check the summary.\n\nsummary(mald_bm_1, prob = 0.8)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT_log ~ 0 + IsWord + PhonLev \n   Data: mald (Number of observations: 5000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-80% CI u-80% CI Rhat Bulk_ESS Tail_ESS\nIsWordTRUE      6.58      0.02     6.55     6.61 1.01     1104     1477\nIsWordFALSE     6.69      0.02     6.66     6.72 1.00     1095     1401\nPhonLev         0.03      0.00     0.03     0.04 1.00     1115     1441\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-80% CI u-80% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.27      0.00     0.27     0.27 1.00     1670     1318\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nIsWordTRUE is the estimated logged RT when IsWord is TRUE: at 80% probability, it is between 6.55 and 6.61 logged milliseconds, i.e. 699 and 742 ms (exp(6.55) and exp(6.61)).\nIsWordFALSE is the estimated logged RT when IsWord is FALSE: at 80% probability, it is between 6.66 and 6.72 logged milliseconds, i.e. 781 and 829 ms (exp(6.66) and exp(6.72)).\nPhonLev is the estimated change in logged RTs for each unit increase of PhonLev: at 80% probability, when PhonLev increases by 1, logged RTs increase by 0.03 to 0.04 logged ms.\n\nMake sure you understand how to interpret the coefficients! A lot of learners of statistics get easily confused by this and when model start to become more complex, things will be even more difficult to disentangle."
  },
  {
    "objectID": "posts/regression-interactions.html#centring-numeric-predictors",
    "href": "posts/regression-interactions.html#centring-numeric-predictors",
    "title": "Regression models: interactions",
    "section": "4 Centring numeric predictors",
    "text": "4 Centring numeric predictors\nIn mald_bm_1, the coefficients for IsWord are the estimated logged RT when PhonLev is 0 and the coefficient for PhonLev is approximately the average change in logged RTs across the two IsWord conditions! This is a very important aspect of regression models with multiple predictors.\nLet’s focus on the when PhonLev is 0 part. In some cases, 0 for numeric predictors is meaningful (for example, if you are counting hand gestures of infants, a count of 0 makes sense, it just means that the infant did not produce any gesture), but in other cases 0 does not make sense: a typical example is duration or length, like segment duration. A segment, like a vowel, can’t be 0 ms long: that would mean there is no vowel!\nIn those cases where 0 is not meaningful for a numeric predictor, a typical approach is to centre that predictor.\nCentring is a transformation by which you subtract the mean value from all of the observations of the numeric predictor. Check the code below if it’s not immediately clear how it works. The mean RT value in the data is about 7. Mathematically, the model is:\n\\[\n\\begin{align}\nlog(RT)_i & \\sim Gaussian(\\mu_i, \\sigma)\\\\\n\\mu_i & = \\alpha_{\\text{W}[i]} + \\beta \\cdot (\\text{PL}_i - \\bar{\\text{PL}})\n\\end{align}\n\\]\n\nmald &lt;- mald |&gt; \n  mutate(\n    # centre variable\n    PhonLev_c = PhonLev - mean(PhonLev)\n  )\n\nLet’s plot both PhonLev and PhonLev_c to see what happened.\nmald |&gt; \n  ggplot(aes(PhonLev)) +\n  geom_density() +\n  geom_rug(alpha = 0.1)\nmald |&gt; \n  ggplot(aes(PhonLev_c)) +\n  geom_density() +\n  geom_rug(alpha = 0.1)\n\n\n\n\n\n\n\n\n\n\nYou will see that the shape of the density distribution is unchanged, but the values on the x-axis are now different. If you compare the two plots, you will notice that in the centred version 0 lies at about the same spot that corresponds to 7 in the non-centred version. This makes sense, since centring transforms the data so that the mean (here, 7) becomes 0. The other values can be thought of as differences from the mean.\nIf you include the centred PhonLev_c in the model, now the estimated logged RT for IsWord will be when PhonLev is at its mean. Why? Because they will be the estimates when PhonLev_c is 0, which corresponds to the mean PhonLev because of the centring transformation!\nLet’s fit the model and check the summary.\n\nmald_bm_2 &lt;- brm(\n  RT_log ~ 0 + IsWord + PhonLev_c,\n  family = gaussian,\n  data = mald,\n  seed = 9284,\n  cores = 4,\n  file = \"data/cache/regression-interactions_mald_bm_2\"\n)\n\nsummary(mald_bm_2, prob = 0.8)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT_log ~ 0 + IsWord + PhonLev_c \n   Data: mald (Number of observations: 5000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-80% CI u-80% CI Rhat Bulk_ESS Tail_ESS\nIsWordTRUE      6.82      0.01     6.82     6.83 1.00     4106     3124\nIsWordFALSE     6.93      0.01     6.93     6.94 1.00     4152     2987\nPhonLev_c       0.03      0.00     0.03     0.04 1.00     5878     3035\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-80% CI u-80% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.27      0.00     0.27     0.27 1.00     3361     2808\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe estimates of the coefficients IsWordTRUE and IsWordFALSE have changed: they are now somewhat higher than the estimates in mald_bm_1. This is because increasing phonetic distance increases RTs too: this means that estimated RTs when PhonLev is 0 will be lower than estimated RTs when PhonLev is at the mean of 7 (i.e. when PhonLev_c is at 0).\nYou will also notice, however, that the estimated effect of PhonLev_c is the same as the estimated effect of PhonLev. This is because centring is a linear transformation: all values are changed in the same way and this does not affect the change in RTs for each unit increase of phonetic distance.\nLet’s plot the predictions from this model with conditional_effects().\n\nconditional_effects(mald_bm_2, \"PhonLev_c:IsWord\")\n\n\n\n\n\n\n\n\nWe can tell that on average the RT for nonce words (IsWord = FALSE) are longer than those for real words (IsWord = TRUE). Moreover, increasing phonetic distance corresponds to increasing RT values: the more unique a word is, the longer it takes to make a lexical decision.\nBut you will also notice something: the estimated regression lines in the two IsWord conditions have exactly the same slope, meaning that the effect of phonetic distance is the same in both conditions.\nThis doesn’t seem to be right when comparing this with the plot we made above, using the raw data. While comparing raw and estimated data is not always straightforward, here we are missing an important tool in our model that creates the discrepancy: predictor interactions (or simply, interactions)."
  },
  {
    "objectID": "posts/regression-interactions.html#interactions-categorical-numeric",
    "href": "posts/regression-interactions.html#interactions-categorical-numeric",
    "title": "Regression models: interactions",
    "section": "5 Interactions: categorical * numeric",
    "text": "5 Interactions: categorical * numeric\nThe model mald_bm_2 estimates the same effect of PhonLev_c in both IsWord conditions because we didn’t include an interaction term: interaction terms allow the model to adjust the effects of each of the predictors in the interaction based on the value of the other predictors in the interaction.\nLet’s look at the mathematical specification of a model with an interaction.\n\\[\n\\begin{align}\nlog(RT)_i & \\sim Gaussian(\\mu_i, \\sigma)\\\\\n\\mu_i & = \\alpha_{\\text{W}[i]} + \\beta_{\\text{W}[i]} \\cdot (\\text{PL}_i - \\bar{\\text{PL}})\n\\end{align}\n\\]\nThe new part is the interaction term: \\(\\beta_{\\text{W}[i]} \\cdot (\\text{PL}_i - \\bar{\\text{PL}})\\): now \\(\\beta\\) is estimated for IsWord = TRUE and IsWord = FALSE.\nInteraction terms in R are specified by using a colon: :: IsWord:PhonLev_c. See the following code.\n\nmald_bm_3 &lt;- brm(\n  RT_log ~ 0 + IsWord + IsWord:PhonLev_c,\n  family = gaussian,\n  data = mald,\n  seed = 9284,\n  cores = 4,\n  file = \"data/cache/regression-interactions_mald_bm_3\"\n)\n\n\n0 + suppresses the intercept (we need this to use to indexing approach instead of the default contrasts approach).\nIsWord corresponds to \\(\\alpha_{\\text{W}[i]}\\) above.\nIsWord:PhonLev_c corresponds to \\(\\beta_{\\text{W}[i]} \\cdot (\\text{PL}_i - \\bar{\\text{PL}})\\).\n\n\nsummary(mald_bm_3, prob = 0.8)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT_log ~ 0 + IsWord + IsWord:PhonLev_c \n   Data: mald (Number of observations: 5000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                      Estimate Est.Error l-80% CI u-80% CI Rhat Bulk_ESS\nIsWordTRUE                6.82      0.01     6.82     6.83 1.00     4650\nIsWordFALSE               6.93      0.01     6.93     6.94 1.00     4842\nIsWordTRUE:PhonLev_c      0.04      0.00     0.04     0.05 1.00     5748\nIsWordFALSE:PhonLev_c     0.03      0.00     0.02     0.03 1.00     5098\n                      Tail_ESS\nIsWordTRUE                3032\nIsWordFALSE               3008\nIsWordTRUE:PhonLev_c      3011\nIsWordFALSE:PhonLev_c     2993\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-80% CI u-80% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.27      0.00     0.27     0.27 1.00     4462     3271\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nIsWordTRUE: the estimated logged RT when IsWord is TRUE and PhonLev_c is 0.\nIsWordFALSE: the estimated logged RT when IsWord is FALSE and PhonLev_c is 0.\nIsWordTRUE:PhonLev_c: the estimated change in RT for each unit increase of PhonLev_c when IsWord is TRUE.\nIsWordFALSE:PhonLev_c: the estimated change in RT for each unit increase of PhonLev_c when IsWord is FALSE.\n\nLet’s transform the estimates to milliseconds (for IsWord) and percentage change (for the interaction with PhoneLev_c).\n\nround(exp(c(6.82, 6.83)))\n\n[1] 916 925\n\nround(exp(c(6.93, 6.94)))\n\n[1] 1022 1033\n\nround(exp(c(0.04, 0.05)), 2)\n\n[1] 1.04 1.05\n\nround(exp(c(0.02, 0.03)), 2)\n\n[1] 1.02 1.03\n\n\nYou can report the results of this model like so:\n\nWe fitted a Bayesian regression with logged reaction times (RTs) as the outcome variable and a Gaussian distribution as the distribution of the outcome. As predictors, we included lexical status (IsWord, categorical, coded as TRUE for real words and FALSE for nonce words) and an interaction between lexical status and phonetic distance (PhonLev_c, numeric centred). The categorical predictor of lexical status was coded using indexing by suppressing the model intercept with the 0 + syntax. See @tab-bm-3 for the coefficient estimates.\nBased on the model, at mean phonetic distance, RTs are between 916 and 925 ms when the word is a real word and between 1022 and 1033 ms when the word is a nonce word, at 80% confidence. For each unit increase of phonetic distance, RTs increase by 4-5% with real words and by 2-3% with nonce words, at 80% probability.\n\nAnd now, the visual proof that including an interaction allows the model to adjust the effect of phonetic distance.\n\nconditional_effects(mald_bm_3, \"PhonLev_c:IsWord\")\n\n\n\n\n\n\n\n\nNote that including an interaction term is important whenever you want to know if indeed the effect of a predictor depends on the value of other predictors. Furthermore, including an interaction term when the effect does not differ between values of other predictors is not an issue for model fitting: you will just get similar estimated effects for all the values of the other predictors.\n\n\n\n\n\n\nNote\n\n\n\nThe very common but dangerous practice of dropping interaction terms when the estimated difference is negligible should be avoided.\nThink about it: you don’t know if predictors “interact” so you include the interaction term. You discover that probably the predictors don’t really interact so you remove the interaction term and refit the model. You report the results of the model without the interaction term.\nThis model will be biased because it is never the case that estimates are exactly the same across values of other predictors. In fact, they are very much always somewhat different and inclusion of predictors in a model should be driven by conceptual theory and not by the results.\nThis practice (of dropping terms based on results) is even more dangerous when adopting a frequentist approach with p-values because a non-significant p-value does not mean there is no interaction (it could just be a matter of low statistical power)."
  },
  {
    "objectID": "posts/regression-interactions.html#wrangling-draws-of-numeric-predictors",
    "href": "posts/regression-interactions.html#wrangling-draws-of-numeric-predictors",
    "title": "Regression models: interactions",
    "section": "6 Wrangling draws of numeric predictors",
    "text": "6 Wrangling draws of numeric predictors\nLet’s inspect the MCMC draws.\n\nmald_3_draws &lt;- as_draws_df(mald_bm_3)\n\nmald_3_draws\n\n\n  \n\n\n\nNothing out of the ordinary: the drawn values for our regression coefficients are in the columns that start with b_ (and there is also sigma of course).\nMost times it is helpful to get estimates not only when you increase your numeric predictor by 1, but also when you increase it further. For example, in our data we see that at lower phonetic distance values, the estimated RTs are quite different depending on IsWord. But with increasing phonetic distance, the difference in RTs between the two IsWord conditions becomes smaller and smaller.\nSo now we can ask the question: what is the estimated difference in RTs between IsWord = TRUE and IsWord = FALSE when phonetic distance is 12?\nSince PhoneLev_c is centred, if we want to get phonetic distance 12 we need to find the corresponding PhonLev_c values. Since the mean of PhonLev is about 7, then 12 corresponds to 5 in PhonLev_c (7 + 5 = 12).\nTo get the estimated RT value in real and nonce words when the phonetic distance is 12, we just add the coefficient of PhonLev_c multiplied by 5. This is because the estimate is for each unit increase of phonetic distance, but we want to get the estimated RT when phonetic distance increases by 5 from the mean (i.e. when it is 12). Look at the code below.\nSince the interaction terms have : in their names and : is a special symbol in R, you need to “protect” the name by surrounding it between back ticks `.\n\nmald_3_preds_5 &lt;- mald_3_draws |&gt; \n  mutate(\n    word_5 = b_IsWordTRUE + (5 * `b_IsWordTRUE:PhonLev_c`),\n    non_word_5 = b_IsWordFALSE + (5 * `b_IsWordFALSE:PhonLev_c`),\n  )\n\nmald_3_preds_5 |&gt; \n  select(word_5:non_word_5)\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\n\n  \n\n\n\nNow that we have the draws of logged RT values for real and nonce words when phonetic distance is 12, we can take the difference between the two and calculate the 80% CrI of that difference. We want the difference in milliseconds, rather than logged milliseconds, so we exponentiate the logged values before taking the difference.\n\nlibrary(posterior)\n\nThis is posterior version 1.6.0\n\n\n\nAttaching package: 'posterior'\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\nmald_3_preds_5 |&gt; \n  mutate(\n    word_nonword_5 = exp(non_word_5) - exp(word_5)\n  ) |&gt; \n  summarise(\n    lo_80 = quantile2(word_nonword_5, 0.1),\n    hi_80 = quantile2(word_nonword_5, 0.9),\n  )\n\n\n  \n\n\n\nYou could add the report above:\n\nAt 80% confidence, when the phonetic distance is 12, RTs when the word is a nonce word are -17 ms shorter to 74 ms longer than when the word is a real word.\n\nWe can also plot the posterior draws of the difference as we would with any posterior draws.\n\nlibrary(ggdist)\n\n\nAttaching package: 'ggdist'\n\n\nThe following objects are masked from 'package:brms':\n\n    dstudent_t, pstudent_t, qstudent_t, rstudent_t\n\nmald_3_preds_5 |&gt; \n  mutate(\n    word_nonword_5 = exp(non_word_5) - exp(word_5)\n  ) |&gt; \n  ggplot(aes(word_nonword_5)) +\n  stat_halfeye()"
  },
  {
    "objectID": "posts/summaries.html",
    "href": "posts/summaries.html",
    "title": "Statistical summaries",
    "section": "",
    "text": "Pre-requisites\n\n\n\n\nR basics"
  },
  {
    "objectID": "posts/summaries.html#summary-measures-overview",
    "href": "posts/summaries.html#summary-measures-overview",
    "title": "Statistical summaries",
    "section": "1 Summary measures: overview",
    "text": "1 Summary measures: overview\n\n\n\n\n\nWe can summarise variables using summary measures. There are two types of summary measures.\n\nMeasures of central tendency indicate the typical or central value of a sample.\nMeasures of dispersion indicate the spread or dispersion of the sample values around the central tendency value.\n\nAlways report a measure of central tendency together with its measure of dispersion!\n\n\n\n\n\n\nMeasures of central tendency\n\n\n\nMean\n\\[\\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n} = \\frac{x_1 + ... + x_n}{n}\\]\nMedian\n\\[\\text{if } n \\text{ is odd, } x_\\frac{n+1}{2}\\]\n\\[\\text{if } n \\text{ is even,  } \\frac{x_\\frac{n}{2} + x_\\frac{n}{2}}{2}\\]\nMode\nThe most common value.\n\n\n\n\n\n\n\n\nMeasures of dispersion\n\n\n\nMinimum and maximum values\nRange\n\\[ max(x) - min(x)\\]\nThe difference between the largest and smallest value.\nStandard deviation\n\\[\\text{SD} = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n-1}} = \\sqrt{\\frac{(x_1 - \\bar{x})^2 + ... + (x_n - \\bar{x})^2}{n-1}}\\]"
  },
  {
    "objectID": "posts/summaries.html#mean",
    "href": "posts/summaries.html#mean",
    "title": "Statistical summaries",
    "section": "2 Mean",
    "text": "2 Mean\nUse the mean with numeric continuous variables, if:\n\nThe variable can take on any positive and negative number, including 0.\n\n\nmean(c(-1.12, 0.95, 0.41, -2.1, 0.09))\n\n[1] -0.354\n\n\n\nThe variable can take on any positive number only.\n\n\nmean(c(0.32, 2.58, 1.5, 0.12, 1.09))\n\n[1] 1.122\n\n\n\n\n\n\n\n\nWarning\n\n\n\nDon’t take the mean of proportions and percentages!\nBetter to calculate the proportion/percentage across the entire data, rather than take the mean of individual proportions/percentages: see this blog post. If you really really have to, use the median."
  },
  {
    "objectID": "posts/summaries.html#median",
    "href": "posts/summaries.html#median",
    "title": "Statistical summaries",
    "section": "3 Median",
    "text": "3 Median\nUse the median with numeric (continuous and discrete) variables.\n\n# even N\nmedian(c(-1.12, 0.95, 0.41, -2.1, 0.09))\n\n[1] 0.09\n\n# odd N\nodd &lt;- c(4, 6, 3, 9, 7, 15)\nmedian(odd)\n\n[1] 6.5\n\n# the median is the mean of the two \"central\" number\nsort(odd)\n\n[1]  3  4  6  7  9 15\n\nmean(c(6, 7))\n\n[1] 6.5\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\nThe mean is very sensitive to outliers.\nThe median is not.\n\nThe following list of numbers does not have obvious outliers. The mean and median are not to different.\n\n# no outliers\nmedian(c(4, 6, 3, 9, 7, 15))\n\n[1] 6.5\n\nmean(c(4, 6, 3, 9, 7, 15))\n\n[1] 7.333333\n\n\nIn the following case, there is quite a clear outlier, 40. Look how the mean is higher than the median. This is because the outlier 40 pulls the mean towards it.\n\n# one outlier\nmedian(c(4, 6, 3, 9, 7, 40))\n\n[1] 6.5\n\nmean(c(4, 6, 3, 9, 7, 40))\n\n[1] 11.5"
  },
  {
    "objectID": "posts/summaries.html#mode",
    "href": "posts/summaries.html#mode",
    "title": "Statistical summaries",
    "section": "4 Mode",
    "text": "4 Mode\nUse the mode with categorical (discrete) variables. Unfortunately the mode() function in R is not the statistical mode, but rather it returns the R object type.\nYou can use the table() function to “table” out the number of occurrences of elements in a vector.\n\ntable(c(\"red\", \"red\", \"blue\", \"yellow\", \"blue\", \"green\", \"red\", \"yellow\"))\n\n\n  blue  green    red yellow \n     2      1      3      2 \n\n\nThe mode is the most frequent value: here it is red, with 3 occurrences.\n\n\n\n\n\n\nWarning\n\n\n\nLikert scales are ordinal (categorical) variables, so the mean and median are not appropriate!\nYou should use the mode (You can use the median with Likert scales if you really really need to…)"
  },
  {
    "objectID": "posts/summaries.html#minimum-and-maximum",
    "href": "posts/summaries.html#minimum-and-maximum",
    "title": "Statistical summaries",
    "section": "5 Minimum and maximum",
    "text": "5 Minimum and maximum\nYou can report minimum and maximum values for any numeric variable.\n\nx_1 &lt;- c(-1.12, 0.95, 0.41, -2.1, 0.09)\n\nmin(x_1)\n\n[1] -2.1\n\nmax(x_1)\n\n[1] 0.95\n\nrange(x_1)\n\n[1] -2.10  0.95\n\n\nNote that the range() function does not return the statistical range (see next section), but simply prints both the minimum and the maximum."
  },
  {
    "objectID": "posts/summaries.html#range",
    "href": "posts/summaries.html#range",
    "title": "Statistical summaries",
    "section": "6 Range",
    "text": "6 Range\nUse the range with any numeric variable.\n\nx_1 &lt;- c(-1.12, 0.95, 0.41, -2.1, 0.09)\nmax(x_1) - min(x_1)\n\n[1] 3.05\n\nx_2 &lt;- c(0.32, 2.58, 1.5, 0.12, 1.09)\nmax(x_2) - min(x_2)\n\n[1] 2.46\n\nx_3 &lt;- c(4, 6, 3, 9, 7, 15)\nmax(x_3) - min(x_3)\n\n[1] 12"
  },
  {
    "objectID": "posts/summaries.html#standard-deviation",
    "href": "posts/summaries.html#standard-deviation",
    "title": "Statistical summaries",
    "section": "7 Standard deviation",
    "text": "7 Standard deviation\nUse the standard deviation with numeric continuous variables, if:\n\nThe variable can take on any positive and negative number, including 0.\n\n\nsd(c(-1.12, 0.95, 0.41, -2.1, 0.09))\n\n[1] 1.23658\n\n\n\nThe variable can take on any positive number only.\n\n\nsd(c(0.32, 2.58, 1.5, 0.12, 1.09))\n\n[1] 0.9895555\n\n\n\n\n\n\n\n\nWarning\n\n\n\nStandard deviations are relative and depend on the measurement unit/scale!\nDon’t use the standard deviation with proportions and percentages!"
  },
  {
    "objectID": "posts/summaries.html#summarise-data-in-r",
    "href": "posts/summaries.html#summarise-data-in-r",
    "title": "Statistical summaries",
    "section": "8 Summarise data in R",
    "text": "8 Summarise data in R\nWhen you work with data, you always want to get summary measures for most of the variables in the data.\nData reports usually include summary measures. It is also important to understand which summary measure is appropriate for which type of variable.\nWe have covered this in the lecture, so we won’t go over it again here. Instead, you will learn how to obtain summary measures using the summarise() function from the dplyr tidyverse package.\nsummarise() takes at least two arguments:\n\nThe data frame to summarise.\nOne or more summary functions.\n\nFor example, let’s get the mean the reaction time column RT. Easy! (First attach the tidyverse and read the song2020/shallow.csv file into a variable called shallow.)\n\nsummarise(shallow, RT_mean = mean(RT))\n\n\n  \n\n\n\nGreat! The mean reaction times of the entire sample is 867.3592 ms.\nYou can round numbers with the round() function. For example:\n\nnum &lt;- 867.3592\nround(num)\n\n[1] 867\n\nround(num, 1)\n\n[1] 867.4\n\nround(num, 2)\n\n[1] 867.36\n\n\nThe second argument sets the number of decimals to round to (by default, it is 0, so the number is rounded to the nearest integer, that is, to the nearest whole number with no decimal values).\nLet’s recalculate the mean by rounding it this time.\n\nsummarise(shallow, RT_mean = round(mean(RT)))\n\n\n  \n\n\n\nWhat if we want also the standard deviation? Easy: we use the sd() function. (Round the mean and SD with the round() function in your code).\n\n# round the mean and SD\nsummarise(shallow, RT_mean = mean(RT), RT_sd = sd(RT))\n\n\n  \n\n\n\nNow we know that reaction times are on average 867 ms long and have a standard deviation of about 293 ms (rounded to the nearest integer).\nLet’s go all the way and also get the minimum and maximum RT values with the min() and max() functions (round all the summary measures).\n\nsummarise(\n  shallow,\n  RT_mean = mean(RT), RT_sd = sd(RT),\n  RT_min = ..., RT_max = ...\n)\n\nFab! When writing a data report, you could write something like this.\n\nReaction times are on average 867 ms long (SD = 293 ms), with values ranging from 0 to 1994 ms.\n\nWe won’t go into the details of what standard deviations are, but you can just think of them as a relative measure of how dispersed the data are around the mean: the higher the SD, the greater the dispersion around the mean, i.e. the greater the variability in the data.\nWhen required, you can use the median() function to calculate the median, instead of the mean(). Go ahead and calculate the median reaction times in the data. Is it similar to the mean?\n\n8.1 NAs\nMost base R functions behave unexpectedly if the vector they are used on contain NA values.\nNA is a special object in R, that indicates that a value is Not Available, meaning that that observation does not have a value.\nFor example, in the following numeric vector, there are 5 objects:\n\na &lt;- c(3, 5, 3, NA, 4)\n\nFour are numbers and one is NA.\nIf you calculate the mean of a with mean() something strange happens.\n\nmean(a)\n\n[1] NA\n\n\nThe functions returns NA.\nThis is because by default when just one value in the vector is NA then operations on the vector will return NA.\n\nmean(a)\n\n[1] NA\n\nsum(a)\n\n[1] NA\n\nsd(a)\n\n[1] NA\n\n\nIf you want to discard the NA values when operating on a vector that contains them, you have to set the na.rm (for “NA remove”) argument to TRUE.\n\nmean(a, na.rm = TRUE)\n\n[1] 3.75\n\nsum(a, na.rm = TRUE)\n\n[1] 15\n\nsd(a, na.rm = TRUE)\n\n[1] 0.9574271\n\n\n\n\n\n\n\n\nQuiz 1\n\n\n\n\nWhat does the na.rm argument of mean() do?\n\n It changes NAs to FALSE. It converts NAs to 0s. It removes NAs before taking the mean.\n\nWhich is the mean of c(4, 23, NA, 5) when na.rm has the default value?\n\n NA. 0. 10.66.\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nCheck the documentation of ?mean.\n\n\n\n\n\n\n\n\n\n\n\nSummary table of summary measures"
  },
  {
    "objectID": "posts/regression-draws.html",
    "href": "posts/regression-draws.html",
    "title": "Regression models: working with MCMC draws",
    "section": "",
    "text": "Prerequisites\n\n\n\n\nIntroduction to regression models (Part II)."
  },
  {
    "objectID": "posts/regression-draws.html#mcmc-what",
    "href": "posts/regression-draws.html#mcmc-what",
    "title": "Regression models: working with MCMC draws",
    "section": "1 MCMC what?",
    "text": "1 MCMC what?\nXXX"
  },
  {
    "objectID": "posts/regression-draws.html#extract-mcmc-posterior-draws",
    "href": "posts/regression-draws.html#extract-mcmc-posterior-draws",
    "title": "Regression models: working with MCMC draws",
    "section": "2 Extract MCMC posterior draws",
    "text": "2 Extract MCMC posterior draws\nLet’s read the winter2012/polite.csv data again first.\n\nlibrary(tidyverse)\n\npolite &lt;- read_csv(\"data/winter2012/polite.csv\")\n\nRows: 224 Columns: 27\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (6): subject, gender, birthplace, musicstudent, task, attitude\ndbl (21): months_ger, scenario, total_duration, articulation_rate, f0mn, f0s...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npolite\n\n\n  \n\n\n\nNow we can reload the Bayesian regression model from Introduction to regression models (Part II). We simply use the same code: but now, instead of the model being fit again, the code will just read the file data/cache/hnr_bm.rds.\n\nlibrary(brms)\n\nhnr_bm &lt;- brm(\n  HNRmn ~ attitude,\n  family = gaussian(),\n  data = polite,\n  cores = 4,\n  file = \"data/cache/hnr_bm\",\n  seed = 2913\n)\n\nIt’s always good to inspect the summary to make sure the model is the correct one.\n\nsummary(hnr_bm, prob = 0.8)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: HNRmn ~ attitude \n   Data: polite (Number of observations: 224) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n               Estimate Est.Error l-80% CI u-80% CI Rhat Bulk_ESS Tail_ESS\nIntercept         16.27      0.16    16.07    16.47 1.00     4136     3237\nattitudepolite     1.25      0.23     0.96     1.55 1.00     4396     2937\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-80% CI u-80% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.69      0.08     1.59     1.80 1.00     4065     2979\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe Regression Coefficients table includes Intercept and attitudepoliteite which is what we expect from the model formula and data. We are good to go!\nNow we can extract the MCMC draws from the model using the as_draws_df() function.\nThis function returns a tibble with values obtained at each draw of the MCMC draws. Since we fitted the model with 4 chains and 1000 sampling draws per chain, there is a total of 4000 drawn values (i.e. 4000 rows).\n\nhnr_bm_draws &lt;- as_draws_df(hnr_bm)\nhnr_bm_draws\n\n\n  \n\n\n\nDon’t worry about the lprior and lp__ columns. The columns .chain, .iteration and .draw indicate:\n\nThe chain number (1 to 4).\nThe iteration number within chain (1 to 1000).\nThe draw number across all chains (1 to 4000).\n\nThe following columns contain the drawn values at each draw for three parameters of the model: b_Intercept, b_attitude and sigma. To remind yourself what these mean, let’s have a look at the mathematical formula of the model.\n\\[\n\\begin{align}\n\\text{HNRmn} & \\sim Gaussian(\\mu, \\sigma) \\\\\n\\mu        & = \\beta_0 + \\beta_1 \\cdot \\text{is\\_polite} \\\\\n\\end{align}\n\\]\nSo:\n\nb_Intercept is \\(\\beta_0\\). This is the mean HNR when the attitude is informal.\nb_attitudepolite is \\(\\beta_1\\). This is the difference in HNR between polite and informal attitude.\nsigma is \\(\\sigma\\). This is the overall standard deviation."
  },
  {
    "objectID": "posts/regression-draws.html#summaries-and-plotting-of-posterior-draws",
    "href": "posts/regression-draws.html#summaries-and-plotting-of-posterior-draws",
    "title": "Regression models: working with MCMC draws",
    "section": "3 Summaries and plotting of posterior draws",
    "text": "3 Summaries and plotting of posterior draws\nThe Regression Coefficients table reports summaries of the distribution of the drawn values of b_Intercept and b_attitudepolite. These summary measures are the mean (Estimate), the standard deviation (Est.error) and the lower and upper limits of the credible interval.\nWe can of course obtain those same measures ourselves from the draws tibble!\n\nlibrary(posterior)\n\nThis is posterior version 1.6.0\n\n\n\nAttaching package: 'posterior'\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\nhnr_bm_draws |&gt; \n  select(-sigma) |&gt; \n  pivot_longer(b_Intercept:b_attitudepolite) |&gt; \n  group_by(name) |&gt; \n  summarise(\n    mean = mean(value), sd = sd(value),\n    lo_80 = quantile2(value, 0.1), up_80 = quantile2(value, 0.9)\n  ) |&gt; \n  # fancy way of mutating multiple columns\n  mutate(\n    across(mean:up_80, ~round(., 2))\n  )\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\n\n  \n\n\n\nHopefully now it is clear where the values in the Regression Coefficient table come from and how these are related to the MCMC draws.\nNext we can plot the (posterior) probability distribution of the draws for any parameter. Let’s plot b_attitudepolite. This will be the posterior probability density of the difference in HNR between the polite and informal attitude.\n\nhnr_bm_draws |&gt;\n  ggplot(aes(b_attitudepolite)) +\n  geom_density() +\n  geom_rug(alpha = 0.2)"
  },
  {
    "objectID": "posts/regression-draws.html#calculate-predictions-based-on-the-draws",
    "href": "posts/regression-draws.html#calculate-predictions-based-on-the-draws",
    "title": "Regression models: working with MCMC draws",
    "section": "4 Calculate predictions based on the draws",
    "text": "4 Calculate predictions based on the draws\nThe question we ended up with in Introduction to regression models (Part II) was: what about the posterior probability of the predicted HNR when the attitude is polite?.\nSo far, we have seen the predicted HNR when attitude is informal and the mean difference between polite and informal.\nTo calculate the predicted HNR when attitude is polite, we should look back at the equation of \\(\\mu\\) from the model mathematical formulae.\n\\[\n\\begin{align}\n\\mu        & = \\beta_0 + \\beta_1 \\cdot \\text{is\\_polite} \\\\\n\\end{align}\n\\]\nWhen is_polite is 0, attitude is informal and we have \\(\\mu = \\beta_0\\). When is_polite is 1, attitude is polite and we have \\(\\mu = \\beta_0 + \\beta_1\\).\nSo, to get the predicted HNR when attitude is polite, we just need to sum b_Intercept (\\(\\beta_0\\)) and b_attitudepolite (\\(\\beta_1\\)).\n\nhnr_bm_draws_pred &lt;- hnr_bm_draws |&gt; \n  mutate(\n    # predicted HNR for informal attitude\n    informal = b_Intercept,\n    # predicted HNR for polite attitude\n    polite = b_Intercept + b_attitudepolite\n  )\n\nhnr_bm_draws_pred\n\n\n  \n\n\n\nThe sum operation is applied row-wise, to each draw. So, for polite, the value of b_Intercept at draw 1 is added to the value of b_attitudepolite at draw 1, and so on. You end up with a list of sums that has the same length as the initial draws (here, 4000, i.e. 1000 per chain!). Then you can summarise and plot this new list as you did with the b_ coefficients.\nTo make that easier, let’s pivot longer.\n\nhnr_bm_draws_pred_long &lt;- hnr_bm_draws_pred |&gt; \n  select(.chain, .iteration, .draw, informal, polite) |&gt; \n  pivot_longer(informal:polite, names_to = \"attitude\", values_to = \"pred\")\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\nhnr_bm_draws_pred_long"
  },
  {
    "objectID": "posts/regression-draws.html#plot-and-summarise-posterior-predictions",
    "href": "posts/regression-draws.html#plot-and-summarise-posterior-predictions",
    "title": "Regression models: working with MCMC draws",
    "section": "5 Plot and summarise posterior predictions",
    "text": "5 Plot and summarise posterior predictions\nNow let’s make a violin plot of the predicted HNR in informal and polite attitude!\n\nhnr_bm_draws_pred_long |&gt; \n  ggplot(aes(attitude, pred)) +\n  geom_violin(width = 0.2) +\n  labs(\n    x = \"Attitude\", y = \"Predicted HNR\"\n  )\n\n\n\n\n\n\n\n\nWe can use the ggdist package to plot something a bit fancier, like a half eye. Check the ?stat_halfeye documentation to learn what those lines are.\n\nlibrary(ggdist)\n\n\nAttaching package: 'ggdist'\n\n\nThe following objects are masked from 'package:brms':\n\n    dstudent_t, pstudent_t, qstudent_t, rstudent_t\n\nhnr_bm_draws_pred_long |&gt; \n  ggplot(aes(pred, attitude)) +\n  stat_halfeye() +\n  labs(x = \"Predicted HNR\", y = \"Attitude\")\n\n\n\n\n\n\n\n\nAnother useful stat is stat_interval().\n\nhnr_bm_draws_pred_long |&gt; \n  ggplot(aes(attitude, pred)) +\n  stat_interval() +\n  labs(\n    x = \"Attitude\", y = \"Predicted HNR\"\n  )\n\n\n\n\n\n\n\n\nAnd finally, let’s summarise the predictions.\n\nhnr_bm_draws_pred_long |&gt; \n  group_by(attitude) |&gt; \n  summarise(\n    mean = mean(pred), sd = sd(pred),\n    lo_80 = quantile2(pred, 0.1), up_80 = quantile2(pred, 0.9)\n  ) |&gt; \n  # fancy way of mutating multiple columns\n  mutate(\n    across(mean:up_80, ~round(., 2))\n  )\n\n\n  \n\n\n\nYou could report the summary like so:\n\nAccording to the model, the predicted HNR when attitude is informal is between 16 and 16.48 at 80% confidence (\\(\\beta\\) = 16.27, SD = 0.16). When attitude is polite, the predicted HNR is between 17.31 and 17.73 (\\(\\beta\\) = 17.53, SD = 0.16)."
  },
  {
    "objectID": "posts/regression-interactions-catcat.html",
    "href": "posts/regression-interactions-catcat.html",
    "title": "Regression models: interactions between categorical predictors",
    "section": "",
    "text": "Prerequisites\n\n\n\n\nRegression models: interactions."
  },
  {
    "objectID": "posts/regression-interactions-catcat.html#introduction",
    "href": "posts/regression-interactions-catcat.html#introduction",
    "title": "Regression models: interactions between categorical predictors",
    "section": "1 Introduction",
    "text": "1 Introduction\nHere you will learn how to include interactions between categorical predictors and how to interpret the output. Relative to categorical-numeric interactions, categorical-categorical interactions are easier to interpret: you basically just get a coefficient for each combination of each level in the two categorical predictors and all of these coefficients can be though of as intercepts."
  },
  {
    "objectID": "posts/regression-interactions-catcat.html#the-data-song-2020",
    "href": "posts/regression-interactions-catcat.html#the-data-song-2020",
    "title": "Regression models: interactions between categorical predictors",
    "section": "2 The data: Song 2020",
    "text": "2 The data: Song 2020\nLet’s read the data.\n\nlibrary(tidyverse)\n\nshallow &lt;- read_csv(\"data/song2020/shallow.csv\") |&gt; \n  filter(Critical_Filler == \"Critical\", RT &gt; 0)\n\nRows: 6500 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Group, ID, List, Target, Critical_Filler, Word_Nonword, Relation_ty...\ndbl (3): ACC, RT, logRT\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe will be looking at RTs and relation type in L1 and L2 participants. Let’s plot the RTs.\n\nshallow |&gt; \n  ggplot(aes(Relation_type, RT)) +\n  geom_jitter(width = 0.2, alpha = 0.3) +\n  stat_summary(fun.data = \"mean_cl_boot\", aes(colour = Relation_type)) +\n  facet_grid(cols = vars(Group)) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nThe means marked by coloured points in the plot suggest there might be a smaller difference in RTs between constituent and non-constituent/unrelated in the L2 than in the L1 group.\nAn interesting question we can ask is how much smaller is the difference in the L1 group compared to the L2 group. We can fit a regression model and wrangle the posterior draws to answer that question."
  },
  {
    "objectID": "posts/regression-interactions-catcat.html#the-model",
    "href": "posts/regression-interactions-catcat.html#the-model",
    "title": "Regression models: interactions between categorical predictors",
    "section": "3 The model",
    "text": "3 The model\nWe want to allow the model to estimate mean RTs for each combination of relation type (Relation_type) and group (Group). Both predictors are categorical. We can achieve this by including an interaction term between the two categorical predictors: Relation_type:Group.\nWhen both predictors are categorical, you only need the interaction term when suppressing the intercept with 0 +. The model will estimate a coefficient for each combination of levels in the two predictors.\n\nshallow_bm &lt;- brm(\n  logRT ~ 0 + Relation_type:Group,\n  family = gaussian,\n  data = shallow,\n  file = \"data/cache/regression-interactions-catcat_shallow_bm\",\n  cores = 4,\n  seed = 6237\n)\n\nCheck out the summary of the model. You will find 6 regression coefficients: 3 relation types (Constituent, Non-constituent, Unrelated) * 2 groups (L1 and L2).\n\nsummary(shallow_bm, prob = 0.8)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: logRT ~ 0 + Relation_type:Group \n   Data: shallow (Number of observations: 1920) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                                    Estimate Est.Error l-80% CI u-80% CI Rhat\nRelation_typeConstituent:GroupL1        6.60      0.02     6.58     6.63 1.00\nRelation_typeNonConstituent:GroupL1     6.73      0.02     6.70     6.76 1.00\nRelation_typeUnrelated:GroupL1          6.71      0.02     6.69     6.73 1.00\nRelation_typeConstituent:GroupL2        6.81      0.02     6.79     6.83 1.00\nRelation_typeNonConstituent:GroupL2     6.87      0.02     6.84     6.90 1.00\nRelation_typeUnrelated:GroupL2          6.88      0.01     6.86     6.90 1.00\n                                    Bulk_ESS Tail_ESS\nRelation_typeConstituent:GroupL1        5737     3141\nRelation_typeNonConstituent:GroupL1     5664     3306\nRelation_typeUnrelated:GroupL1          5700     2801\nRelation_typeConstituent:GroupL2        5314     2445\nRelation_typeNonConstituent:GroupL2     6533     2703\nRelation_typeUnrelated:GroupL2          5573     3056\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-80% CI u-80% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.31      0.01     0.30     0.31 1.00     5850     3101\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWe can easily plot this with conditional_effects(), like so.\n\nconditional_effects(shallow_bm, \"Group:Relation_type\")\n\n\n\n\n\n\n\n\nInspection of the estimated RTs for each relation type in the two groups suggests the difference between constituent and non-constituent/unrelated is larger in L1 than in L2 participants. Let’s calculate the difference in the two groups and then the difference between these differences."
  },
  {
    "objectID": "posts/regression-interactions-catcat.html#get-the-difference-of-differences",
    "href": "posts/regression-interactions-catcat.html#get-the-difference-of-differences",
    "title": "Regression models: interactions between categorical predictors",
    "section": "4 Get the difference of differences",
    "text": "4 Get the difference of differences\nAs usual, we need to obtain the posterior draws.\n\nshallow_bm_draws &lt;- as_draws_df(shallow_bm)\n\nNow, to get the difference between constituent and non-constituent/unrelated:\n\nWe must first get the mean of the non-constituent and unrelated relation types.\nThen we can take the difference between the exponentiated posterior draws of the constituent type and the exponentiated posterior draws of the mean non-constituent/unrelated type. The exponentiation transforms the draws from logged ms to ms and then we take the difference.\nFinally, we take the difference between the calculated differences in ms.\n\nRun the code and inspect the shallow_bm_diff tibble to understand what is going on.\n\nshallow_bm_diff &lt;- shallow_bm_draws |&gt; \n  mutate(\n    # 1. Mean non-constituent/unrelated for L1 and L2\n    L1_NCU = mean(c(`b_Relation_typeNonConstituent:GroupL1`, `b_Relation_typeUnrelated:GroupL1`)),\n    L2_NCU = mean(c(`b_Relation_typeNonConstituent:GroupL2`, `b_Relation_typeUnrelated:GroupL2`)),\n    \n    # 2. Difference between non-constituent/unrelated and constituent\n    L1_diff = exp(L1_NCU) - exp(`b_Relation_typeConstituent:GroupL1`),\n    L2_diff = exp(L2_NCU) - exp(`b_Relation_typeConstituent:GroupL2`),\n    \n    # 3. Difference of difference\n    L2_L1_diff = L2_diff - L1_diff\n  )\nshallow_bm_diff |&gt; \n  select(L1_NCU:L2_L1_diff)\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\n\n  \n\n\n\nThe column L2_L1_diff contains the posterior draws of the difference between the constituent and non-constituent/unrelated type in the two groups. We can proceed as usual, like with any other posterior, and calculate the CrI of this difference of differences.\n\nlibrary(posterior)\n\nThis is posterior version 1.6.0\n\n\n\nAttaching package: 'posterior'\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\nshallow_bm_diff |&gt; \n  summarise(\n    lo_80 = quantile2(L2_L1_diff, prob = 0.1),\n    hi_80 = quantile2(L2_L1_diff, prob = 0.9),\n    lo_80 = round(lo_80), hi_80 = round(hi_80),\n  )\n\n\n  \n\n\n\nHere, at 80% confidence, the difference between constituent and non-constituent/unrelated types in the L2 group is 6 to 53 ms smaller than the difference in the L1 group."
  },
  {
    "objectID": "posts/bar-charts.html",
    "href": "posts/bar-charts.html",
    "title": "Bar charts",
    "section": "",
    "text": "Prerequisites\n\n\n\n\nPlotting basics in R"
  },
  {
    "objectID": "posts/bar-charts.html#bar-charts",
    "href": "posts/bar-charts.html#bar-charts",
    "title": "Bar charts",
    "section": "1 Bar charts",
    "text": "1 Bar charts\n\n\n\n\n\n\nBar charts\n\n\n\nBar charts are useful when you are counting things. For example:\n\nNumber of verbs vs nouns vs adjectives in a corpus.\nNumber of languages by geographic area.\nNumber of correct vs incorrect responses.\n\nThe bar chart geometry is geom_bar().\n\n\nWe will first create a plot with counts of the number of languages in global_south by their endangerment status and then a plot where we also split the counts by macro-area.\n\n1.1 Number of languages of the Global South by status\nTo create a bar chart, you can use the geom_bar() geometry.\n\n\n\n\n\n\nBar chart axes\n\n\n\nIn a simple bar chart, you only need to specify one axis, the x-axis, in the aesthetics aes().\nThis is because the counts that are placed on the y-axis are calculated by the geom_bar() function under the hood.\nThis quirk is something that confuses many new learners, so make sure you internalise this.\n\n\nGo ahead and complete the following code to create a bar chart.\n\nglobal_south |&gt;\n  ggplot(aes(x = status)) +\n  ...\n\nNote how we’re using |&gt; to pipe the glot_status data frame into the ggplot() function. This works because ggplot()’s first argument is the data, and piping is a different way of providing the first argument to a function.\nAs mentioned above, the counting for the y-axis is done automatically. R looks in the status column and counts how many times each value in the column occurs in the data frame.\nIf you did things correctly, you should get the following plot.\n\n\n\n\n\n\n\n\nFigure 1: Number of languages by endangerment status.\n\n\n\n\n\nThe x-axis is now status and the y-axis corresponds to the number of languages by status (count). As mentioned above, count is calculated under the hood for you (you will learn how to count levels with count() later in the course).\nYou could write a description of the plot that goes like this:\n\nThe number of languages in the Global South by endangered status is shown as a bar chart in Figure 1. Among the languages that are endangered, the majority are threatened or shifting.\n\nWhat if we want to show the number of languages by endangerment status within each of the macro-areas that make up the Global South? Easy! You can make a stacked bar chart. This requires we mutate the data first."
  },
  {
    "objectID": "posts/bar-charts.html#stacked-bar-charts",
    "href": "posts/bar-charts.html#stacked-bar-charts",
    "title": "Bar charts",
    "section": "2 Stacked bar charts",
    "text": "2 Stacked bar charts\nA special type of bar charts are the so-called stacked bar charts.\n\n\n\n\n\n\nStacked bar chart\n\n\n\nA stacked bar chart is a bar chart in which each contains a “stack” of shorter bars, each indicating the counts of some sub-groups.\nThis type of plot is useful to show how counts of something vary depending on some other grouping (in other words, when you want to count the occurrences of a categorical variable based on another categorical variable). For example:\n\nNumber of languages by endangerment status, grouped by geographic area.\nNumber of infants by head-turning preference, grouped by first language.\nNumber of past vs non-past verbs, grouped by verb class.\n\n\n\nTo create a stacked bar chart, you just need to add a new aesthetic mapping to aes(): fill. The fill aesthetic lets you fill bars or areas with different colours depending on the values of a specified column.\nLet’s make a plot on language endangerment by macro-area.\nComplete the following code by specifying that fill should be based on status.\n\nglobal_south |&gt;\n  ggplot(aes(x = Macroarea, ...)) +\n  geom_bar()\n\nYou should get the following.\n\n\n\n\n\n\n\n\nFigure 2: Number of languages by macro-area and endangerment status.\n\n\n\n\n\nA write-up example:\n\nFigure 2 shows the number of languages by geographic macro-area, subdivided by endangerment status. Africa, Eurasia and Papunesia have substantially more languages than the other areas.\n\n\n\n\n\n\n\nQuiz 4\n\n\n\nWhat is wrong in the following code?\ngestures |&gt;\n  ggplot(aes(x = status), fill = Macroarea) +\n  geom_bar()"
  },
  {
    "objectID": "posts/bar-charts.html#filled-stacked-bar-charts",
    "href": "posts/bar-charts.html#filled-stacked-bar-charts",
    "title": "Bar charts",
    "section": "3 Filled stacked bar charts",
    "text": "3 Filled stacked bar charts\nIn the plot above it is difficult to assess whether different macro-areas have different proportions of endangerment. This is because the overall number of languages per area differs between areas.\nA solution to this is to plot proportions instead of raw counts.\nYou could calculate the proportions yourself, but there is a quicker way: using the position argument in geom_bar().\nYou can plot proportions instead of counts by setting position = \"fill\" inside geom_bar(), like so:\n\nglobal_south |&gt;\nggplot(aes(x = Macroarea, fill = status)) +\n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\nFigure 3: Proportion of languages by macro-area and endangerment status.\n\n\n\n\n\nThe plot now shows proportions of languages by endangerment status for each area separately.\nNote that the y-axis label is still “count” but should be “proportion”. Use labs() to change the axes labels and the legend name.\n\nglobal_south |&gt;\nggplot(aes(x = Macroarea, fill = status)) +\n  geom_bar(position = \"fill\") +\n  labs(\n    ...\n  )\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nIf to change the name of the colour legend, you use the colour argument in labs(), guess which argument you should use for fill?\n\n\n\nYou should get this.\n\n\n\n\n\n\n\n\nFigure 4: Proportion of languages by macro-area and endangerment status.\n\n\n\n\n\nWith this plot it is easier to see that different areas have different proportions of endangerment. In writing:\n\nFigure 4 shows proportions of languages by endangerment status for each macro-area. Australia, South and North America have a substantially higher proportion of extinct languages than the other areas. These areas also have a higher proportion of near extinct languages. On the other hand, Africa has the greatest proportion of non-endangered languages followed by Papunesia and Eurasia, while North and South America are among the areas with the lower proportion, together with Australia which has the lowest.\n\n\n\n\n\n\n\nNext\n\n\n\n\nFaceting plots\nAdvanced plotting"
  },
  {
    "objectID": "posts/data-types.html",
    "href": "posts/data-types.html",
    "title": "Data types",
    "section": "",
    "text": "Pre-requisites\n\n\n\n\nIntroduction to Quarto"
  },
  {
    "objectID": "posts/data-types.html#tabular-data",
    "href": "posts/data-types.html#tabular-data",
    "title": "Data types",
    "section": "1 Tabular data",
    "text": "1 Tabular data\n\n\n\n\n\n\nTabular data\n\n\n\nTabular data is data that has a form of a table: i.e. values structured in columns and rows.\n\n\nMost of the data we will be using in this course will be tabular and the files will be in the .csv format.\nThe comma separated values format (.csv) is the best format to save data in because it is basically a plain text file, it’s quick to parse, and can be opened and edited with any software (plus, it’s not a proprietary format like .docx or .xlsx—these formats are specific to particular software).\nThis is what a .csv file looks like when you open it in a text editor (showing only the first few lines).\nGroup,ID,List,Target,ACC,RT,logRT,Critical_Filler,Word_Nonword,Relation_type,Branching\nL1,L1_01,A,banoshment,1,423,6.0474,Filler,Nonword,Phonological,NA\nL1,L1_01,A,unawareness,1,603,6.4019,Critical,Word,Unrelated,Left\nL1,L1_01,A,unholiness,1,739,6.6053,Critical,Word,Constituent,Left\nL1,L1_01,A,bictimize,1,510,6.2344,Filler,Nonword,Phonological,NA\nThe file contains tabular data (data that is structured as columns and rows, like a spreadsheet).\nTo separate the values of each column, a .csv file uses a comma , (hence the name “comma separated values”) to separate the values in every row.\nThe first line of the file indicates the names of the columns of the table:\nGroup,ID,List,Target,ACC,RT,logRT,Critical_Filler,Word_Nonword,Relation_type,Branching\nThere are 11 columns. The rest of the rows is the data, i.e. the values of each column separated by commas.\nL1,L1_01,A,banoshment,1,423,6.0474,Filler,Nonword,Phonological,NA\nL1,L1_01,A,unawareness,1,603,6.4019,Critical,Word,Unrelated,Left\nL1,L1_01,A,unholiness,1,739,6.6053,Critical,Word,Constituent,Left\nL1,L1_01,A,bictimize,1,510,6.2344,Filler,Nonword,Phonological,NA\nThis might look a bit confusing, but you will see later that, after importing this type of file, you can view it as a nice spreadsheet (as you would in Excel).\nAnother common type of tabular data file is spreadsheets, like spreadsheets created by Microsoft Excel or Apple Numbers. These are all proprietary formats that require you to have the software that were created with if you want to modify them.\nPortability and openness are important aspects of conducting ethical research, so that using open and non-proprietary file types makes your research more accessible and doesn’t privilege those who have access to specific software (remember, R is free!).\nThere are also variations of the comma separated values type, like tab separated values files (.tsv, which uses tab characters instead of commas) and fixed-width files (usually .txt, where columns are separated by as many white spaces as needed so that the columns align).\n\n1.1 Non-tabular data\nOf course, R can import also data that is not tabular, like map data and complex hierarchical data.\nWe will dip our toes into map data at the end of course, but virtually all of the data we will use will be tabular, just because that’s the format you need to do data visualisation and analyses.\n\n\n1.2 .rds files\nR has a special way of saving data: .rds files.\n.rds files allow you to save an R object to a file on your computer, so that you can read that file in when you need it.\nA common use for .rds files is to save tabular data that you have processed so that it can be readily used in many different scripts or even by other people.\nIn the following sections you will learn how to import (aka read) three types of data: .csv, Excel and .rds files."
  },
  {
    "objectID": "posts/data-types.html#download-the-data-files",
    "href": "posts/data-types.html#download-the-data-files",
    "title": "Data types",
    "section": "2 Download the data files",
    "text": "2 Download the data files\nThroughout the course we will be using data files that come from linguistic research. You should download now the data files from the QML Data website according to the following instructions.\nPlease, follow these instructions carefully.\n\nDownload the zip archive with all the data by right-clicking on the following link and download the file: data.zip.\nUnzip the zip file to extract the contents. (If you don’t know how to do this, search for it online for your operating system!)\nCreate a folder called data/ (the slash is there just to remind you that it’s a folder, but you don’t have to include it in the name) in the Quarto project you are using for the course.\n\nTo create a folder, go to the Files tab of the bottom-right panel in RStudio.\nMake sure you are viewing the project’s main folder.\nClick on the New Folder button, enter “data” in the text box and click OK\n\nMove the contents of the data.zip archive into the data/ folder.\n\nOpen a Finder or File Explorer window.\nNavigate to the folder where you have extracted the zip file (it will very likely be the Downloads/ folder).\nCopy the contents of the zip file.\nIn Finder or File Explorer, navigate to the Quarto project folder, then the data/ folder, and paste the contents in there. (You can also drag and drop if you prefer.)\n\n\nThe rest of the tutorial will assume that you have created a folder called data/ in the Quarto project folder and that the files you downloaded are in that folder. The data folder should like something like this:\ndata/\n└── cameron2020/\n    └── gestures.csv\n└── coretta2018/\n    └── formants.csv\n    └── token-measures.csv\n└── ...\nI recommend that you start being very organised with your files in other projects from now on, whether it’s for this course or your dissertation or else. I also suggest to avoid overly nested structures (for example, avoid having one folder for each week for this course. Rather, save all data files in the data/ folder).\n\n\n\n\n\n\nOrganising your files\n\n\n\n\n\nThe Open Science Framework has the following recommendations that apply very well to any type of research project.\n\nUse one folder per project. This will also be your RStudio project folder.\nSeparate raw data from derived data.\nSeparate code from data.\nMake raw data read-only.\n\nTo learn more about this, check the OSF page Organising files.\nIn brief, what these recommendations mean is that you want a folder for your research project/course/else, and inside the folder two folders: one for data and one for code.\nThe data/ folder could further contain raw/ for raw data (data that should not be lost or changed, for example collected data or annotations) and derived/ for data that derives from the raw data, for example through automated data processing.\nI usually also have a separate folder called figs/ or img/ where I save plots. Of course which folders you will have it’s ultimately up to you and needs will vary depending on the project and field!"
  },
  {
    "objectID": "posts/data-types.html#import-.csv-files",
    "href": "posts/data-types.html#import-.csv-files",
    "title": "Data types",
    "section": "3 Import .csv files",
    "text": "3 Import .csv files\nLet’s start with data from this paper: Song et al. 2020. Second language users exhibit shallow morphological processing. DOI: 10.1017/S0272263120000170.\nThe study consisted of a lexical decision task in which participants were first shown a prime, followed by a target word for which they had to indicate whether it was a real word or a nonce word.\nThe prime word belonged to one of three possible groups (Relation_type in the data) each of which refers to the morphological relation of the prime and the target word:\n\nUnrelated: for example, prolong (assuming unkindness as target, [[un-kind]-ness]).\nConstituent: unkind.\nNonConstituent: kindness.\n\n\n3.1 The tidyverse packages\nImporting .csv files is very easy. You can use the read_csv() function from a collection of R packages known as the tidyverse.\nTo import data in R we will use the read_csv() function from the readr package, one of the tidyverse packages.\nInstalling the tidyverse packages is easy: you just need to install the tidyverse package and that will take care of installing the most important packages in the collection (called the “core” tidyverse packages).\nGo ahead and install the tidyverse from the Packages tab.\n\n\n3.2 read_csv()\n\n\n\n\n\n\nDid you open the RStudio Quarto project?\n\n\n\nBefore moving on, make sure that you have opened the RStudio Quarto project correctly (see warning at the top of the tutorial)\n\n\nThe read_csv() function from the readr package only requires you to specify the file path as a string (remember, strings are quoted between \" \", for example \"year_data.txt\"). On my computer, the file path of song2020/shallow.csv is /Users/ste/dal/data/song2020/shallow.csv, but on your computer the file path will be different, of course.\nAlso, note that it is not enough to use the read_csv() function. You also must assign the output of the read_csv() function (i.e. the data we are reading) to a variable, using the assignment arrow &lt;-, just like we were assigning values to variables in the previous weeks.\nAnd since the read_csv() is a function from the tidyverse, you first need to attach the tidyverse packages with library(tidyverse) (remember, you need to attach packages only once per session). This will attach the core tidyverse packages, including readr. Of course, you can also attach the individual packages directly: library(readr). If you use library(tidyverse) there is no need to attach individual tidyverse packages.\nBefore reading the data, create a new R script named tutorial_w03.R and save it in the code/ folder of your Quarto project.\nGenerally, you start the script with calls to library() to load all the packages you need for the script.\nNow we only need one package, tidyverse, but in most cases you will need more than one! The best practice is to attach all of packages first, in the top of your script. Please, get in the habit of doing this from now, so that you can keep your scripts tidy and pretty!\n\n\n\n\n\n\nWarning\n\n\n\nPlease, don’t include install.packages() in your R scripts!\nRemember, you only have to install a package once, and you can just type it in the Console.\nBut DO include library() in your scripts.\n\n\nAt the top of tutorial_w03.R, write the following lines of code. Then run the code.\n\nlibrary(tidyverse)\n\nshallow &lt;- read_csv(\"./data/song2020/shallow.csv\")\n\nRows: 6500 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Group, ID, List, Target, Critical_Filler, Word_Nonword, Relation_ty...\ndbl (3): ACC, RT, logRT\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nIf you look at the Environment tab, you will see song2020 under Data.\n\n\n\n\n\n\nData frames and tibbles\n\n\n\nIn R, a data table is called a data frame.\nTibbles are special data frame created with the read functions from the tidyverse. If you are curious about the difference, check this page.\nIn this course’s tutorials, “data frame” and “tibble” will be used interchangeably (since we are using the read functions from the tidyverse, all resulting data frames will be tibbles).\n\n\nBut wait, what is that \"./data/song2020/shallow.csv\"? That’s a relative path. We briefly mentioned relative paths above, but let’s understand the details now. You will be able to view the data soon.\n\n\n3.3 Relative paths\n\n\n\n\n\n\nRelative path\n\n\n\nA relative path is a file path that is relative to a folder. The folder the path starts at is represented by ./.\n\n\nWhen you are using R scripts in Quarto projects, the ./ folder paths are relative to is the project folder! This is true whichever the name of the folder/project and whichever it’s location on your computer.\nFor example, if your project it’s called awesome_proj and it’s in Downloads/stuff/, then if you write ./data/results.csv you really mean Downloads/stuff/awesome_proj/data/results.csv!\nHow does R know the path is relative to the project folder?\nThat is because when working with Quarto projects, all relative paths are relative to the project folder (i.e. the folder with the .Rproj file)!\nThe folder which relative paths are relative to is called the working directory (directory is just another way of saying folder).\n\n\n\n\n\n\nWorking directory\n\n\n\nThe working directory is the folder which relative paths are relative to.\nWhen using Quarto projects, the working directory is the project folder.\n\n\nThe code read_csv(\"./data/song2020/shallow.csv\") above will work because you are using a Quarto project and inside the project folder there is a folder called data/ and in it there’s the song2020/shallow.csv file.\nSo from now on I encourage you to use Quarto projects, R scripts and relative paths always!\nThe benefit of doing so is that, if you move your project or rename it, or if you share the project with somebody, all the paths will just work because they are relative!\n\n\n\n\n\n\nGet the working directory\n\n\n\nYou can get the current working directory with the getwd() command.\nRun it now in the Console! Is the returned path the project folder path?\nIf not, it might be that you are not working from a Quarto project. Check the top-right corner of RStudio: is the project name in there or do you see Project (none)?\nIf it’s the latter, you are not in a Quarto project, but you are running R from somewhere else (meaning, the working directory is somewhere else). If so, close RStudio and open the project.\n\n\n\n\n3.4 View the data\nNow we can finally view the data.\nThe easiest way is to click on the name of the data listed in the Environment tab, in the top-right panel of RStudio.\nYou will see a nicely formatted table, as you would in a programme like Excel.\nData tables in R (i.e. tabular, spread-sheet like data) are called data frames or tibbles.1\nThe shallow data frame contains 11 columns (called variables in the Environment tab). The 11 columns are the following:\n\nGroup: L1 vs L2 speakers of English.\nID: Subject unique ID.\nList: Word list (A to F).\nTarget: Target word in the lexical decision trial.\nACC: Lexical decision response accuracy (0 incorrect response, 1 correct response).\nRT: Reaction times of response in milliseconds.\nlogRT: Logged reaction times.\nCritical_Filler: Whether the trial was a filler or critical.\nWord_Nonword: Whether the Target was a real Word or a Nonword.\nRelation_type: The type of relation between prime and target word (Unrelated, NonCostituent, Constituent, Phonological).\nBranching: Constituent syntactic branching, Left and Right (shout out to Charlie Puth).\n\n\n\n\n\n\n\nQuiz 3\n\n\n\nHow many rows does shallow have?\n\n 11 650 6500"
  },
  {
    "objectID": "posts/data-types.html#import-excel-sheets",
    "href": "posts/data-types.html#import-excel-sheets",
    "title": "Data types",
    "section": "4 Import Excel sheets",
    "text": "4 Import Excel sheets\nTo read an Excel file we need first to attach the readxl package. It should already be installed, because it comes with the tidyverse. If not, then install it.\n\nlibrary(readxl)\n\nThen we can use the read_excel() function. Let’s read the file.\n\nrelatives &lt;- read_excel(\"./data/los2023/relatives.xlsx\")\n\nNow you can view the tibble los2023.\nNote that if the Excel file has more than one sheet, you can specify the sheet number when reading the file (the default is sheet = 1).\n\nrelatives_2 &lt;- read_excel(\"./data/los2023/relatives.xlsx\", sheet = 2)\n\nThe second sheet in los2023/relatives.xlx contains the description of the columns in the first sheet."
  },
  {
    "objectID": "posts/data-types.html#import-.rds-files",
    "href": "posts/data-types.html#import-.rds-files",
    "title": "Data types",
    "section": "5 Import .rds files",
    "text": "5 Import .rds files\nAnother useful type of data files is a file type specifically designed for r: .rds files.\nUsually, each .rds file contains one R object, like one tibble.\nYou can read .rds files with the readRDS() function.\n\nglot_status &lt;- readRDS(\"./data/coretta2022/glot_status.rds\")\n\nAs always, you need to assign the output of the function to a variable, here glot_status.\n\n\n\n\n\n\n.rds files\n\n\n\n.rds files are a type of R file which can store any R object and save it on disk.\nR objects can be saved to an .rds file with the saveRDS() function and they can be read with the readRDS() function.\n\n\nView the glot_status tibble now.\nIt is also very easy to save a tibble to an .rds file with the saveRDS() function.\nFor example:\n\nsaveRDS(shallow, \"./data/song2020/shallow.rds\")\n\nThe first argument is the name of the tibble object and the second argument is the file path to save the object to."
  },
  {
    "objectID": "posts/data-types.html#practice",
    "href": "posts/data-types.html#practice",
    "title": "Data types",
    "section": "6 Practice",
    "text": "6 Practice\n\n\n\n\n\n\nPractice 1\n\n\n\n\n\nRead the following files in R, making sure you use the right read_*() function.\n\nkoppensteiner2016/takete_maluma.txt (a tab separated file)\npankratz2021/si.csv\nGo to https://datashare.ed.ac.uk/handle/10283/4006 and download the file conflict_data_.xlsx. Read both sheets (“conflict_data2” and “demographics”). Any issues? (I suggest looking at the spread sheet in Excel if it helps)."
  },
  {
    "objectID": "posts/data-types.html#summary",
    "href": "posts/data-types.html#summary",
    "title": "Data types",
    "section": "7 Summary",
    "text": "7 Summary\n\n\n\n\n\n\n\nYou can import tabular data in R with the read_*() functions from the tidyverse package readr.\nYou can view data in RStudio as spreadsheets.\n\n\n\n\n\n\n\n\n\n\nNext\n\n\n\n\nImport multiple files\nStatistical variables"
  },
  {
    "objectID": "posts/data-types.html#footnotes",
    "href": "posts/data-types.html#footnotes",
    "title": "Data types",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA tibble is a special data frame. We will learn more about tibbles in the following weeks.↩︎"
  },
  {
    "objectID": "posts/intro-regression-categorical.html",
    "href": "posts/intro-regression-categorical.html",
    "title": "Introduction to regression models (Part III): include categorical predictors",
    "section": "",
    "text": "Prerequisites\n\n\n\n\nIntroduction to regression models (Part II)."
  },
  {
    "objectID": "posts/intro-regression-categorical.html#categorical-predictors",
    "href": "posts/intro-regression-categorical.html#categorical-predictors",
    "title": "Introduction to regression models (Part III): include categorical predictors",
    "section": "1 Categorical predictors",
    "text": "1 Categorical predictors\nIn theory, regression models support predictors that are numeric. Imagine trying to calculate \\(y = 3 + 2x\\) where \\(x\\) is categorical height, with levels “high” and “low”. That doesn’t make any sense, right?\nA categorical variable doesn’t work as is and it needs to be transformed to numbers. This transformation is called coding of categorical variable in the regression world.\nWhen fitting a regression models, R does this transformation for you automatically under the hood. So you don’t have to do it manually, but learning exactly what this transformation entails is necessary to understand how to interpret the summary of the model.\nThere are a few ways of coding categorical predictors as numbers. These ways are called contrasts in R. The default coding system is the treatment contrasts system.\nLet’s see what treatment contrasts are.\n\n\n\n\n\n\nNote\n\n\n\nAs with anything else in stats, naming of coding systems is not an established matter and the same coding can have different names, and vice versa the same name could refer to different systems.\nFor an excellent overview, see https://debruine.github.io/faux/articles/contrasts.html."
  },
  {
    "objectID": "posts/intro-regression-categorical.html#treatment-coding",
    "href": "posts/intro-regression-categorical.html#treatment-coding",
    "title": "Introduction to regression models (Part III): include categorical predictors",
    "section": "2 Treatment coding",
    "text": "2 Treatment coding\nTreatment coding (“treatment contrasts” in R) use 0 and 1 to code categorical predictors.\nFor example, the variable attitude from the polite data (winter2012/polite.csv) has two levels: inf and pol.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\npolite &lt;- read_csv(\"data/winter2012/polite.csv\")\n\nRows: 224 Columns: 27\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (6): subject, gender, birthplace, musicstudent, task, attitude\ndbl (21): months_ger, scenario, total_duration, articulation_rate, f0mn, f0s...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npolite\n\n\n  \n\n\nunique(polite$attitude)\n\n[1] \"inf\" \"pol\"\n\n\nWe can code the levels of attitude using 0 for inf and 1 for pol. Let’s call this new coded variable is_polite: [is_polite = 0] corresponds to [attitude = informal] and [is_polite = 1] to [attitude = polite].\n\n\n\nattitude\nis_polite\n\n\n\n\ninf(ormal)\n0\n\n\npol(ite)\n1\n\n\n\nWith treatment coding, the first level in the predictor is called the reference level. Based on how we have coded attitude, the reference level is informal.\nBy default, the order of levels in a categorical predictor is based on alphabetical order. But you can specify the order manually using the factor() function (you will see how below).\n\npolite %&gt;%\n  mutate(\n    is_polite = ifelse(attitude == \"inf\", 0, 1)\n  ) %&gt;%\n  select(HNRmn, attitude, is_polite)\n\n\n  \n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nRemember that this is done automatically by R under the hood for you so you never have to do it by hand!!!\nWe will use attitude in the model.\n\n\n\n2.1 Modelling Harmonics-to-Noise Ratio (HNR) by attitude\nLet’s assume that the HNR values come from a Gaussian distribution.1 In notation:\n\\[\n\\text{HNRmn} \\sim Gaussian(\\mu, \\sigma)\n\\]\nNow, we want to estimate \\(\\mu\\) so that we take into consideration whether the attitude is “informal” or “polite”. In other words, we want to model HNR as a function of attitude, in R code HNRmn ~ attitude.\nattitude is a categorical variable (a chr character column in polite) that will be coded automatically in the model using the default treatment coding system as we have seen above (this is done under the hood by R for you!): [attitude = informal] is 0 and [attitude = polite] is 1.\nNow we allow \\(\\mu\\) to vary depending on attitude. In the formula, we include the coded variable is_polite rather than the original variable.\n\\[\n\\mu = \\beta_0 + \\beta_1 \\cdot \\text{is\\_polite}\n\\]\nLet’s unpack the equation. There’s a bit of algebra in what follows, so take your time if this is a bit out of your comfort zone. But it’s worth it—familiarity with basic algebra will also help you become more comfortable working with linear models.\n\n\\(\\beta_0\\) is the mean HNR \\(\\mu\\) when [attitude = informal]. That’s because the coded variable \\(is_attitude\\) takes on the value 0 when [attitude = informal]. Multiplying \\(\\beta_1\\) by 0 means that \\(\\beta_1\\) vanishes from the equation, and all that’s left is \\(\\beta_0\\).\n\n\\[\n\\begin{aligned}\n\\mu &= \\beta_0 + \\beta_1 \\cdot \\text{is\\_polite} \\\\\n\\mu_{attitude = inf} &= \\beta_0 + \\beta_1 \\cdot 0 \\\\\n\\mu_{attitude = inf} &= \\beta_0 \\\\\n\\end{aligned}\n\\]\n\n\\(\\beta_1\\) is the difference in mean HNR \\(\\mu\\) between the mean HNR when [attitude = polite] and the mean valence when [attitude = informal]. That’s because \\(is_polite\\) takes on the value 1 when [attitude = polite], and multiplying \\(\\beta_1\\) by 1 leaves it unchanged in the equation.\n\n\\[\n\\begin{aligned}\n\\mu &= \\beta_0 + \\beta_1 \\cdot \\text{is\\_polite}\\\\\n\\mu_{attitude = inf} &= \\beta_0 + \\beta_1 \\cdot 1 \\\\\n\\mu_{attitude = pol} &= \\beta_0 + \\beta_1 \\\\\n\\end{aligned}\n\\]\nFrom this, we know that \\(\\beta_1\\) represents the difference between the mean HRN when [attitude = informal] and the mean HNR when [attitude = polite]? Remember that \\(\\mu_{mod = smell} = \\beta_0\\), as we said in the first point above. We can substitute this into the equation from the last point, and then isolate \\(\\beta_1\\) step by step as follows:\n\\[\n\\begin{aligned}\n\\beta_0 & = \\mu_{attitude=inf} \\\\\n\\mu_{attitude=pol} &= \\beta_0 + \\beta_1 \\\\\n\\mu_{attitude=pol} &= \\mu_{attitude=inf} + \\beta_1 \\\\\n\\mu_{attitude=pol} - \\mu_{attitude=inf} &= \\mu_{attitude=inf} + \\beta_1 - \\mu_{attitude=inf} \\\\\n\\mu_{attitude=pol} - \\mu_{attitude=inf} &= \\beta_1\n\\end{aligned}\n\\]\nSo, \\(\\beta_1\\) really is the difference between the means HNR value when attitude is polite vs informal.\n\n\n\n\n\n\nImportant\n\n\n\nThis is always a point of confusion for beginners.\nWhile the intercept (\\(\\beta_0\\)) corresponds to the mean of the outcome variable when the categorical predictor is at its reference level, \\(\\beta_1\\) corresponds to the difference in the mean outcome variable when the categorical predictor level is the other level vs when it is the reference level.\n\n\nHere’s the equation of the model we will be fitting.\n\\[\n\\begin{align}\n\\text{HNRmn} & \\sim Gaussian(\\mu, \\sigma) \\\\\n\\mu        & = \\beta_0 + \\beta_1 \\cdot \\text{is\\_polite} \\\\\n\\end{align}\n\\]\n\n\n\n\n\n\nQuiz 1\n\n\n\nSince informal speech in the data seem to have on average lower HNR than polite speech, will \\(\\beta_1\\) be:\n\n Positive. 0. Negative.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nBe careful: \\(\\beta_1\\) is the difference in HMRmn between polite and informal attitude (polite - informal), rather than the difference between informal and polite attitude (informal - polite). This is because inf (informal) is the reference level of attitude.\n\n\n\n\n\n\n\n2.2 Run the model\nNow we know what the model will do, we can run it.\n\nlibrary(brms)\n\nLoading required package: Rcpp\n\n\nLoading 'brms' package (version 2.21.0). Useful instructions\ncan be found by typing help('brms'). A more detailed introduction\nto the package is available through vignette('brms_overview').\n\n\n\nAttaching package: 'brms'\n\n\nThe following object is masked from 'package:stats':\n\n    ar\n\nhnr_bm &lt;- brm(\n  HNRmn ~ attitude,\n  family = gaussian(),\n  data = polite,\n  cores = 4,\n  file = \"data/cache/hnr_bm\",\n  seed = 2913\n)\n\nWhen reporting the model specification, you can write the following:\n\nWe fitted a Bayesian model with mean HNR as the outcome variable and attitude (informal vs polite) as the only predictor. A Gaussian family was used as the outcome distribution family. The attitude predictor was coded with the default treatment contrasts (with “informal” as the reference level).\n\n\n\n2.3 Interpret the model output\nLet’s inspect the model summary.\n\nsummary(hnr_bm)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: HNRmn ~ attitude \n   Data: polite (Number of observations: 224) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept         16.27      0.16    15.97    16.59 1.00     4136     3237\nattitudepolite     1.25      0.23     0.80     1.70 1.00     4396     2937\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.69      0.08     1.54     1.86 1.00     4065     2979\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nLook at the Regression Coefficients table.\n\n\nRegression Coefficients:\n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept         16.27      0.16    15.97    16.59 1.00     4136     3237\nattitudepolite     1.25      0.23     0.80     1.70 1.00     4396     2937\n\n\nWe get two coefficients:\n\nIntercept: this is our \\(\\beta_0\\).\nattitudepol: this is our \\(\\beta_1\\).\n\nEach coefficient has an Estimate and an Est.Error (estimate error). As we have seen in previous tutorials, these are the mean and SD of the posterior probability distribution of the coefficients.\nThis means that:\n\nThe probability distribution of the mean HNR when [attitude = informal] has mean = 16.27 and SD = 0.16.\nThe probability distribution of the difference between the mean HNR of [attitude = polite] and that of [attitude = informal] is has mean = 1.25 and SD = 0.23.\n\n\n\n\n\n\n\nQuiz 2\n\n\n\nWhat is the mean HNR for taste words?\n\n\n\n\n\n\nHint\n\n\n\n\n\nJust use the formula of \\(\\mu\\) and replace the \\(\\beta\\) coefficients with the mean of the respective distributions.\n\n\n\n\n\nNow let’s look at the CrIs (CI in the model summary) of the coefficients. Based on the reported 95% CrIs, we can say that:\n\nWe can be 95% confident that (you can also say, “there is a 95% probability that”), based on the data and the model, the mean HNR of informal speech is between 16 and 16.6 dB.\nThere is a 95% probability that the difference in mean HNR between polite and informal speech is between 0.8 and 1.7 dB. In other words, HNR increases by 0.8 to 1.7 dB in polite speech relative to informal speech.\n\nYou could report these results in the following way:\n\nAccording to the model, the mean HNR of informal speech is 16 dB (SD = 0.16), with a 95% probability that it is between 16 and 16.6 dB. At 95% confidence, the difference in HNR between polite and informal speech is between 0.8 and 1.7 (\\(\\beta\\) = 1.25, SD = 0.23).\n\nBut what about the probability distribution of the HNR of polite speech? This information is not in the summary per se.\nTo obtain that, we need to do a bit of data wrangling with the model draws. You can learn about it in Working with MCMC draws.\n\n\n\n\n\n\nNext\n\n\n\n\nRegression models: working with MCMC draws\nRegression: Indexing of categorical predictors"
  },
  {
    "objectID": "posts/intro-regression-categorical.html#footnotes",
    "href": "posts/intro-regression-categorical.html#footnotes",
    "title": "Introduction to regression models (Part III): include categorical predictors",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that, in real-life research contexts, you should decide which distribution to use for different outcome variables by drawing on previous research that has assessed the possible distribution families for those variables, on domain-specific expertise or common-sense. You will see that in most cases with linguistic phenomena we know very little about the nature of the variables we work with, hence it can be difficult to know which family of distributions to use.↩︎"
  },
  {
    "objectID": "posts/wrangle-pivoting.html#pivoting",
    "href": "posts/wrangle-pivoting.html#pivoting",
    "title": "Pivoting",
    "section": "1 Pivoting",
    "text": "1 Pivoting\nPivoting is the process of changing the shape of the data from a long format to a wide format and vice versa. You can find nice animations on Garrick’s website that illustrate the process.\nCh 5 of the R4DS book introduces the concept of long and wide formats and teaches you how to pivot data using the pivot_wider() and pivot_longer() functions from tidyr (a tidyverse package).\nMake sure you try the code out yourself and feel free to try the exercises as well (you can find the solutions here).\nThe documentation of the pivot functions is also a good place to learn about how they work: https://tidyr.tidyverse.org/articles/pivot.html."
  },
  {
    "objectID": "posts/wrangle-pivoting.html#practice",
    "href": "posts/wrangle-pivoting.html#practice",
    "title": "Pivoting",
    "section": "2 Practice",
    "text": "2 Practice\nYou can practice with the coretta2018/ultrasound/ data which includes a set of files with data from ultrasound tongue imaging of Italian and Polish speakers.\n\nfiles &lt;- list.files(\n  \"data/coretta2018/ultrasound\",\n  full.names = TRUE,\n  pattern = \"*-tongue-cart.tsv\"\n)\n\n# Column names of the first 14 columns. The rest of the columns are X and Y\n# coordinates of tongue contours of 42 points along the contour:\n# X1,Y1,X2,Y2,X3,Y3,...,X42,Y42.\n#\n# Note that R automatically names unnamed columns with X followed by\n# the column number, so the 84 coordinate columns will be all named Xn.\ncolumns &lt;- c(\n  \"speaker\",\n  \"seconds\",\n  \"rec_date\",\n  \"prompt\",\n  \"label\",\n  \"TT_displacement_sm\",\n  \"TT_velocity\",\n  \"TT_velocity_abs\",\n  \"TD_displacement_sm\",\n  \"TD_velocity\",\n  \"TD_velocity_abs\",\n  \"TR_displacement_sm\",\n  \"TR_velocity\",\n  \"TR_velocity_abs\"\n)\n\ntongue &lt;- read_tsv(files, id = \"file\", col_names = columns, na = \"*\")\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 7598 Columns: 99\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (4): speaker, rec_date, prompt, label\ndbl (94): seconds, TT_displacement_sm, TT_velocity, TT_velocity_abs, TD_disp...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntongue\n\n\n  \n\n\n\nIf you are wondering what na = \"*\" does, it just tells R that cells with * in them should be treated as NAs.\nFab! Now why don’t you practice pivoting and change the tongue tibble from a wide format to a long format where each row has the X,Y coordinates of a single point out of the 42 points?\nI will give you a head start by renaming the columns with the X,Y coordinates for you. Make sure you inspect the code and understand what it does.\n\nnew_col_names &lt;- paste0(rep(c(\"X\", \"Y\"), 42), \"_\", rep(1:42, each = 2))\n\n# The square brakets [] are a way of extracting specific items from a vector.\n# This method is called \"indexing\" and specifically this is the \"braket indexing\"\n# method. See the Box below for a more detailed explanation.\nexisting_col_names &lt;- colnames(tongue)[1:15]\n\ncolnames(tongue) &lt;- c(existing_col_names, new_col_names)\n\nNow transform the data so that you end up with three new columns that replace all the X_n, Y_n columns: point with the point number (1 to 42), X with the x coordinate and Y with the y coordinate. You will need to use the separate() function to separate the X_n, Y_n names into X and n and Y and n.\n\n\n\n\n\n\nHint\n\n\n\n\n\nFollow this workflow:\n\nPivot all the Xn,Yn columns from wide to longer.\nSeparate the column that has X_n’s and X_y’s into two columns: one that has X or Y and one with the point number 1 to 42. You can use the separate() function to do so. Check ?separate.\nNow pivot from long to wider so that you get an X column and a Y column\n\nIt should look like this (only relevant columns shown):\n\n\n\n  \n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ntongue |&gt; \n  pivot_longer(X_1:Y_42, names_to = \"coord\", values_to = \"value\") |&gt; \n  drop_na(value) |&gt; \n  separate(coord, c(\"axis\", \"point\")) |&gt; \n  pivot_wider(names_from = axis, values_from = value) |&gt; \n  select(prompt, point, X, Y)"
  },
  {
    "objectID": "posts/regression-indexing.html",
    "href": "posts/regression-indexing.html",
    "title": "Regression: Indexing of categorical predictors",
    "section": "",
    "text": "Prerequisites\n\n\n\n\nIntroduction to regression models (Part III): include categorical predictors."
  },
  {
    "objectID": "posts/regression-indexing.html#categorical-predictors-contrasts-vs-indexing",
    "href": "posts/regression-indexing.html#categorical-predictors-contrasts-vs-indexing",
    "title": "Regression: Indexing of categorical predictors",
    "section": "1 Categorical predictors: contrasts vs indexing",
    "text": "1 Categorical predictors: contrasts vs indexing\nCategorical predictors in regression models are coded by default using the so-called “treatment” contrasts, a type of contrast coding.\nThis means that the intercept of the model will correspond to the predicted value when the categorical predictor is at the reference level, and the other coefficients will be the difference between the predicted value when the categorical predictor is at the other levels and the intercept.\nThis way of coding categorical predictors makes it more difficult to set up priors and can at times make it less straightforward to interpret the model summary.\nIn this post, you will learn about a different method of coding categorical predictors, called indexing.\nWith this method, the model estimates the predicted value for each level of the predictor, rather than for just the reference level. This means that you don’t get coefficients that are differences between predicted values at different levels.\nLet’s see an example of both methods using data from Coretta 2018."
  },
  {
    "objectID": "posts/regression-indexing.html#the-data",
    "href": "posts/regression-indexing.html#the-data",
    "title": "Regression: Indexing of categorical predictors",
    "section": "2 The data",
    "text": "2 The data\n\n# Install the coretta2018itapol package first if needed.\n# remotes::install_github(\"stefanocoretta/coretta2018itapol\")\nlibrary(coretta2018itapol)\ndata(\"token_measures\")\n\nHere’s a preview of the data.\n\n# Let's remove NAs from the v1_duration column\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ntoken_measures &lt;- token_measures |&gt; \n  drop_na(v1_duration)\n\ntoken_measures"
  },
  {
    "objectID": "posts/regression-indexing.html#treatment-contrasts",
    "href": "posts/regression-indexing.html#treatment-contrasts",
    "title": "Regression: Indexing of categorical predictors",
    "section": "3 Treatment contrasts",
    "text": "3 Treatment contrasts\n\nlibrary(brms)\n\nLoading required package: Rcpp\n\n\nLoading 'brms' package (version 2.21.0). Useful instructions\ncan be found by typing help('brms'). A more detailed introduction\nto the package is available through vignette('brms_overview').\n\n\n\nAttaching package: 'brms'\n\n\nThe following object is masked from 'package:stats':\n\n    ar\n\nget_prior(\n  v1_duration ~ c2_phonation,\n  family = lognormal,\n  data = token_measures,\n)\n\n\n  \n\n\n\nWe get an Intercept (that’s the expected vowel duration when the following consonant is voiced) and a class = b prior for the coefficient c2_phonationvoiceless. The coefficient c2_phonationvoiceless is the difference between the vowel duration when the following consonant is voiceless and the vowel duration when the following consonant is voiced.\n\\[\n\\begin{align}\nvdur & \\sim LogNormal(\\mu, \\sigma) \\\\\n\\mu & = \\beta_0 + \\beta_1 \\cdot \\text{voiceless}\n\\end{align}\n\\]\nwhere \\(\\text{voiceless}\\) is an “indicator” (aka dummy) variable that “turns on” the \\(\\beta_1\\) coefficient in the formula of \\(\\mu\\) when the following consonant is voiceless.\nHence why \\(\\beta_1\\) is a difference between expected values (the value when C2 is voiceless - the value when C2 is voiced).\nSetting priors on \\(\\beta_0\\) and \\(\\beta_1\\) means we need to think about:\n\nWhat we expect the mean value duration to be when C2 is voiced, and\nWhat we expect the difference in mean between vowels followed by a voiceless vs voiced consonant.\n\nIt is much easier to set priors on the mean value duration when C2 is voiced and when C2 is voiceless. We can use indexing to achieve that."
  },
  {
    "objectID": "posts/regression-indexing.html#indexing",
    "href": "posts/regression-indexing.html#indexing",
    "title": "Regression: Indexing of categorical predictors",
    "section": "4 Indexing",
    "text": "4 Indexing\nWith indexing, each level in a categorical predictor gets it’s own coefficient and the model estimates the predicted value of the outcome at each level.\nDifferently from setting contrasts with contrasts()&lt;-, indexing is applied by suppressing the intercept of the model with 0 +.\n\nget_prior(\n  v1_duration ~ 0 + c2_phonation,\n  family = lognormal,\n  data = token_measures,\n)\n\n\n  \n\n\n\nYou can see that there is no class = Intercept in the output of get_prior() because we “suppressed” the intercept of the model.\nNow, c2_phonationvoiced is the mean vowel duration when the following consonant is voiced and c2_phonationvoiceless is the mean vowel duration when the following consonant is voiceless."
  },
  {
    "objectID": "posts/regression-indexing.html#fitting-a-model-with-indexing",
    "href": "posts/regression-indexing.html#fitting-a-model-with-indexing",
    "title": "Regression: Indexing of categorical predictors",
    "section": "5 Fitting a model with indexing",
    "text": "5 Fitting a model with indexing\n\\[\n\\begin{align}\nvdur_i & \\sim LogNormal(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\alpha_{\\text{Phon}[i]} \\\\\n\\end{align}\n\\]\n\nm_i &lt;- brm(\n  v1_duration ~ 0 + c2_phonation,\n  family = lognormal,\n  data = token_measures,\n  seed = 1425,\n  file = \"data/cache/regression-indexing-m_i\"\n)\n\n\nsummary(m_i)\n\n Family: lognormal \n  Links: mu = identity; sigma = identity \nFormula: v1_duration ~ 0 + c2_phonation \n   Data: token_measures (Number of observations: 1342) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nc2_phonationvoiced        4.65      0.01     4.62     4.67 1.00     4501\nc2_phonationvoiceless     4.54      0.01     4.52     4.57 1.00     4626\n                      Tail_ESS\nc2_phonationvoiced        3019\nc2_phonationvoiceless     3162\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.32      0.01     0.31     0.34 1.00     4025     2811\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "posts/regression-indexing.html#get-the-difference-between-levels",
    "href": "posts/regression-indexing.html#get-the-difference-between-levels",
    "title": "Regression: Indexing of categorical predictors",
    "section": "6 Get the difference between levels",
    "text": "6 Get the difference between levels\nWe can now quickly calculate the difference between c2_phonationvoiced and c2_phonationvoiceless with the avg_comparisons() function from the marginaleffects package.\n\nlibrary(marginaleffects)\n# you will also have to install the collapse package\n\navg_comparisons(m_i, variables = \"c2_phonation\")\n\n\n Estimate 2.5 % 97.5 %\n    -10.9 -14.5  -7.35\n\nTerm: c2_phonation\nType:  response \nComparison: mean(voiceless) - mean(voiced)\nColumns: term, contrast, estimate, conf.low, conf.high, predicted_lo, predicted_hi, predicted, tmp_idx \n\n\nSo based on the model and data, vowels followed by voiceless consonants are 7-14.5 ms shorter than vowels followed by voiced stops, at 95% probability.\n\n\n\n\n\n\nNext\n\n\n\n\nRegression models: interactions"
  },
  {
    "objectID": "posts/plot-faceting.html",
    "href": "posts/plot-faceting.html",
    "title": "Faceting plots",
    "section": "",
    "text": "Prerequisites\n\n\n\n\nPlotting basics in R\nlibrary(tidyverse)\ntheme_set(theme_light())"
  },
  {
    "objectID": "posts/plot-faceting.html#faceting-and-panels",
    "href": "posts/plot-faceting.html#faceting-and-panels",
    "title": "Faceting plots",
    "section": "1 Faceting and panels",
    "text": "1 Faceting and panels\nSometimes we might want to separate the data into separate panels within the same plot.\nWe can achieve that easily using faceting."
  },
  {
    "objectID": "posts/plot-faceting.html#politeness-and-speech",
    "href": "posts/plot-faceting.html#politeness-and-speech",
    "title": "Faceting plots",
    "section": "2 Politeness and speech",
    "text": "2 Politeness and speech\nLet’s revisit the plots from Plotting basics in R. We will use the polite data again.\nThis is the plot you previously made. Try and reproduce it by writing the code yourself (you also have to read in the data!).\n\n\n\n\n\n\n\n\nFigure 1: Scatter plot of mean f0 and H1-H2 difference.\n\n\n\n\n\n\n2.1 Does being a music student matter?\nThat looks great, but we want to know if being a music student has an effect on the relationship of f0mn and H1H2.\nIn the plot above, the aesthetics mappings are the following:\n\nf0mn on the x-axis.\nH1H2 on the y-axis.\ngender as colour.\n\nHow can we separate data further depending on whether the participant is a music student or not (musicstudent)?\nWe can create panels using facet_grid(). This function takes lists of variables to specify panels in rows and/or columns.\n\n2.1.1 Faceting\nFaceting a plot allows to split the plot into multiple panels, arranged in rows and columns, based on one or more variables.\nTo facet a plot, use the facet_grid() function.\nThe syntax is a bit strange. You can specify rows of panels with the rows argument and columns of panels with cols argument, but you have to include column names inside vars(), like this:\n\npolite |&gt;\n  ggplot(aes(f0mn, H1H2, colour = gender)) +\n  geom_point() +\n  facet_grid(cols = vars(musicstudent)) +\n  labs(\n    x = \"Mean f0 (Hz)\",\n    y = \"H1-H2 difference (dB)\",\n    colour = \"Gender\"\n  )\n\n\n\n\n\n\n\nFigure 2: Scatter plot of mean f0 and H1-H2 difference in non-music students (left) vs music students (right).\n\n\n\n\n\nYou could write a description of this plot like this:\n\nFigure 2 shows mean f0 and H1-H2 difference as a scatter plot. The two panels indicate whether the participant was a student of music. Within each panel, the participant’s gender is represented by colour (red for female and blue for male). Male participants tend to have higher H1-H2 differences and lower mean f0 than females. From the plot it can also be seen that there is greater variability in H1-H2 difference in female music students compared to female non-music participants. Within each group of gender by music student there does not seem to be any specific relation between mean f0 and H1-H2 difference.\n\nThe polite data also has a column attitude with values inf for informal and pol for polite. Subjects were asked to speak either as if they were talking to a friend (inf attitude) or to someone with a higher status (pol attitude).\nRecreate the last plot, this time faceting also by attitude. Use the rows column to create two separate rows for each value of attitude.\n\npolite |&gt;\n  ggplot(aes(f0mn, H1H2, colour = gender)) +\n  geom_point() +\n  facet_grid(cols = vars(musicstudent), rows = ...)\n\nNow write a description of the plot."
  },
  {
    "objectID": "posts/computer-basics.html#some-computer-basics",
    "href": "posts/computer-basics.html#some-computer-basics",
    "title": "Basic computer literacy",
    "section": "1 Some computer basics",
    "text": "1 Some computer basics\nIn the tutorial last week you’ve been playing around with R, RStudio and R scripts.\nBut what if you want to import data in R?\nEasy! You can use the read_*() functions (* is just a place holder for specific types of files to read, like read_csv() or read_excel()) to read your files into R. But before we dive in, let’s first talk about some computer basics. (You can skip this section if it’s too basic for you.)\n\n\n\n\n\n\nEnable all extensions\n\n\n\nBefore moving on, we recommend you enable the option to show all file extensions in the File Explorer/Finder.\nFollow the instructions here:\n\nWindows: https://support.microsoft.com/en-us/windows/common-file-name-extensions-in-windows-da4a4430-8e76-89c5-59f7-1cdbbc75cb01\nmacOS (For all files): https://support.apple.com/en-gb/guide/mac-help/mchlp2304/mac#:~:text=In%20the%20Finder%20on%20your,“Show%20all%20filename%20extensions”.\n\n\n\n\n1.1 Files, folder and file extensions\nFiles saved on your computer live in a specific place. For example, if you download a file from a browser (like Google Chrome, Safari or Firefox), the file is normally saved in the Download folder.\nBut where does the Download folder live? Usually, in your user folder! The user folder normally is the name of your account or a name you picked when you created your computer account. In my case, my user folder is simply called ste.\n\n\n\n\n\n\nUser folder\n\n\n\nThe user folder is the folder with the name of your account.\n\n\n\n\n\n\n\n\nHow to find your user folder name\n\n\n\n\n\nOn macOS\n\nGo to Finder &gt; Preferences/Settings.\nGo to Sidebar.\nThe name next to the house icon is the name of your home folder.\n\nOn Windows\n\nRight-click an empty area on the navigation panel in File Explorer.\nFrom the context menu, select the ‘Show all folders’ and your user profile will be added as a location in the navigation bar.\n\n\n\n\nSo, let’s assume I download a file, let’s say big_data.csv, in the Download folder of my user folder.\nNow we can represent the location of the big_data.csv file like so:\nste/\n└── Downloads/\n    └── big_data.csv\nTo mark that ste and Downloads are folders, we add a final forward slash /. That simply means “hey! I am a folder!”. big_data.csv is a file, so it doesn’t have a final /.\nInstead, the file name big_data.csv has a file extension. The file extension is .csv. A file extension marks the type of file: in this the big_data file is a .csv file, a comma separated value file (we will see an example of what that looks like later).\nDifferent file type have different file extensions:\n\nExcel files: .xlsx.\nPlain text files: .txt.\nImages: .png, .jpg, .gif.\nAudio: .mp3, .wav.\nVideo: .mp4, .mov, .avi.\nEtc…\n\n\n\n\n\n\n\nFile extension\n\n\n\nA file extension is a sequence of letters that indicates the type of a file and it’s separated with a . from the file name.\n\n\n\n\n1.2 File paths\nNow, we can use an alternative, more succinct way, to represent the location of the big_data.csv:\nste/Downloads/big_data.csv\nThis is called a file path! It’s the path through folders that lead you to the file. Folders are separated by / and the file is marked with the extension .csv.\n\n\n\n\n\n\nFile path\n\n\n\nA file path indicates the location of a file on a computer as a path through folders that lead you to the file.\n\n\nNow the million pound question: where does ste/ live on my computer???\nUser folders are located in different places depending on the operating system you are using:\n\nOn macOS: the user folder is in /Users/.\n\nYou will notice that there is a forward slash also before the name of the folder. That is because the /Users/ folder is a top folder, i.e. there are no folders further up in the hierarchy of folders.\nThis means that the full path for the big_data.csv file on a computer running macOS would be: /Users/ste/Downloads/big_data.csv.\n\nOn Windows: the user folder is in C:/Users/\n\nYou will notice that C is followed by a colon :. That is because C is a drive, which contains files and folders. C: is not contained by any other folder, i.e. there are no other folders above C: in the hierarchy of folders.\nThis means that the full path for the big_data.csv file on a Windows computer would be: C:/Users/ste/Downloads/big_data.csv.\n\n\nWhen a file path starts from a top-most folder, we call that path the absolute file path.\n\n\n\n\n\n\nAbsolute path\n\n\n\nAn absolute path is a file path that starts with a top-most folder.\n\n\nThere is another type of file paths, called relative paths. A relative path is a partial file path, relative to a specific folder. You will learn how to use relative paths below, when we will go through importing files in R using R scripts below.\nImporting files in R is very easy with the tidyverse packages. You just need to know the file type (very often the file extension helps) and the location of the file (i.e. the file path).\nThe next sections will teach you how to import data in R!\n\n\n\n\n\n\nQuiz 1\n\n\n\nWhich of the following is an absolute path?\n\n Downloads/courses/dal/data/ /Users/smruti/Downloads/data/files/ sascha/Documents/files_pdf/paper.pdf\n\n\n\n\n\n\n\n\n\nNext\n\n\n\n\nR basics"
  },
  {
    "objectID": "map.html",
    "href": "map.html",
    "title": "The Adventurer’s Map",
    "section": "",
    "text": "Here you can find the “Adventurer’s Map” to the course. The Map shows the pathways you can take among the Notebook entries. This Map will grow throughout the semester as new entries are added to the Notebook.\n\n\n\n\n\n\ngraph TD\n  A[Start here]\n  B[Hodotics]\n  click B href \"posts/hodotics.html\" \"Go to page\"\n  C[Computer basics]\n  click C href \"posts/computer-basics.html\" \"Go to page\"\n  D[R basics]\n  click D href \"posts/r-basics.html\" \"Go to page\"\n  E[Statistical variables]\n  click E href \"posts/stat-variables.html\" \"Go to page\"\n  F[Intro to Quarto]\n  click F href \"posts/intro-quarto.html\" \"Go to page\"\n  G[R Troubleshooting]\n  click G href \"posts/r-troubleshooting.html\" \"Go to page\"\n  H[Data types]\n  click H href \"posts/data-types.html\" \"Go to page\"\n  I[Read multiple files]\n  click I href \"posts/import-multiple.html\" \"Go to page\"\n  J[Introduction to regression]\n  click J href \"posts/intro-regression.html\" \"Go to page\"\n  K[Introduction to plotting]\n  click K href \"posts/intro-plot.html\" \"Go to page\"\n  L[Wrangling data]\n  click L href \"posts/wrangling.html\" \"Go to page\"\n  M[Plotting basics]\n  click M href \"posts/plotting-basics.html\" \"Go to page\"\n  N[Bar charts]\n  click N href \"posts/bar-charts.html\" \"Go to page\"\n  O[Filtering]\n  click O href \"posts/wrangle-filter.html\" \"Go to page\"\n  P[Mutating]\n  click P href \"posts/wrangle-mutate.html\" \"Go to page\"\n  Q[Pivoting]\n  click Q href \"posts/wrangle-pivoting.html\" \"Go to page\"\n  Z[Faceting plots]\n  click Z href \"posts/plot-faceting.html\" \"Go to page\"\n  AA[Density plots]\n  click AA href \"posts/plot-density.html\" \"Go to page\"\n  AB[Advanced plotting]\n  click AB href \"posts/plot-advanced.html\" \"Go to page\"\n  AC[Summary measures]\n  click AC href \"posts/summaries.html\" \"Go to page\"\n  \n  AD[Empirical research]\n  click AD href \"posts/research-context.html\" \"Go to page\"\n  AE[Research Qs and Hs]\n  click AE href \"posts/research-qh.html\" \"Go to page\"\n  AF[Data analysis]\n  click AF href \"posts/data-analysis.html\" \"Go to page\"\n  AG[Inference]\n  click AG href \"posts/inference.html\" \"Go to page\"\n  AH[Statistics]\n  click AH href \"posts/statistics.html\" \"Go to page\"\n  AI[Repro/repli]\n  click AI href \"posts/repro-repli.html\" \"Go to page\"\n  AJ[Proj Management]\n  click AJ href \"posts/management.html\" \"Go to page\"\n  AK[File management]\n  click AK href \"posts/file-manage.html\" \"Go to page\"\n  \n  R[Regression: predictors]\n  click R href \"posts/intro-regression-predictors.html\" \"Go to page\"\n  S[Regression: categorical predictors]\n  click S href \"posts/intro-regression-categorical.html\" \"Go to page\"\n  T[MCMC draws]\n  click T href \"posts/regression-draws.html\" \"Go to page\"\n  U[Indexing predictors]\n  click U href \"posts/regression-indexing.html\" \"Go to page\"\n  V[Interactions]\n  click V href \"posts/regression-interactions.html\" \"Go to page\"\n  W[Categorical-categorical interactions]\n  click W href \"posts/regression-interactions-catcat.html\" \"Go to page\"\n  X[Bernoulli regression]\n  click X href \"posts/regression-bernoulli.html\" \"Go to page\"\n  \n  A --&gt; B & C\n  \n  B --&gt; AD & AF & AJ\n  AD --&gt; AE\n  AE --&gt; AI\n  AF --&gt; AG & AH\n  AJ --&gt; AK\n  \n  C --&gt; D\n  \n  D --&gt; E & F & G\n  \n  F --&gt; H\n  H --&gt; I & E\n  I --&gt; Q\n  \n  E --&gt; J & K & L & AC\n  \n  K --&gt; M\n  M --&gt; N & Z & AA\n  N --&gt; AB\n  AA --&gt; AB\n\n  L --&gt; O & P & Q\n  P --&gt; N & AA\n  \n  %% Regression\n  J --&gt; R --&gt; S --&gt; T & U\n  U --&gt; V & X\n  V --&gt; W"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "The Schedule",
    "section": "",
    "text": "The course is structured in a “choose your own adventure” format, so there isn’t a set content for each week.\n\n\n\n\n\n\nImportant\n\n\n\nEach week, you will attend two classes:\n\nA one-hour seminar.\nA two-hour lab.\n\nNote that you are divided in two groups, so that the seminar and lab run twice each week, you just need to attend one of the seminars and one of the labs each week.\n\n\nThe seminars will have a different major theme each week, which you will have to research before class and discuss in groups in class.\nYou are also expected to learn quantitative methods outside of class, at your own time (as in a flipped classroom). This website has a list of entries in the Adventurer’s Notebook. You are not restricted to those posts, and you can learn from other resources, however if this is the very first time you are learning quantitative methods I recommend you stick to those since they have been curated for beginners. You can find a map of how the different posts build on each other in the Adventurer’s Map.\nThe labs will create the opportunity for you to practice quantitative data analysis with R. Each week you will pick a “challenge” from a list or come up with your own challenge and work towards solving it using R. More details will be shared soon.\nAs part of the Assessments, you will have to complete a group project. You can start working on the project any time during the course, but the last two labs will be dedicated to working on your project while being able to get help from the instructor (Stefano) and the Tutors."
  },
  {
    "objectID": "schedule.html#choose-your-own-adventure",
    "href": "schedule.html#choose-your-own-adventure",
    "title": "The Schedule",
    "section": "",
    "text": "The course is structured in a “choose your own adventure” format, so there isn’t a set content for each week.\n\n\n\n\n\n\nImportant\n\n\n\nEach week, you will attend two classes:\n\nA one-hour seminar.\nA two-hour lab.\n\nNote that you are divided in two groups, so that the seminar and lab run twice each week, you just need to attend one of the seminars and one of the labs each week.\n\n\nThe seminars will have a different major theme each week, which you will have to research before class and discuss in groups in class.\nYou are also expected to learn quantitative methods outside of class, at your own time (as in a flipped classroom). This website has a list of entries in the Adventurer’s Notebook. You are not restricted to those posts, and you can learn from other resources, however if this is the very first time you are learning quantitative methods I recommend you stick to those since they have been curated for beginners. You can find a map of how the different posts build on each other in the Adventurer’s Map.\nThe labs will create the opportunity for you to practice quantitative data analysis with R. Each week you will pick a “challenge” from a list or come up with your own challenge and work towards solving it using R. More details will be shared soon.\nAs part of the Assessments, you will have to complete a group project. You can start working on the project any time during the course, but the last two labs will be dedicated to working on your project while being able to get help from the instructor (Stefano) and the Tutors."
  },
  {
    "objectID": "schedule.html#schedule-overview",
    "href": "schedule.html#schedule-overview",
    "title": "The Schedule",
    "section": "2 Schedule overview",
    "text": "2 Schedule overview\nThe following tables gives you an approximate schedule of the weekly themes, although they are subject to change also in light of your preferences. Check the week-by-week boxes below for a more detailed description of what you should be doing each week.\n\n\n\nWeek\nSeminar\nLab\n\n\n\n\n1\nWelcome and practicalities\nChallenge: Learn the basics of R\n\n\n2\nMore than numbers: quantitative methods and uncertainty\nChallenge\n\n\n3\nThe research process: research ethics and questionable practices\nChallenge\n\n\n4\nLanguage endangerment and small numbers\nChallenge\n\n\n5\nLanguage technology and machine learning\nChallenge: Statistical modelling\n\n\n6\nThe language of the climate crisis\nChallenge\n\n\n7\nEquality, diversity and inclusiveness in quantitative linguistics\nChallenge\n\n\n8\nQualitative vs quantitative: a real dichotomy?\nChallenge\n\n\n9\nOpen research and scholarship\nChallenge: Communicate your research\n\n\n10\nWhat’s next?\nGroup project\n\n\n11\nBonus topic\nGroup project"
  },
  {
    "objectID": "schedule.html#weekly-breakdown",
    "href": "schedule.html#weekly-breakdown",
    "title": "The Schedule",
    "section": "3 Weekly breakdown",
    "text": "3 Weekly breakdown\n\n\n\n\n\n\nWeek 1\n\n\n\n\n\nIn the seminar\n\nCome to the seminar class. We will go through the course format, assessment and other practicalities.\n\nIn your own time\n\nCarefully read the homepage of the Course website.\nComplete the Intake Form (the link to the form can be found on the Learn website).\nInstall R, RStudio and Quarto: please, follow the instructions in the Setup page. You should do this before the first lab class.\n\nIn the lab\n\nCome to the lab class. Learn about the basics of R. Optionally, if you need to freshen up your computer basics, you can check Basic computer literacy.\n\n\n\n\n\n\n\n\n\n\nWeek 2\n\n\n\n\n\nIn your own time\n\nRead Research methods and hodotics.\nWatch The Golem of Prague. And/or read the related chapter of Statistical Rethinking, The Golem of Prague.\nPick and read any entry/ies from the Notebook.\n\nIn the seminar\n\nGroup discussion: quantitative methods and uncertainty.\n\nIn the lab\n\nGroup activity: pick a Challenge."
  },
  {
    "objectID": "posts/stat-variables.html",
    "href": "posts/stat-variables.html",
    "title": "Statistical variables",
    "section": "",
    "text": "Pre-requisites\n\n\n\n\nR basics"
  },
  {
    "objectID": "posts/stat-variables.html#statistical-variables",
    "href": "posts/stat-variables.html#statistical-variables",
    "title": "Statistical variables",
    "section": "1 Statistical variables",
    "text": "1 Statistical variables\nA (statistical) variable is any characteristics, number, or quantity that can be measured or counted.\n\nWhen you observe or measure something, you are taking note of the values generated by the variable.\nIt’s called variable because it varies (ha!).\nThe opposite of a variable is a constant.\n\nSome examples:\n\nToken number of telic verbs and atelic verbs in written Sanskrit.\nVoice Onset Time of stops in Mapudungun.\nFriendliness ratings of synthetic speech.\n…\n\nTry and think of more!"
  },
  {
    "objectID": "posts/stat-variables.html#types-of-variables",
    "href": "posts/stat-variables.html#types-of-variables",
    "title": "Statistical variables",
    "section": "2 Types of variables",
    "text": "2 Types of variables\nYou will find that some statistics textbooks overcomplicate things when it come to types of variables. From an applied statistics perspective you only need to be able to identify numeric vs categorical variables and continuous vs discrete variables.\n\n2.1 Numeric vs categorical variables\n\nThis distinction is quite self-explanatory:\n\nNumeric variables are variables that are numbers.\nCategorical variables are variables that correspond to categories, groups or levels on a scale.\n\n\n\n\n\n\n\nExamples\n\n\n\nNumeric variables\n\nNumber of multi-verb predicates in a book.\nDuration of stressed vowels.\nRating score between 0-100.\n\nCategorical variables\n\nGender (non-binary, female, male, …).\nFirst vs second language users.\nEjective vs non-ejective consonant.\n\n\n\nLearning how to recognise variables is a fundamental skill in quantitative data analysis, since the type of variables determines the type of analyses you can carry out.\n\n\n2.2 Continuous vs discrete variables\n\nOrthogonal to the numeric/categorical distinction, there is the continuous vs discrete distinction.\nThis one can be at times less straightforward.\n\nA continuous variable is a variable that can take on any value between any two numbers.\nA discrete variable is a variable that can only take on a set of values, and no value in between.\n\nNumeric variables can be either be continuous or discrete, while categorical variables can only be discrete.\n\n\n\n\n\n\nNote\n\n\n\nNumeric continuous variable: between any two values there is an infinite number of values.\n\nThe variable can take on any positive and negative number, including 0.\nThe variable can take on any positive number only.\nProportions and percentages: The variable can take on any number between 0 and 1.\n\nNumeric discrete variable: between any two consecutive values there are no other values.\n\nCounts: The variable can take only on any positive integer number.\n\nCategorical (discrete) variable. There are three main subtypes.\n\nBinary or dichotomous: The variable can take only one of two values.\nThe variable can take any of three of more values (sometimes called a multinomial variable).\nOrdinal: The variable can take any of three of more values and the values have a natural order."
  },
  {
    "objectID": "posts/stat-variables.html#operationalisation",
    "href": "posts/stat-variables.html#operationalisation",
    "title": "Statistical variables",
    "section": "3 Operationalisation",
    "text": "3 Operationalisation\nThe action of operationalisation consists in choosing how to measure something: as a numeric or as a categorical variable.\nIn some cases, the choice is obvious, but in most cases something could be operationalised either way and it will be a matter of study design that helps you choose.\nFor example, age can be operationalised as years (numeric) or as age groups, like young vs old (categorical).\nThink of ways to operationalise the following variables:\n\nVoice Onset Time.\nFriendliness of speech.\nLexical frequency.\n\n\n\n\n\n\n\nNext\n\n\n\n\nWrangling data\nImport multiple files\nStatistical summaries\nIntroduction to plotting\nIntroduction to regression modelling"
  },
  {
    "objectID": "posts/research-context.html#empirical-research",
    "href": "posts/research-context.html#empirical-research",
    "title": "Empirical research and research context",
    "section": "1 Empirical research",
    "text": "1 Empirical research\nEmpirical research is one approach to research. This type of research focusses on learning about the Universe through data and observation.\n\n\n\n\n\n\nEmpirical research\n\n\n\nThe word empirical is related to experience, and in the context of research it basically means “based on experience (i.e. data and observation)”.\nLear more about the etymology of empirical here."
  },
  {
    "objectID": "posts/research-context.html#axes-of-research",
    "href": "posts/research-context.html#axes-of-research",
    "title": "Empirical research and research context",
    "section": "2 Axes of research",
    "text": "2 Axes of research\n\nThere are two main “axes” of empirical research types:\n\nExploratory vs corroboratory.\nDescriptive vs explanatory.\n\n\n\n\n\n\n\nExploratory vs corroboratory research\n\n\n\n\nExploratory research is about exploring the data looking for patterns, associations, features and so on. This type of research is also known as “hypothesis generating” because exploration can lead to the formulation of new hypotheses.\nCorroboratory research (aka as confirmatory research) is about checking expectations against data. It is also known as “hypothesis testing” because it is about testing hypotheses using data.\n\n\n\nWhile there is still a lot of prejudice against exploratory research (typical sentiments are “it doesn’t have theory”) it is an important way of doing research, as recognised by important scholars like Tukey. In We need both Exploratory and Confirmatory, Tukey stressed the importance of both approaches to research.\n\n\n\n\n\n\nDescriptive vs explanatory research\n\n\n\n\nDescriptive research is about describing facts through observation and collection of data. In other words, descriptive research is about the what.\nExplanatory research is about explaining facts, i.e. understanding why they are the way they are. In other words, explanatory research is about the why."
  },
  {
    "objectID": "posts/research-context.html#research-objectives",
    "href": "posts/research-context.html#research-objectives",
    "title": "Empirical research and research context",
    "section": "3 Research objectives",
    "text": "3 Research objectives\nThere can be three types of research objectives. Each has its merits and to improve our understanding of the Universe we need all three, although there is nothing wrong for any one study to focus just on one or two!\n\n\n\n\n\n\nEstablish facts\n\n\n\nResearch can establish facts and fill a gap in the knowledge of one or more phenomena.\nThe aim of establishing facts is to accumulate evidence of particular events, features, associations.\nExamples:\n\nWhat are the uses of the Sanskrit verb gam ‘to go’?\nWhat is the duration of vowels in Mawayana (Arawakan)?\nDo people interact with AI as with other people?\n\n\n\n\n\n\n\n\n\nImprove fit of framework to facts\n\n\n\nResearch can improve the fit of a specific framework to established facts. Usually this is done to fine-tune a framework in light of new evidence but it also just works when you want to test new expectations/hypotheses. When the facts do not match the expectations, researchers modify the framework to accommodate the results.\nIn some cases, a framework can be totally abandoned in light of the facts, or a new one could be developed.\nExamples:\n\nStrong exemplar-based models preclude the possibility of abstract representations, but certain categorisation tasks seem to involve abstract representations so these must be included in exemplar-based models.\n\n\n\n\n\n\n\n\n\nCompare fit of different frameworks to facts\n\n\n\nThis objective allows researcher two compare two or more frameworks in light of empirical results. The main prerequisite for this approach is that each framework must have different expectations in relation to the phenomenon at hand.\nWhen different frameworks entail different and exclusive hypotheses, one can test the hypotheses with data: the results might help excluding certain hypotheses and keep others. The frameworks that generate the excluded hypotheses have to be abandoned (unless they can be modified to fit the new results, see above, while still be different enough from other frameworks).\nExamples:\n\nThere are two possible models for the bilingual lexicon: Word association and concept mediation. Which one better describes and explains the data?\nA strict feed-forward architecture of grammar does not allow phonetic details to be sensitive to morphological structure, while some exemplar-based models allow that.\n\n\n\nEach of the three objectives are important in research, but note that in order to really advance our understanding of things the third objective is fundamental: it is only by directly comparing different frameworks that we can accumulate knowledge and weed out inaccurate explanations."
  },
  {
    "objectID": "posts/research-context.html#research-context",
    "href": "posts/research-context.html#research-context",
    "title": "Empirical research and research context",
    "section": "4 Research context",
    "text": "4 Research context\nEllis and Levy (2008) introduce a nice break-down of the concept of “research context”.\nThe image below is a schematic representation of different aspects of the research context, from the most general to the most specific. An example of each is also provided."
  },
  {
    "objectID": "posts/management.html",
    "href": "posts/management.html",
    "title": "Research project management",
    "section": "",
    "text": "Prerequisites\n\n\n\n\nResearch Methods and Hodotics"
  },
  {
    "objectID": "posts/management.html#managing-your-research",
    "href": "posts/management.html#managing-your-research",
    "title": "Research project management",
    "section": "1 Managing your research",
    "text": "1 Managing your research\n\n\n\n\n\nManaging a research project, whether it’s a solo project like a dissertation or it’s a large-scale project with many people, involves a lot of project management skills.\nThe three most important aspects of research project managements are:\n\nProject planning: anything related to planning and executing your research project, including managing other team members.\nResearch management: managing references, papers, research notes and more.\nFile management: probably the most neglected aspect of research project management, managing your files, from collected data to images to written work.\n\nTo learn more about the three aspects of research project management, check the “Next” box below.\n\n\n\n\n\n\nNext\n\n\n\n\nFile management"
  },
  {
    "objectID": "posts/hodotics.html",
    "href": "posts/hodotics.html",
    "title": "Research Methods and Hodotics",
    "section": "",
    "text": "Research Methods are about the theory, methods and practice of conducting research. I like to call the discipline that deals with research methods Hodotics.\nOne way of categorising different aspects of research methods is represented in the following figure.\n\n\n\n\n\nYou can think of research methods as the combination of:\n\nThe research process: the process of conducting a research project, from determining the research context to communicating results.\nProject management: the process of managing a research project, from project planning to writing-up.\nDigital skills: all research involves using computers (at least at some point if not throughout) so that computer literacy and digital skills are nowadays a fundamental aspect of research.\nPhilosophy: all research is not performed in a vacuum and a lot of philosophical questions shape the entire research process.\nEthics: all research is not performed in a social vacuum and ethical considerations are a fundamental aspect of research."
  },
  {
    "objectID": "posts/hodotics.html#research-methods",
    "href": "posts/hodotics.html#research-methods",
    "title": "Research Methods and Hodotics",
    "section": "",
    "text": "Research Methods are about the theory, methods and practice of conducting research. I like to call the discipline that deals with research methods Hodotics.\nOne way of categorising different aspects of research methods is represented in the following figure.\n\n\n\n\n\nYou can think of research methods as the combination of:\n\nThe research process: the process of conducting a research project, from determining the research context to communicating results.\nProject management: the process of managing a research project, from project planning to writing-up.\nDigital skills: all research involves using computers (at least at some point if not throughout) so that computer literacy and digital skills are nowadays a fundamental aspect of research.\nPhilosophy: all research is not performed in a vacuum and a lot of philosophical questions shape the entire research process.\nEthics: all research is not performed in a social vacuum and ethical considerations are a fundamental aspect of research."
  },
  {
    "objectID": "posts/hodotics.html#the-research-process",
    "href": "posts/hodotics.html#the-research-process",
    "title": "Research Methods and Hodotics",
    "section": "2 The research process",
    "text": "2 The research process\nLet’s zoom in on the research process.\n\n\n\n\n\n\nContext: the research context includes several aspects of the research process, including the background (i.e. the previous literature and current knowledge) and the rational of the study (from the general topic to specific research questions/hypotheses).\nData acquisition: this is the process of gathering data to be used in the study. Data acquisition covers many different types of processes, from experimental set-ups to corpus queries.\nData analysis is the process of analysing the acquired data using qualitative, quantitative or a mixed methods. This part of the research process also include interpreting the output of such analysis.\nCommunication: finally, the last step in a research process cycle is to communicate what was done and what was learned, both to the research community and to the wider public.\n\n\n\n\n\n\n\nNext\n\n\n\n\nEmpirical research and research context\nQuantitative data analysis\nResearch project management"
  },
  {
    "objectID": "posts/data-analysis.html",
    "href": "posts/data-analysis.html",
    "title": "Quantitative data analysis",
    "section": "",
    "text": "Prerequisites\n\n\n\n\nResearch Methods and Hodotics"
  },
  {
    "objectID": "posts/data-analysis.html#data-analysis",
    "href": "posts/data-analysis.html#data-analysis",
    "title": "Quantitative data analysis",
    "section": "1 Data analysis",
    "text": "1 Data analysis\n\nData analysis is anything that relates to analysing data, whether you collected it yourself or you used pre-existing data.\nThere are two main approaches to data analysis:\n\nQuantitative data analysis is about learning from measured data. Data can be operationalised in many different ways and these determine the type of analyses you can apply.\nQualitative data analysis is about learning from the features and characteristics of the data.\n\n\nNote that while it is common to talk about “quantitative vs qualitative data” in fact in most cases data can be conceived as both quantitative and qualitative. It is really how we approach the data that can be quantitative and/or qualitative."
  },
  {
    "objectID": "posts/data-analysis.html#quantitative-data-analysis",
    "href": "posts/data-analysis.html#quantitative-data-analysis",
    "title": "Quantitative data analysis",
    "section": "2 Quantitative data analysis",
    "text": "2 Quantitative data analysis\n\nQuantitative analyses are usually comprised of three parts (these are not strictly distinct and the boundaries are sometimes blurred):\n\nSummarise data with summary measures.\nVisualise data with plots.\nModel data with statistical models."
  },
  {
    "objectID": "posts/data-analysis.html#numbers-have-no-meaning",
    "href": "posts/data-analysis.html#numbers-have-no-meaning",
    "title": "Quantitative data analysis",
    "section": "3 Numbers have no meaning",
    "text": "3 Numbers have no meaning\nBoth qualitative and quantitative approaches are valid and necessary to improve our understanding of things. Moreover, even a very complex quantitative analysis will always contain some qualitative aspects.\n\n\n\n\n\n\nImportant\n\n\n\nThe numbers have no way of speaking for themselves. We speak for them. We imbue them with meaning.\n— Nate Silver, The Signal and the Noise\n\n\nThere’s a lot of wisdom in that quote. Numbers do not mean anything by themselves. We need to interpret numbers, “imbue them with meaning”, based on many aspects of research and beyond, including our own identity and positionality.\n\n\n\n\n\n\nNext\n\n\n\n\nInference and uncertainty\nWhat is statistics?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative Methods for LEL",
    "section": "",
    "text": "Welcome to the main site of the course Quantitative Methods for Linguistics and English Language (Semester 1).\nThis website is your go-to place throughout the semester for any info related to the course."
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Quantitative Methods for LEL",
    "section": "1 Course description",
    "text": "1 Course description\n\n\n\n\n\n\nThis course is an introduction to quantitative data analysis (including data wrangling, visualisation and modelling) as commonly employed in linguistics, using the R software.\nThis course has a “choose your own adventure” format. Learn more in Schedule.\nAmong the topics that you can learn there are the following:\n\nThe basics of quantitative data analysis.\nData preparation.\nData summaries.\nPrinciples of data visualisation.\nStatistical modelling with linear models.\nStatistical inference using Bayesian inference (posterior probability distributions and credible intervals) and frequentist inference (p-values and confidence intervals).\n\nAt completion of the course you will have gained the following skills:\n\nImport common data formats, tidy and transform data.\nChoosing and reporting appropriate summary measures.\nUsing compelling visualisations to communicate a specific message about patterns in the data.\nMaster linear models for different types of data (continuous measures and binary outcomes).\nUsing Bayesian inference to answer research questions and avoid common interpretation pitfalls of frequentist techniques.\n\nExamples from different branches of linguistics will be used to provide you with hands-on experience in quantitative data analysis and Open Research practices."
  },
  {
    "objectID": "index.html#course-rationale",
    "href": "index.html#course-rationale",
    "title": "Quantitative Methods for LEL",
    "section": "2 Course rationale",
    "text": "2 Course rationale\n\n\n\n\n\n\nThis course is designed to help you develop the necessary skills for conducting and interpreting analyses of data as commonly employed in linguistics.\nThe content and objectives of the course are in response to recent advances in our understanding of the theory behind research methods.\nRecent meta-scientific research has identified three important aspects of research: the reproducibility, replicability and generalisability.\n\nA result is reproducible when the same analysis steps performed on the same dataset consistently produces the same answer.\nA result is replicable when the same analysis performed on different datasets produces qualitatively similar answers.\nA result is generalisable when a different analysis workflow performed on different data sets produces qualitatively similar answers.\n\nSee Definitions for a more detailed explanation.\nHowever, based on surveys from different disciplines, we are currently facing the three research crises (reproducibility, replicability and generalisability crises) by which most results are neither reproducible, nor replicable, nor generalisable (Munafò et al. 2017, Simmons et al. 2011, Ioannidis 2005, Yarkoni 2022).\nThe Open Research movement (also known as Open Science or Open Scholarship, Crüwell et al. 2019) was developed with the aim of improving our understanding of these crises and with the objective of providing researchers with guidelines and tools to produce reproducible, replicable and generalisable research.\nThe statistical philosophy adopted in the course is that of the New Statistics (Cumming 2014, Kruschke and Liddell 2018. The main goal of the New Statistics is to shift the attention from statistical significance to estimation with quantification of uncertainty."
  },
  {
    "objectID": "index.html#support",
    "href": "index.html#support",
    "title": "Quantitative Methods for LEL",
    "section": "3 Support",
    "text": "3 Support\n\n\n\n\n\n\nAsk for help\n\n\n\nIf at any point during the course you don’t feel comfortable with any aspect of the course, you are unsure about anything that has been covered in class or in your own time, you are struggling to keep up with the course workload, you are experiencing mental of physical distress due to a pre-existing or new illness, medical condition or disability, or you find yourself unable to access basic needs like food or housing, please do get in touch with me and/or the PPLS support (go to the PPLS UG or MSc Hub on SharePoint &gt; Support for students &gt; Health & Wellbeing).\nWe are humans first and the rapidly-changing new world we are living in now can put us under pressure. What is most important to me is that you are first and foremost healthy and able to participate to the course, and that you succeed and get the most out of the course.\nIt is OK not to be OK, and remember that you are not alone. Other people, teachers and students, might be struggling right now or have struggled before and might have gone through what you are going through now. Remember that support exists for you, so please do reach out. If you see somebody close to you struggling, please let them know about the available support network and encourage them to reach out."
  },
  {
    "objectID": "index.html#contacts",
    "href": "index.html#contacts",
    "title": "Quantitative Methods for LEL",
    "section": "4 Contacts",
    "text": "4 Contacts\n\n\n\n\n\n\nYou can reach me (Stefano) at s.coretta@ed.ac.uk or on Teams.\nIf you want to book office hours with me, you can do so here: https://bit.ly/33BH84L.\nHere’s the list of course tutors:\n\nAlice Marikan\nAnna Laoide-Kemp\nCarine Abraham\nGeorges Sakr\nOdysseas Myresiotis Alivertis\nPonrawee Prasertsom"
  },
  {
    "objectID": "challenges/language-diversity.html",
    "href": "challenges/language-diversity.html",
    "title": "Language endangerment and linguistic diversity",
    "section": "",
    "text": "Linguistic diversity is a feature of specific geographic areas which tries to quantify how “diverse” that particular area is linguistically. You can think of linguistic diversity as “linguistic density”.\nThe majority of the world’s languages are endangered, which means that they will disappear while being replaced by other languages (usually, historically colonising languages).\nAn interesting question is whether areas that are more diverse are also those areas with the greatest proportion of endangered languages.\nYou can try and answer that question using the Glottolog data: coretta2022/glot_status.rds."
  },
  {
    "objectID": "challenges/chatgpt-sentiment.html",
    "href": "challenges/chatgpt-sentiment.html",
    "title": "Sentiment analysis of Tweets on ChatGPT",
    "section": "",
    "text": "Kaggle is a great resource for finding data sets that can be useful in Natural Language Processing.\nThe ChatGPT sentiment analysis data set contains Tweets about ChatGPT, coded by a sentiment analysis.\nHow does ChatGPT fare in these Tweets?"
  },
  {
    "objectID": "content.html",
    "href": "content.html",
    "title": "QML",
    "section": "",
    "text": "Week\nTopic\n\n\n\n\n\n\n\n\n1\nQuantitative methods and uncertainty\nSlides\nTutorial\n\n\n\n\n2\nData wrangling\nSlides\nTutorial\n\n\n\n\n3\nData visualisation\nSlides\nTutorial\n\n\n\n\n4\nStatistical modeling basics\nSlides\nTutorial\n\n\n\n\n5\nCategorical predictors\nSlides\nTutorial\nF1\n\n\n\n6 Catch up\nNo classes\n\n\n\n\n\n\n7\nBinary outcomes\nSlides\nTutorial\n\n\n\n\n8\nMultiple predictors and interactions\nSlides\nTutorial\n\nS1\n\n\n9\nContinuous predictors\nSlides\nTutorial\n\n\n\n\n10\nResearch process: an overview\nSlides\nTutorial\nF2\n\n\n\n11\nObtaining p-values (optional)\nSlides\nTutorial\n\n\n\n\n12\n\n\n\n\nS2"
  },
  {
    "objectID": "content.html#schedule-overview",
    "href": "content.html#schedule-overview",
    "title": "QML",
    "section": "",
    "text": "Week\nTopic\n\n\n\n\n\n\n\n\n1\nQuantitative methods and uncertainty\nSlides\nTutorial\n\n\n\n\n2\nData wrangling\nSlides\nTutorial\n\n\n\n\n3\nData visualisation\nSlides\nTutorial\n\n\n\n\n4\nStatistical modeling basics\nSlides\nTutorial\n\n\n\n\n5\nCategorical predictors\nSlides\nTutorial\nF1\n\n\n\n6 Catch up\nNo classes\n\n\n\n\n\n\n7\nBinary outcomes\nSlides\nTutorial\n\n\n\n\n8\nMultiple predictors and interactions\nSlides\nTutorial\n\nS1\n\n\n9\nContinuous predictors\nSlides\nTutorial\n\n\n\n\n10\nResearch process: an overview\nSlides\nTutorial\nF2\n\n\n\n11\nObtaining p-values (optional)\nSlides\nTutorial\n\n\n\n\n12\n\n\n\n\nS2"
  },
  {
    "objectID": "content.html#weekly-schedule",
    "href": "content.html#weekly-schedule",
    "title": "QML",
    "section": "2 Weekly schedule",
    "text": "2 Weekly schedule\n\n2.1 Week 1: Quantitative methods and uncertainty\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nQuestions\n\nWhat is quantitative data analysis?\nWhat is the inference process?\nHow can we talk about uncertainty and variability?\nWhich are the limits of quantitative methods?\n\nSkills\n\nThink critically about statistics, uncertainty and variability.\nUse R to perform simple calculations.\nMaster the basics of the programming language R.\nUse RStudio.\n\n\n\n\n\n\n\n\n\n\nHomework\n\n\n\n\n\nCourse website\n\nCarefully read the homepage.\nFamiliarise yourself with this Course content page (note that the materials will be updated throughout the course).\n\nIntake form\n\nYou must complete the intake form before coming to the Tuesday lecture.\nThe link to the form can be found on the Learn website.\n\nInstall R and RStudio\n\nFor this course, you need to install both R and RStudio.\nNOTE: If you have installed either R or RStudio prior to January 2023, please make sure you delete both R and RStudio from your laptop.\nPlease, follow the instructions in the Setup page.\n\n\n\n\n\n\n\n\n\n\nMaterials\n\n\n\n\n\n\nLecture slides.\nWorkshop tutorial.\n\n\n\n\n\n\n\n\n\n\nSuggested readings\n\n\n\n\n\nMain textbooks\n\nStatistics for Linguists with R, by Bodo Winter (S4LR) Ch. 1. [via library]\nR for Data Science (R4DS) Ch. 1, Ch. 2. [online book]\nStatistical (Re)thinking, by Richard McElreath (SReT), Ch. 1. [via library]\n\nFrom the lecture\n\nEllis and Levy 2008. Framework of Problem-Based Research: A Guide for Novice Researchers on the Development of a Research-Worthy Problem\nSilberzahn et al. 2018. Many Analysts, One Data Set: Making Transparent How Variations in Analytic Choices Affect Results\nCoretta et al. 2023. Multidimensional signals and analytic flexibility: Estimating degrees of freedom in human speech analyses\nCumming 2014. The New Statistics: Why and How\nKurschke and Liddell 2018. The Bayesian New Statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective\n\nReplication\n\n👉 Assessing the replication landscape in experimental linguistics.\nThe Stark realities of reproducible statistically orientated sociological research: Some newer rules of the sociological method.\n\nOther\n\nMethods as theory.\nMolnar 2022. Modeling Mindsets: The many cultures of learning from data.\nDarwin Holmes 2020. Researcher Positionality - A Consideration of Its Influence and Place in Qualitative Research - A New Researcher Guide\nJafar 2018. What is positionality and should it be expressed in quantitative studies?\n\n\n\n\n\n\n2.2 Week 2: Data wrangling\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nQuestions\n\nWhat are the types of statistical variables?\nWhich summary measures are appropriate for which types of variables?\nWhat are common measures central tendency?\nWhat are common measures of dispersion?\n\nSkills\n\nOrganise files efficiently.\nImport tabular data in R.\nObtain mean, median, mode, range and standard deviation.\nUse R scripts to save and reuse code.\n\n\n\n\n\n\n\n\n\n\nMaterials\n\n\n\n\n\n\nLecture slides.\nWorkshop tutorial.\nWorkshop files (right-click and download):\n\nshallow.csv\n\n\n\n\n\n\n\n\n\n\n\nSuggested readings\n\n\n\n\n\nMain textbooks\n\nS4LR Ch. 3. [via library]\nR4DS Ch. 3 and Ch. 4. [online book]\n\n\n\n\n\n\n2.3 Week 3: Data visualisation\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nQuestions\n\nWhat are the principles of good data visualisation?\nWhich are the main components of a plot?\nWhich are the appropriate plots for different types of data?\nHow can we visualise uncertainty?\n\nSkills\n\nCreate common types of plots with ggplot2.\nUse colour and shape to effectively convey meaning.\nDescribe a plot in writing and comment on observable patterns.\nCreate styled HTML reports.\n\n\n\n\n\n\n\n\n\n\nMaterials\n\n\n\n\n\n\nLecture slides.\nWorkshop tutorial.\nWorkshop files (right-click and download):\n\npolite.csv\nglot_status.rds\n\n\n\n\n\n\n\n\n\n\n\nSuggested readings\n\n\n\n\n\nMain textbooks\n\nR4DS Ch. 2. [online book]\nggplot2 documentation.\n\nFrom the lecture\n\nSpiegelhalter 2020. The Art of Statistics: Learning from Data.\n\nOther\n\nGabry et al 2019. Visualization in Bayesian workflow.\nPolitzer-Ahles and Piccini. On visualizing phonetic data from repeated measures experiments with multiple random effects.\nFundamentals of Data Visualisation.\nData viz catalogues\n\nDirectory of visualisations\nData viz catalogue\nData Viz project\nTop 50\nData Viz\n\nTutorials\n\nRaincloud plots\nLabels\nGraphic design\n\nColour\n\nColorBrewer2.\nMetBrewer\nUse colour wisely.\n\nCaveats\n\nSame stats different data.\nBehind bars.\nIssues with error bars.\nI’ve stopped using boxplots.\n\n\n\n\n\n\n\n2.4 Week 4: Statistical modeling basics\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nQuestions\n\nWhat are probability distributions?\nHow can we describe probability distributions with statistical parameters?\nWhat are the frequentist and Bayesian view of statistical parameters?\nHow can we estimate parameters using statistical models?\n\nSkills\n\nTransform data by creating new columns (mutate) and filtering based on specific values (filter).\nUse logical operators to transform data.\nFit a statistical model to estimate the mean and standard deviation of a Gaussian variable with brm().\nInterpret the summary of the model and understand the meaning of the reported estimates.\n\n\n\n\n\n\n\n\n\n\nMaterials\n\n\n\n\n\n\n[optional] The Golem of Prague (video lecture of SreT Ch 1).\nLecture slides.\nWorkshop tutorial.\nWorkshop files (right-click and download):\n\nalb_vot.csv\n\n\n\n\n\n\n\n\n\n\n\nSuggested readings\n\n\n\n\n\nMain textbooks\n\nR4DS Ch. 2. [online book]\nggplot2 documentation.\nS4LR Ch 3. [via library]\nSReT Ch 2, sparingly (we have not covered everything in the chapter yet). [via library]\n\nOther\nThe following resources will be helpful throughout the course. Note they cover aspects that we have not yet discussed (some will be in the following weeks, others won’t be due to time), but do bookmark these because they will be valuable when you will be working on your dissertation.\n\nLinear Models and Mixed Models with R tutorials (1 and 2) by Bodo Winter (author of S4LR) for a general overview of the type of models we focus on in this course.\nOne Thousand and One names: table with naming conventions for different types of linear models.\nLinear Models: A cheat-sheet: use this to find out which building blocks you need for your linear model.\n\n\n\n\n\n\n2.5 Week 5: Categorical predictors\n\n\n\n\n\n\nFormative assessment 1\n\n\n\n\nDUE on Thu 19 October at noon.\nFormative assessment 1 requires you to complete a few guided exercises of the type that will be included in Summative 1.\nFind instructions and data here: https://github.com/uoelel/qml-f1\n\n\n\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nQuestions\n\nHow do we model variables using categorical predictors?\nWhich are the most common coding systems for categorical predictors?\nHow do we interpret the model output when there are categorical predictors?\nHow can we quickly check model goodness?\n\nSkills\n\nMaster contrast coding in R for categorical predictors.\nUnderstand treatment coding.\nFit, interpret and plot models with a categorical predictor.\nReporting of model specification and results.\n\n\n\n\n\n\n\n\n\n\nMaterials\n\n\n\n\n\n\nLecture slides.\nWorkshop tutorial.\nWorkshop files (right-click and download):\n\nsenses_valence.csv\n\n\n\n\n\n\n\n\n\n\n\nSuggested readings\n\n\n\n\n\nMain textbooks\n\nR4DS Ch. 17. [online book]\nS4LR Ch 7. [via library]\nSReT Sec 5.3. [via library]\n\nOther\n\nFactors, coding and contrasts: blog post on factors in linear models. It also discusses interactions, which we will cover in Weeks 8-9.\n\n\n\n\n\n\n2.6 Week 6: Catch-up Week\n\n\n\n\n\n\nHomework\n\n\n\n\n\nThere is no homework as such, so take the time to revise the materials and/or catch up with the previous weeks’ materials.\nThere will be no classes.\n\n\n\n\n\n2.7 Week 7: Binary outcomes\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nQuestions\n\nHow can we visualise proportions of binary outcomes (yes/no, correct/incorrect, …)?\nWhich distribution do binary outcomes follow?\nWhat is the relationship between probabilities and log-odds?\nHow do we interpret log-odds and odds?\n\nSkills\n\nPlot binary data as proportions in ggplot2.\nPivot data from wide to long with tidyr.\nFit, interpret and plot linear models with binary outcome variables, using the Bernoulli distribution family.\nConvert between log-odds, odds and probabilities.\n\n\n\n\n\n\n\n\n\n\nMaterials\n\n\n\n\n\n\nLecture slides.\nWorkshop tutorial.\nWorkshop files (right-click and download):\n\ntakete_maluma.txt.\n\n\n\n\n\n\n\n\n\n\n\nSuggested readings\n\n\n\n\n\nMain textbooks\n\nR4DS Ch. 6. [online book]\nS4LR Ch 12. [via library]\nSReT Ch 11. [via library]\n\n\n\n\n\n\n2.8 Week 8: Multiple predictors and interactions\n\n\n\n\n\n\nSummative 1: Week 8 (Thu 9 November at noon)\n\n\n\nDue on Thursday 9 November at noon\nThe first summative contains a series of guided exercises that cover things done in Weeks 1 to 7.\nYou can find the instructions and data for the first summative here: https://github.com/uoelel/qml-s1/.\n\n\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nQuestions\n\nWhat is a factorial design?\nHow do we estimate and interpret the effects of multiple predictors?\nHow do we deal with situations when one predictor’s effect is different, depending on the value of the other predictor?\nHow can such interactions between predictors be built into our models?\nHow do we interpret model estimates of interactions?\n\nSkills\n\nRun and interpret models with multiple predictors.\nInterpret interactions between two predictors.\nPlot posterior and conditional probabilities from models with interactions.\nPractice transforming and back-transforming variables.\n\n\n\n\n\n\n\n\n\n\nMaterials\n\n\n\n\n\n\nLecture slides.\nWorkshop tutorial.\nWorkshop files (right-click and download):\n\nshallow.csv.\ndur-ita-pol.csv.\n\n\n\n\n\n\n\n2.9 Week 9: Continuous predictors and interactions\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\n\nQuestions\n\nHow do we model predictors that aren’t categorical, but continuous?\nHow do we interpret model estimates for continuous predictors?\nHow do we fit and interpret interactions involving continuous predictors?\n\nSkills\n\nCentre continuous predictors.\nRun and interpret models with continuous predictors.\nInterpret interactions that are categorical * continuous (in the lecture) and continuous * continuous (in the tutorial).\n\n\n\n\n\n\n\n\n\n\nMaterials\n\n\n\n\n\n\nLecture slides.\nWorkshop tutorial.\nWorkshop files (right-click and download):\n\nsi.csv.\n\n\n\n\n\n\n\n2.10 Week 10: Research process - An overview\n\n\n\n\n\n\nFormative assessment 2\n\n\n\n\n\n\nDUE on Thursday 23 November at noon.\nF2 requires you to read, plot and model data. Summative 2 will have the same format.\n\n\n\n\n\n\n\n\n\n\nHomework\n\n\n\n\n\nPlease, read the following before coming to class on Wednesday (there will be no lecture on Tuesday).\n\nStroop Effect in Language\nHalf a Century of Research on the Stroop Effect: An Integrative Review, pp 163–165 plus one extra section of your choice.\n\n\n\n\n\n\n\n\n\n\nMaterials\n\n\n\n\n\n\nLecture slides.\nWorkshop tutorial.\nWorkshop files: https://github.com/uoelel/qml-stroop.\n\n\n\n\n\n\n\n\n\n\nUseful resources\n\n\n\n\n\nThe following resources are a useful summaries of conceptual, practical and terminological aspects of linear models in general.\n\nLinear Models Illustrated: a Shiny web app that illustrates linear models. Especially helpful to understand interactions.\nOne Thousand and One names: table with naming conventions for different types of linear models.\nLinear Models: A cheat-sheet: use this to find out which building blocks you need for your linear model.\n\n\n\n\n\n\n2.11 Week 11: Obtaining p-values (Optional)\n\n\n\n\n\n\nMaterials\n\n\n\n\n\n\nLecture slides.\nWorkshop tutorial.\n\n\n\n\n\n\n\n\n\n\nSuggested readings\n\n\n\n\n\n\nMotulsky 2014, Common misconceptions about data analysis and statistics.\nTressoldi et al 2015, The pervasive avoidance of prospective statistical power: Major consequences and practical solutions.\nCassidy et al 2019, Failing grade: 89 per-cent of introduction to psychology textbooks that define/explain statistical significance do so incorrectly.\nGigerenzer 2004, Mindless statistics.\nWagenmakers 2007. A practical solution to the pervasive problems of p-values.\nInterpreting (frequentist) Confidence Intervals.\n\n\n\n\n\n\n2.12 Week 12\n\n\n\n\n\n\nSummative 2: Week 12 (Thu 7 December at noon)\n\n\n\nDue on Thursday 7 December at noon\nIn the second summative assessment, you will:\n\nSelect a dataset from a list and its associated research questions.\nAnalyse the data using one linear model.\nWrite a report about the data, the model, and your findings.\n\nYou can find the instructions and data for the first summative here: https://github.com/uoelel/qml-s2/."
  },
  {
    "objectID": "assessments.html",
    "href": "assessments.html",
    "title": "QML",
    "section": "",
    "text": "This course will be self-assessed on the basis of a portfolio consisting of a Reflective Journal and a Group Project (including a Final Reflection).1"
  },
  {
    "objectID": "assessments.html#assessment-overview",
    "href": "assessments.html#assessment-overview",
    "title": "QML",
    "section": "",
    "text": "This course will be self-assessed on the basis of a portfolio consisting of a Reflective Journal and a Group Project (including a Final Reflection).1"
  },
  {
    "objectID": "assessments.html#reflective-journal",
    "href": "assessments.html#reflective-journal",
    "title": "QML",
    "section": "2 Reflective Journal",
    "text": "2 Reflective Journal\n\n\n\n\n\n\nDeadlines\n\n\n\n\nYou should post your self-reflection each week after you have attended your classes.\nThere isn’t a hard deadline and if necessary you can skip a week and post two reflections the week after.\n\n\n\nLearning is an active process: it happens best when we’re all engaged. Learning is also something that we can learn how to do better. Think of a hobby you might have or a skill you’ve learned: you’ve probably perfected it for a while, paying close attention to what did or didn’t work.\nThat’s why both students and the lecturer (Stefano) will reflect on how the course is going and what they’ve learned. After each class we’ll all write short self-reflections in the Reflective Journal. I’ll also comment on yours and you’ll be able to comment on mine.\nEach of us will have our own Reflective Journal on Learn. The tutors and I will be able to see your Reflective Journal, but you won’t be able to see other students’ Journals. My Journal (the lecturer’s) will be visible to all and you will also be able to comment on it.\n\n2.1 Weekly self-reflections\nAfter every class you’ll need to write one self-reflection in the Reflective Journal on Learn. Afterwards you need to read my feedback on your entries.\nWhat happens is:\n\nWe have class.\nWe reflect on what we’ve learned and what our expectations are for next class.\nWe write down brief reflections (1-paragraph) in one entry in the Reflective Journal on Learn, e.g. one titled “Week 1 Seminar”, “Week 1 Lab”, and so on.\nI comment on your reflections, and you might comment on mine.\nI might also give general feedback in a collaborative discussion in class or via Learn announcements.\n\nIn sum: after any given class, you’ll write a short self-reflection. Within a reasonable amount of time, I’ll react to these."
  },
  {
    "objectID": "assessments.html#group-project-and-final-reflection",
    "href": "assessments.html#group-project-and-final-reflection",
    "title": "QML",
    "section": "3 Group project and Final Reflection",
    "text": "3 Group project and Final Reflection\n\n\n\n\n\n\nDeadline\n\n\n\n\nYou must submit your Final Reflection, including the product of your Group Project (see details below) by Thursday 12 December at noon on Learn &gt; Assessment.\n\n\n\nAs part of the assessment you will have to work on a Group Project.\nThis project can be anything (some ideas below) so the structure of what you will have to submit differs depending on what you end up doing.\nIndependent of the type of project, you will have to write a Final Reflection on your experience with the course and with working on you Group Project. This reflection is what you submit to Learn by the deadline. The reflection should contain the “product” of your Group Project, either as text following the reflection or as a link (since the Group Project can be anything).\nSome ideas for Group Projects:\n\nPick a linguistic question that interests you and run a mini quantitative study (this could well be a study where you also act as participants).\nFind a linguistic topic and collect published effects from the relevant literature (the first step towards a full meta-analysis).\nDesign exercises and/or tutorials for the course using linguistic data.\nWrite a collection of sonnets on the replicability crisis.\nWrite an essay on minoritised statisticians.\nChoreograph an interpretative dance that illustrates that Markov Chain Monte Carlo algorithm."
  },
  {
    "objectID": "assessments.html#feedback-and-marking",
    "href": "assessments.html#feedback-and-marking",
    "title": "QML",
    "section": "4 Feedback and marking",
    "text": "4 Feedback and marking\nFeedback will be provided to you (1) during class, (2) as comments on your weekly self-reflections and (3) during office hours (it is up to you to book meetings with me and/or the tutors).\nThis course is self-assessed with co-moderation, which means that at the end of the course you will assign yourself a mark and the mark will be moderated in conversation between you and the Lecturer (Stefano)/Tutors. The conversation will happen either in a meeting or via email:\n\nIf you would like to have a conversation, I will arrange 15-minute meetings the week of December 16.\nIf you don’t want to have a conversation, me or the Tutors will get in touch with you via email only in the case that adjustments to the mark have to be made (which in the experience of other colleagues using this approach happens quite rarely!).\n\n\n\n\n\n\n\nSelf-assessment with co-moderation\n\n\n\n\nBased on your learning, you assign yourself a mark (see section on impression-based step marking) when submitting the Final Reflection (by December 12).\nThe Course Organiser/Tutors will moderate the self-assigned mark as part of a dialogue with you.\n\n\n\n\n4.1 Impression-based step-marking\nTo help you thinking about which mark to assign yourself, we will use an “impression-based step-marking” procedure. The procedure works as follows:\n\nFirst, think about the mark band you think you deserve.\nThen, choose a step within the band (see below for an explanation).\n\nOnce you pick a mark band, you need to pick a step within that band. You can find a description of the bands here: Extended Common Marking Scheme. Within each mark band, there are three steps: 2, 5 or 8 (for example, in the 60s band there is 62, 65 or 68). The criteria for each of 2, 5, or 8 are the following:\n\n\n\n\n\n\n\nStep\nCriteria\n\n\n\n\n2\nThe criteria for the mark band have been achieved, but there might be minor issues.\n\n\n5\nThe criteria for the mark band have been achieved satisfactorily.\n\n\n8\nThe criteria for the mark band have been achieved fully."
  },
  {
    "objectID": "assessments.html#footnotes",
    "href": "assessments.html#footnotes",
    "title": "QML",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSome of the text on this page is from Itamar Kastner’s Morphology course site.↩︎"
  },
  {
    "objectID": "challenges.html",
    "href": "challenges.html",
    "title": "The Adventurer’s Notebook",
    "section": "",
    "text": "Sentiment analysis of Tweets on ChatGPT\n\n\n\n\n\n\nStefano Coretta\n\n\n\n\n\n\n\n\n\n\n\n\nIs ‘gay speech’ a thing?\n\n\n\n\n\n\nStefano Coretta\n\n\n\n\n\n\n\n\n\n\n\n\nLanguage endangerment and linguistic diversity\n\n\n\n\n\n\nStefano Coretta\n\n\n\n\n\n\n\n\n\n\n\n\nMorphology in Scottish Gaelic\n\n\n\n\n\n\nStefano Coretta\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "challenges/gay-speech.html",
    "href": "challenges/gay-speech.html",
    "title": "Is ‘gay speech’ a thing?",
    "section": "",
    "text": "A topic of debate, but still perhaps an understudied topic, is whether it exists a type of speech that specifically indexes sexuality and if so what its features are.\nA classic study is Sounding Gay: Pitch Properties in the Speech of Gay and Straight Men by Rudolf Gaudio (https://doi.org/10.2307/455948). There are many more studies that looked at the subject, spanning several features from sibilants to prosody to lexical choices.\nFor this challenge, you can scout the literature for quantitative results and maybe try and replicate the analysis of an existing study."
  },
  {
    "objectID": "challenges/morpho-gaelic.html",
    "href": "challenges/morpho-gaelic.html",
    "title": "Morphology in Scottish Gaelic",
    "section": "",
    "text": "This data set on the morphology of Scottish Gaelic contains data from a linguistic questionnaire on Scottish Gaelic morphology, from several locations.\nCan you find a pattern in morphological variation based on geographic areas?"
  },
  {
    "objectID": "notebook.html",
    "href": "notebook.html",
    "title": "The Adventurer’s Notebook",
    "section": "",
    "text": "Basic computer literacy\n\n\n\n\n\n\nStefano Coretta\n\n\nJun 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nData types\n\n\nLearn about types of data and how to import data in R\n\n\n\nStefano Coretta\n\n\nJun 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical variables\n\n\nLearn about the type of statistical variables\n\n\n\nStefano Coretta\n\n\nSep 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical summaries\n\n\nLearn about descriptive summary measures\n\n\n\nStefano Coretta\n\n\nSep 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nResearch Methods and Hodotics\n\n\nResearch Methods are about the theory, methods and practice of conducting research\n\n\n\nStefano Coretta\n\n\nJun 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to regression models\n\n\nLearn the basics of regression models, a powerful and flexible, yet simple, way of modelling data\n\n\n\nStefano Coretta\n\n\nJun 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nR basics\n\n\nLearn the basics of R, a statistical programming language.\n\n\n\nStefano Coretta\n\n\nJun 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nR Troubleshooting\n\n\nLearn about common R mistakes and where to find help\n\n\n\nStefano Coretta\n\n\nJun 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nRegression: Indexing of categorical predictors\n\n\nLearn about indexing of categorical predictors in regression models\n\n\n\nStefano Coretta\n\n\nJun 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Quarto\n\n\nLearn how to write dynamic documents with both text and code using Quarto\n\n\n\nStefano Coretta\n\n\nJun 27, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting basics\n\n\nLearn the basics of plotting in R with ggplot2\n\n\n\nStefano Coretta\n\n\nJun 27, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nReading multiple data files\n\n\n\n\n\n\n\n\n\nJul 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWrangling data with R\n\n\nLearn the basics of wrangling data with R\n\n\n\nStefano Coretta\n\n\nJul 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPivoting\n\n\nLearn how to change the format of data tables from long to wide and viceversa using the pivot functions from tidyr\n\n\n\nStefano Coretta\n\n\nJul 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMutate data\n\n\nLearn about mutating data using the tidyverse\n\n\n\nStefano Coretta\n\n\nJul 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBar charts\n\n\n\n\n\n\n\n\n\nJul 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nFilter data\n\n\n\n\n\n\nStefano Coretta\n\n\nJul 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to regression models (Part II): include numeric predictors\n\n\nLearn how to include numeric predictors in your regression models\n\n\n\nStefano Coretta\n\n\nJul 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to regression models (Part III): include categorical predictors\n\n\nLearn how to include categorical predictors in your regression models\n\n\n\nStefano Coretta\n\n\nJul 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nRegression models: working with MCMC draws\n\n\nLearn how extract, wrangle and plot MCMC posterior draws\n\n\n\nStefano Coretta\n\n\nAug 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nRegression models: interactions\n\n\nAdd and interpret interactions in regression models\n\n\n\nStefano Coretta\n\n\nAug 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nRegression models: interactions between categorical predictors\n\n\nAdd and interpret interactions of two categorical predictors\n\n\n\nStefano Coretta\n\n\nAug 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nRegression: binary outcome variables\n\n\nLearn how to fit regression models with binary outcome variable using the Bernoulli/binomial distribution\n\n\n\nStefano Coretta, Elizabeth Pankratz\n\n\nSep 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to plotting\n\n\nLearn about basic principles of good graphs\n\n\n\nStefano Coretta\n\n\nSep 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nFaceting plots\n\n\n\n\n\n\n\n\n\nSep 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nDensity plots\n\n\n\n\n\n\n\n\n\nSep 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced plotting\n\n\n\n\n\n\n\n\n\nSep 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nQuantitative data analysis\n\n\nLearn more about quantitative data analysis\n\n\n\nStefano Coretta\n\n\nSep 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nInference and uncertainty\n\n\nLearn about the inference process and the uncertainty that comes with it\n\n\n\nStefano Coretta\n\n\nSep 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nEmpirical research and research context\n\n\nLearn more about empirical research and the research context\n\n\n\nStefano Coretta\n\n\nSep 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nResearch questions and hypotheses\n\n\nLearn more about research questions, hypotheses and how to formulate them\n\n\n\nStefano Coretta\n\n\nSep 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is statistics?\n\n\nLearn about the inference process and the uncertainty that comes with it\n\n\n\nStefano Coretta\n\n\nSep 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nFile management\n\n\n\n\n\n\nStefano Coretta\n\n\nSep 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nResearch project management\n\n\n\n\n\n\nStefano Coretta\n\n\nSep 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nReproducibility and replicability\n\n\nLearn about the meaning of reproducible, replicable, robust and generalisable research\n\n\n\nStefano Coretta\n\n\nSep 15, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/file-manage.html",
    "href": "posts/file-manage.html",
    "title": "File management",
    "section": "",
    "text": "Prerequisites\n\n\n\n\nResearch project management"
  },
  {
    "objectID": "posts/file-manage.html#data-management-plan-dmp",
    "href": "posts/file-manage.html#data-management-plan-dmp",
    "title": "File management",
    "section": "1 Data Management Plan (DMP)",
    "text": "1 Data Management Plan (DMP)\nA Data Management Plan (DMP) covers anything related to data types, their volume (size), how you will acquire or collect them, where and how you will store them, and aspects about data integrity, confidentiality, retention and destruction, sharing and deposit.\nThe University of Edinburgh has two important resources:\n\nUoE Research Data Management policy.\nUoE DMP Online.\n\nHere is what a DMP project looks like in the UoE DMP Online tool."
  },
  {
    "objectID": "posts/file-manage.html#research-compendium",
    "href": "posts/file-manage.html#research-compendium",
    "title": "File management",
    "section": "2 Research Compendium",
    "text": "2 Research Compendium\nResearch compendia are all the data, files, publications related to a research project. Here some definitions:\n\nA research compendium accompanies, enhances, or is a scientific publication providing data, code, and documentation for reproducing a scientific workflow.\n\n—Research Compendium\n\nA research compendium is a collection of all digital parts of a research project including data, code, texts (protocols, reports, questionnaires, meta data). The collection is created in such a way that reproducing all results is straightforward.\n\n—The Turing Way: Research Compendia\nIt is important to think about how you will structure and keep up your research compendium, from the initial stages throughout the project and after completion.\nIt is usually helpful to have a single folder on a computer which contains everything. The following sections give you some tips on how to organise the folder."
  },
  {
    "objectID": "posts/file-manage.html#organising-and-naming-files",
    "href": "posts/file-manage.html#organising-and-naming-files",
    "title": "File management",
    "section": "3 Organising and naming files",
    "text": "3 Organising and naming files\nSome of the following suggestions come from Organising files and File naming which are part of the Open Science Framework (OSF) Support pages.\nTo organise your files:\n\nCreate one folder and make that the folder for your research project folder.\nIn that folder, create folders for data/ and for scripts/ (and plots/, dissertation/, etc).\nIn data/ have a raw/ and derived/ folder:\nRaw data (data that, if lost, it is very unfortunate; for example, experiment data, data which was manually annotated, etc) should be saved in data/raw/.\nDerived data (data that is derived with scripts) should be saved in data/derived/.\n\nThe figure below shows an example of an MSc research project.\n\n\n\n\n\nYou also want to carefully think about file naming. Check the File naming for tips.\nAvoid common pitfalls in naming your files, like the ones in the following example."
  },
  {
    "objectID": "posts/file-manage.html#back-up",
    "href": "posts/file-manage.html#back-up",
    "title": "File management",
    "section": "4 Back up",
    "text": "4 Back up\nMake sure you have a backup system in place.\n\nSave copies of the entire folder in an external hard drive.\nSaving copies of the entire folder in an online storage service (iCloud Drive, One Drive, DropBox, Google Drive, …).\n\nBut if you are working on that copy via syncing, make sure you have a second independent place you back up to, like a hard drive.\n\nUsing a versioning system like git."
  },
  {
    "objectID": "posts/file-manage.html#research-projects-are-dynamic",
    "href": "posts/file-manage.html#research-projects-are-dynamic",
    "title": "File management",
    "section": "5 Research projects are dynamic",
    "text": "5 Research projects are dynamic\n\nBe prepared to change how files and folders are organised after you start.\nProjects evolve over time and sometimes you need to clean things up.\nUse a good system to mark versions in your files.\n\nTwo simple systems to mark file versions:\n\nUse full DATE in the file name\n\ndissertation-2022-11-21.\ndissertation-2023-03-01.\n\nOr use version number\n\nInspired by Semantic versioning from programming but can be helpful with research files too!\ndissertation-v1.0.\ndissertation-v1.1.\ndissertation-v2.0."
  },
  {
    "objectID": "posts/file-manage.html#licensing",
    "href": "posts/file-manage.html#licensing",
    "title": "File management",
    "section": "6 Licensing",
    "text": "6 Licensing\nA license gives someone official permission to reuse something while protecting the intellectual property of the original creator.\nUse open licenses to ensure the data/code can be used by other researchers.\nThe Creative Commons licenses are now common in research."
  },
  {
    "objectID": "posts/inference.html",
    "href": "posts/inference.html",
    "title": "Inference and uncertainty",
    "section": "",
    "text": "Prerequisites\n\n\n\n\nQuantitative data analysis"
  },
  {
    "objectID": "posts/inference.html#inference-process",
    "href": "posts/inference.html#inference-process",
    "title": "Inference and uncertainty",
    "section": "1 Inference process",
    "text": "1 Inference process\n\n\n\n\n\nImbuing numbers with meaning (see this section) is a good characterisation of the inference process.\nHere is how it works. We have a question about something. Let’s imagine that this something is the population of British Sign Language signers.\nWe want to know whether the cultural background of the BSL signers is linked to different pragmatic uses of the sign for BROTHER.\nBut we can’t survey the entire population of BSL signers.1\nSo instead of surveying all BSL users, we take a sample from the BSL population.\nThe sample is our empirical data (the product of our study or observation).\nNow, how do we go from data/observation to answering our question about the use of BROTHER? We can use the inference process!\n\n\n\n\n\n\nInference process\n\n\n\nInference is the process of understanding something about a population based on the sample (aka the data) taken from that population.\n\n\nThe figure below is a schematic representation of the inference process."
  },
  {
    "objectID": "posts/inference.html#inference-is-not-infallible",
    "href": "posts/inference.html#inference-is-not-infallible",
    "title": "Inference and uncertainty",
    "section": "2 Inference is not infallible",
    "text": "2 Inference is not infallible\nHowever, inference, despite being based on data, does not guarantee that the answers to our questions are right or even that they are true.\nIn fact, any observation we make comes with a certain degree of uncertainty and variability.\n\n\n\n\n\n\nExtra\n\n\n\n\n\n\nCheck out this article: https://www.scientificamerican.com/article/if-you-say-science-is-right-youre-wrong/.\nFind out about Popper’s view of falsification and fallibilism.\nLearn more about uncertainty and subjectivity in research: Vasishth and Gelman 2021, Gelman and Hennig 2017."
  },
  {
    "objectID": "posts/inference.html#uncertainty-and-variability",
    "href": "posts/inference.html#uncertainty-and-variability",
    "title": "Inference and uncertainty",
    "section": "3 Uncertainty and variability",
    "text": "3 Uncertainty and variability\n\nPliny the Elder was a Roman philosopher who died in the Vesuvius eruption in 79 CE. The only second certain thing must have been death.\nMoving away from dark irony, as researchers we have to deal with:\n\nUncertainty in any observation of a phenomenon due to measurement error or because we can directly measure what we want to measure.\nVariability among different observations of the same phenomenon due to natural and inevitable variability and measurement error.\n\nThe following picture is a reconstruction of what Galileo Galilei saw when he pointed one of his first telescopes towards Saturn, based on his 1610 sketch: a blurry circle flanked by two smaller blurry circles.\n\nOnly six years later, telescopes were much better and Galileo could correctly identify that the flaking circles were not spheres orbiting around Saturn, but rings.\nThe moral of the story is that at any point in history we are like Galileo in at least some of our research: we might be close to understanding something but not quite there yet."
  },
  {
    "objectID": "posts/inference.html#statistics-as-a-tool",
    "href": "posts/inference.html#statistics-as-a-tool",
    "title": "Inference and uncertainty",
    "section": "4 Statistics as a tool",
    "text": "4 Statistics as a tool\nSo what do we do with such uncertainty and variability? We can use statistics to quantify them!\nStatistics is a tool that helps us quantifying uncertainty and controlling for variability.\nBut what is exactly statistics? Check What is statistics? to find out.\n\n\n\n\n\n\nNext\n\n\n\n\nWhat is statistics?"
  },
  {
    "objectID": "posts/inference.html#footnotes",
    "href": "posts/inference.html#footnotes",
    "title": "Inference and uncertainty",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNOTE: Population can be a set of anything, not just a specific group of people. For example, the words in a dictionary can be a “population”; or the antipassive constructions of Austronesian languages…↩︎"
  },
  {
    "objectID": "posts/repro-repli.html",
    "href": "posts/repro-repli.html",
    "title": "Reproducibility and replicability",
    "section": "",
    "text": "Prerequisites\n\n\n\n\nResearch questions and hypotheses"
  },
  {
    "objectID": "posts/repro-repli.html#definitions",
    "href": "posts/repro-repli.html#definitions",
    "title": "Reproducibility and replicability",
    "section": "1 Definitions",
    "text": "1 Definitions\nThe terms reproducible and replicable are often used interchangeably but The Turning Way has proposed a semantic scheme which introduces two more terms: robust and generalisable.\nCheck out the section Table of Definitions for Reproducibility from The Turing Way book, which defines the four terms."
  },
  {
    "objectID": "posts/research-qh.html",
    "href": "posts/research-qh.html",
    "title": "Research questions and hypotheses",
    "section": "",
    "text": "Prerequisites\n\n\n\n\nEmpirical research"
  },
  {
    "objectID": "posts/research-qh.html#research-questions",
    "href": "posts/research-qh.html#research-questions",
    "title": "Research questions and hypotheses",
    "section": "1 Research questions",
    "text": "1 Research questions\nResearch questions are testable questions whose answers directly address the problem.\nThey take the form of actual questions:\n\nWhat is the average speech rate of adolescents vs that of older adults?\nWhat happens to infants syntactic processing when they move from a monolingual to a multilingual environment?\nIs the morphological complexity of languages spoken by larger populations different from that of languages spoken by smaller populations?\n\nResearch questions are always necessary, independent of the type and objective of the research.\nWhile there is an undue pressure on researcher to come up with “novel” research questions all the time, it is perfectly fine to ask the same question multiple times."
  },
  {
    "objectID": "posts/research-qh.html#research-hypotheses",
    "href": "posts/research-qh.html#research-hypotheses",
    "title": "Research questions and hypotheses",
    "section": "2 Research hypotheses",
    "text": "2 Research hypotheses\nResearch hypotheses are statements (not questions) about the research problem.\nHypotheses must be falsifiable (there can be in principle an outcome that shows them to be false). See below.\nHypotheses are never true nor confirmed. We can only corroborate hypothesis, and it’s a long term process. The same hypothesis has to be tested again and again, but multiple researchers in multiple contexts.\nResearch is not a one-off matter: knowledge can only be acquired slowly and with a lot of effort.\nIt is however perfectly fine to run a study with only research questions, without a research hypothesis. As long as you clearly state whether you are talking about research questions or research hypotheses, you are fine.\n\n2.1 Falsifiability\nYou might be wondering what it is meant by “falsifiable” statements. From Seven examples of falsifiability by John Spacey:\n\nA statement is falsifiable if it can be contradicted by an observation. If such observation is impossible to make with current technology, falsifiability is not achieved.\n\nSome examples of falsifiable hypotheses:\n\n“Life only exists on Earth.” (it would be falsified by the observation of life somewhere else).\n“If there is a 1st person exclusive dual, then there is also a 1st person inclusive dual.” [Universal 1871] (it would be falsified by the observation of languages with a 1st person exclusive dual but without the incluve alternative).\n“Infants start uttering full sentences only after their 12th month of life.” (it would be falsified by the observation of infants uttering full sentences before their 12th month of life).\n\nAnd some examples of non-falsifiable hypotheses.\n\n“Life might exist outside of the Solar system.” (if we observe life outside the Solar system or we don’t, the statement is still true).\n“Languages with a 1st person inclusive dual can have a 1st person exclusive dual.” (whether we observe a language with both 1st inclusive and exclusive dual or not, the statement is still true.)\n\n\n\n\n\n\n\nNext\n\n\n\n\nReproducibility and replicability"
  },
  {
    "objectID": "posts/statistics.html",
    "href": "posts/statistics.html",
    "title": "What is statistics?",
    "section": "",
    "text": "Prerequisites\n\n\n\n\nQuantitative data analysis"
  },
  {
    "objectID": "posts/statistics.html#what-is-statistics",
    "href": "posts/statistics.html#what-is-statistics",
    "title": "What is statistics?",
    "section": "1 What is statistics?",
    "text": "1 What is statistics?\nStatistics is a tool. But what does it do? There are at least four ways of looking at statistics as a tool.\n\nStatistics is the science concerned with developing and studying methods for collecting, analyzing, interpreting and presenting empirical data. (From UCI Department of Statistics)\nStatistics is the technology of extracting information, illumination and understanding from data, often in the face of uncertainty. (From the British Academy)\nStatistics is a mathematical and conceptual discipline that focuses on the relation between data and hypotheses. (From the Standford Encyclopedia of Philosophy)\nStatistics as the art of applying the science of scientific methods. (From ORI Results, Nature)\n\nTo quote a statistician:\n\n\n\n\n\n\nNote\n\n\n\nStatistic is both a science and an art.\nIt is a science in that its methods are basically systematic and have general application and an art in that their successful application depends, to a considerable degree, on the skill and special experience of the statistician, and on his knowledge of the field of application.\n—L. H. C. Tippett\n\n\n\n\n\n\n\n\nExtra\n\n\n\n\n\nCheck out the etymology of statistics here: https://en.wiktionary.org/wiki/statistics#Etymology_1."
  },
  {
    "objectID": "posts/statistics.html#what-statistics-is-not",
    "href": "posts/statistics.html#what-statistics-is-not",
    "title": "What is statistics?",
    "section": "2 What statistics is NOT",
    "text": "2 What statistics is NOT\nStatistics is a lot of things, but it is also not a lot of things.\n\nStatistics is not maths, but it is informed by maths.\nStatistics is not about hard truths not how to seek the truth (see Inference and uncertainty).\nStatistics is not a purely objective endeavour. In fact there are a lot of subjective aspects to statistics (see below).\nStatistics is not a substitute of common sense and expert knowledge.\nStatistics is not just about \\(p\\)-values and significance testing.\n\nAs Gollum would put it, all that glisters is not gold."
  },
  {
    "objectID": "posts/statistics.html#many-analysts-one-data-set-subjectivity-exposed",
    "href": "posts/statistics.html#many-analysts-one-data-set-subjectivity-exposed",
    "title": "What is statistics?",
    "section": "3 Many Analysts, One Data Set: subjectivity exposed",
    "text": "3 Many Analysts, One Data Set: subjectivity exposed\n\n\n\n\n\nIn Silberzahn et al 2018, a group of researchers asked 29 independent analysis teams to answer the following question based on provided data: Is there a link between player skin tone and number of red cards in soccer?\nCrucially,\n\n69% of the teams reported an effect of player skin tone, and 31% did not.\nIn total, the 29 teams came up with 21 unique types of statistical analysis.\n\nThese results clearly show how subjective statistics is and how even a straightforward question can lead to a multitude of answers.\nTo put it in Silberzah et al’s words:\n\nThe observed results from analyzing a complex data set can be highly contingent on justifiable, but subjective, analytic decisions.\n\nThis is why you should always be somewhat sceptical of the results of any single study: you never know what results might have been found if another research team did the study. This is a reason for why replicating research is very important.\n\nCoretta et al 2023 tried something similar, but in the context of the speech sciences: they asked 30 independent analysis teams (84 signed up, 46 submitted an analysis, 30 submitted usable analyses) to answer the question Do speakers acoustically modify utterances to signal atypical word combinations?\nIncredibly, the 30 teams submitted:\n\n109 individual analyses. A bit more than 3 analyses per team!\n52 unique measurement specifications and 47 unique model specifications.\n\n\nNine teams out of the thirty (30%) reported to have found at least one statistically reliable effect (based on the inferential criteria they specified). Of the 170 critical model coefficients, 37 were claimed to show a statistically reliable effect (21.8%).\n\n—Coretta et al, 2023"
  },
  {
    "objectID": "posts/statistics.html#the-new-statistics",
    "href": "posts/statistics.html#the-new-statistics",
    "title": "What is statistics?",
    "section": "4 The “New Statistics”",
    "text": "4 The “New Statistics”\nThe Silberzahn et al and Coretta et al studies are just the tip of the iceberg. We are currently facing a “research crisis”.\n\nCumming 2014 proposes a new approach to statistics, which he calls the “New Statistics”, in response to the research crisis.\nThe New Statistics mainly addresses three problems:\n\nPublished research is a biased selection of all (existing and possible) research.\nData analysis and reporting are often selective and biased.\nIn many research fields, studies are rarely replicated, so false conclusions persist.\n\nTo help solve those problems, it proposes these solutions (among others):\n\nPromoting research integrity.\nShifting away from statistical significance to estimation.\nBuilding a cumulative quantitative discipline."
  },
  {
    "objectID": "posts/statistics.html#the-bayesian-new-statistics",
    "href": "posts/statistics.html#the-bayesian-new-statistics",
    "title": "What is statistics?",
    "section": "5 The Bayesian New Statistics",
    "text": "5 The Bayesian New Statistics\nKurschke and Liddell 2018 revisit the New Statistics and make a further proposal: to adopt the historically older but only recently popularised approach of Bayesian statistics. They call this the Bayesian New Statistics.\nThe classical approach to statistics is the frequentist method, based on work by Fisher, and Neyman and Pearson. Frequentist statistics is based on rejecting the “null hypothesis” (i.e. the hypothesis that there is no difference between groups) using p-values.\nBayesian statistics provides researchers with more appropriate and more robust ways to answer research questions, by reallocation of belief/credibility across possibilities.\nYou can learn more about the frequentist and the Bayesian approach here (TBA!!!)."
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Pre-workshop set-up",
    "section": "",
    "text": "Please, follow these instructions carefully to get ready at least one day before the workshop."
  },
  {
    "objectID": "setup.html#pre-requisites",
    "href": "setup.html#pre-requisites",
    "title": "Pre-workshop set-up",
    "section": "1 Pre-requisites",
    "text": "1 Pre-requisites\nBefore installing the necessary software, make sure you have installed or updated the following software.\n\nThe latest version of R (https://cloud.r-project.org).\nThe latest version of RStudio (https://www.rstudio.com/products/rstudio/download/#download).\nYour operating system is up-to-date."
  },
  {
    "objectID": "setup.html#installation",
    "href": "setup.html#installation",
    "title": "Pre-workshop set-up",
    "section": "2 Installation",
    "text": "2 Installation\n\n\n\n\n\n\nImportant\n\n\n\nIf you have previously installed the C++ toolkit or if you have recently updated your OS, please follow these instructions to reinstall them.\n\n\nNow you will need to install a few packages and extra software.\nHere is an overview of what you will install:\n\nC++ toolchain.\nR packages: tidyverse, brms, tidybayes, extraDistr.\n\n\n2.1 1. Install the C++ toolchain\nThe package brms used in the workshop requires a working C++ toolchain to compile models.\n\n2.1.1 Windows\nFor Windows, follow the instructions here: https://cran.r-project.org/bin/windows/Rtools/rtools43/rtools.html\n\n\n2.1.2 macOS\nFor macOS, open the Terminal and write the following line then press enter/return:\nxcode-select --install\nYou’ll see a panel that asks you to install the Xcode Command Line Tools. Install them. Downloading and installation will take 30 to 60 minutes.\n\n\n2.1.3 Linux\nFor Linux, follow the instructions here: https://github.com/stan-dev/rstan/wiki/Configuring-C-Toolchain-for-Linux\n\n\n\n2.2 2. Install the R packages\nYou need to install the following packages:\ninstall.packages(c(\"tidyverse\", \"brms\", \"tidybayes\", \"extraDistr\"))\nIt will take several minutes to install the packages, depending on your system and configuration.\nIf after opening the workshop project in RStudio you get asked to install extra packages or software, please do so.\n\n\n2.3 Check your installation\nRun the following in the RStudio Console:\nexample(stan_model, package = \"rstan\", run.dontrun = TRUE)\nIf you see some strange looking text printed in the Console and then fit and fit2 in the Environment, then you are sorted!"
  },
  {
    "objectID": "setup.html#troubleshooting",
    "href": "setup.html#troubleshooting",
    "title": "Pre-workshop set-up",
    "section": "3 Troubleshooting",
    "text": "3 Troubleshooting\nIf you are having issues with installation, post a question on Piazza. Note that we won’t reply to questions sent via emails, unless they are related to personal matters."
  },
  {
    "objectID": "posts/wrangle.html#data-wrangling",
    "href": "posts/wrangle.html#data-wrangling",
    "title": "Wrangling data with R",
    "section": "1 Data wrangling",
    "text": "1 Data wrangling\nData wrangling encompasses three main steps in the R pipeline:\n\nImporting data.\nTidying data.\nTransforming data."
  },
  {
    "objectID": "posts/wrangle.html#tidying-data",
    "href": "posts/wrangle.html#tidying-data",
    "title": "Wrangling data with R",
    "section": "2 Tidying data",
    "text": "2 Tidying data\nData tidying is about reshaping the data so that they are in a tidy format.\nThe concept of tidy data was introduced by Wickham 2014. The following illustrations by Allison Horst explain what it is meant with tidy data.\n\n\n\n\n\n\n\n\n\n\nI recommend that if you have control over the layout of tabular data you use a tidy format (columns are variables and rows are observations). However, we sometimes have to use pre-existing data which might be messy or we need to modify our own tidy data for specific purposes (like making a plot that requires a different layout).\nThe nice thing about tidy data is that once they are tidy you can reshape them easily in whichever way you want.\nThe tidyverse package tidyr allows users to tidy up messy data with several functions. (It’s called the “tidyverse” because all packages are designed to work with tidy data!).\nThe most important procedure for tidying up data is pivoting. You can learn more about pivoting in Pivoting."
  },
  {
    "objectID": "posts/wrangle.html#transforming-data",
    "href": "posts/wrangle.html#transforming-data",
    "title": "Wrangling data with R",
    "section": "3 Transforming data",
    "text": "3 Transforming data\nTransforming data encompasses several operations, each of which can be achieved with specific tidyverse functions.\n\nYou can filter data based on specific columns and criteria with filter().\nMutate columns or create new ones based on existing columns with mutate().\nTo summarise data use the summarise() function.\nIt is also possible to join data using the mutating join functions. See Joins of the R for Data Science book.\n\n\n\n\n\n\n\nNext\n\n\n\n\nFilter data\nMutate data\nSummary measures."
  },
  {
    "objectID": "posts/intro-quarto.html",
    "href": "posts/intro-quarto.html",
    "title": "Introduction to Quarto",
    "section": "",
    "text": "Prerequisites\n\n\n\n\nR basics"
  },
  {
    "objectID": "posts/intro-quarto.html#quarto",
    "href": "posts/intro-quarto.html#quarto",
    "title": "Introduction to Quarto",
    "section": "1 Quarto",
    "text": "1 Quarto\nKeeping track of the code you use for data analysis is a very important aspect of research project managing: not only the code is there if you need to rerun it later, but it allows your data analysis to be reproducible (i.e., it can be reproduced by you or other people in such a way that starting with the same data and code you get to the same results).\n\n\n\n\n\n\nReproducible research\n\n\n\nResearch is reproducible when the same data and same code return the same results.\nSee the Definitions page of The Turing Way for definitions of reproducible, replicable, robust and generalisable research.\n\n\nR scripts are great for writing code, and you can even document the code (add explanations or notes) with comments (i.e. lines that start with #).\nBut for longer text or complex data analysis reports, R scripts can be a bit cumbersome.\nA solution to this is using Quarto files (they have the .qmd extension).\n\n\n\n\n\n\nQuiz 1\n\n\n\nWhen is research not reproducible?\n\n a. When the results do not match the researcher's expectations. b. When the the same data and code as in the original study do not produce the published results. c. When research conducted by a different research team with new data does not produce the results as published in the original study. \n\n\n\n\n\n\n\nHint\n\n\n\n\n\nResearch is reproducible when you can produce the same results using the original data and code/methods.\nResearch is replicable when you can produce the same results using new data and the original code/methods.\nSee https://the-turing-way.netlify.app/reproducible-research/overview/overview-definitions.html#table-of-definitions-for-reproducibility.\n\n\n\n\n\n\n1.1 Code… and text!\nQuarto is a file format that allows you to mix code and formatted text in the same file.\nThis means that you can write dynamic reports using Quarto files: dynamic reports are just like analysis reports (i.e. they include formatted text, plots, tables, code output, code, etc…) but they are dynamic in the sense that if, for example, data or code changes, you can just rerun the report file and all code output (plots, tables, etc…) is updated accordingly!\n\n\n\n\n\n\nDynamic reports in Quarto\n\n\n\nQuarto is a file type with extension .qmd in which you can write formatted text and code together.\nQuarto can be used to generate dynamic reports: these are files that are generated automatically from the file source, ensuring data and results in the report are always up to date.\n\n\n\n\n1.2 Formatting text\nR comments in R scripts cannot be formatted (for example, you can’t make text bold or italic).\nText in Quarto files can be fully formatted using a simple but powerful mark-up language called markdown.\nYou don’t have to learn markdown all in one go, so I encourage you to just learn it bit by bit, at your pace. You can look at the the Markdown Guide for an in-depth intro and/or dive in the Markdown Tutorial for a hands-on approach.\nA few quick pointers (you can test them in the Markdown Live Preview):\n\nText can be made italics by enclosing it between single stars: *this text is in italics*.\nYou can make text bold with two stars: **this text is bold!**.\nHeadings are created with #:\n\n# This is a level-1 heading\n\n## This is a level-2 heading\n\n\n\n\n\n\nMark-up, Markdown\n\n\n\nA mark-up language is a text-formatting system consisting of symbols or keywords that control the structure, formatting or relationships of textual elements. The most common mark-up languages are HTML, XML and TeX.\nMarkdown is a simple yet powerful mark-up language.\n\n\n\n\n1.3 Create a .qmd file\n\n\n\n\n\n\nImportant\n\n\n\nWhen working through these tutorials, always make sure you are in the course Quarto Project you created before.\nYou know you are in a Quarto Project because you can see the name of the Project in the top-right corner of RStudio, next to the light-blue cube icon.\nIf you see Project (none) in the top-right corner, that means you are not in the Quarto Project.\nTo make sure you are in the Quarto project, you can open the project by going to the project folder in File Explorer (Windows) or Finder (macOS) and double click on the .Rproj file.\n\n\nTo create a new .qmd file, just click on the New file button (the white square with the green plus symbol), then Quarto Document.... (If you are asked to install/update packages, do so.)\n\n\n\n\n\nA window will open. Add a title of your choice and your name. Make sure the Use visual markdown editor is NOT ticked, then click Create (you will be free to use the visual editor later, but it is important that you first see what a Quarto document looks like under the hood first).\n\n\n\n\n\nA new .qmd file will be created and will open in the File Editor panel in RStudio.\nNote that creating a Quarto file does not automatically save it on your computer. To do so, either use the keyboard short-cut CMD+S/CTRL+S or click on the floppy disk icon in the menu below the file tab.\n\n\n\n\n\nSave the file inside the code/ folder with the following name: tutorial-w04.qmd.\nRemember that all the files of your RStudio project don’t live inside RStudio but on your computer.\n\n\n1.4 Parts of a Quarto file\nA Quarto file usually has three main parts:\n\nThe YAML header (green in the screenshot below).\nCode chunks (red).\nText (blue).\n\n\n\n\n\n\nEach Quarto file has to start with a YAML header, but you can include as many code chunks and as much text as you wish, in any order.\n\n\n\n\n\n\nQuarto: YAML header\n\n\n\nThe header of a .qmd file contains a list of key: value pairs, used to specify settings or document info like the title and author.\nYAML headers start and end with three dashes ---.\n\n\n\n\n\n\n\n\nQuarto: Code chunks\n\n\n\nCode chunks start and end with three back-ticks ``` and they contain code.\n{r} indicates that the code is R code. Settings can be specified inside the chunk with the #| prefix: for example #| label: setup sets the name of the chunk (the label) to setup.\n\n\n\n\n1.5 Working directory\nWhen using Quarto projects, the working directory (the directory all relative paths are relative to) is the project folder.\nHowever, when running code from a Quarto file, the code is run as if the working directory were the folder in which the file is saved.\nThis isn’t an issue if the Quarto file is directly in the project folder, but in our case our Quarto files live in the code/ folder within the project folder (and it is good practice to do so!).\nWe can instruct R to always run code from the project folder (i.e. the working directory is the project folder). This is when the _quarto.yml file comes into play.\nOpen the _quarto.yml file in RStudio (you can simply click on the file in the Files tab and that will open the file in the RStudio editor). Add the line execute-dir: project under the title. Note that indentation should be respected, so the line you write should align with title:, not with project:.\nproject:\n  title: \"dal\"\n  execute-dir: project\nNow, all code in Quarto files, no matter where they are saved, will be run with the project folder as the working directory.\n\n\n1.6 How to add and run code\nYou will use the Quarto document you created to write text and code for this tutorial.\nDelete everything in the Quarto document below the YAML header. It’s just example text—we’re not attached to it!\nThis is what the Quarto document should look like now (if your YAML header also contains “format:html, that’s completely fine):\n\n\n\n\n\nNow add an empty line and in the following line write a second-level heading ## Attach packages, followed by two empty lines. Like so:\n\n\n\n\n\nNow we can insert a code chunk to add the code to attach the tidyverse. To insert a new code chunk, you can click on the Insert a new code chunk button (the little green square icon with a C and a plus) , or you can press OPT+CMD+I/ALT+CTRL+I.\n\n\n\n\n\nA new R code chunk will be inserted at the text cursor position.\nNow go ahead and add the following lines of code inside the R code chunk.\n#| label: setup\n\nlibrary(tidyverse)\n\n\n\n\n\n\nRunning code in Quarto documents\n\n\n\nTo run the code, you have two options:\n\nYou click the small green triangle in the top-right corner of the chunk. This runs all the code in the code chunk.\nEnsure the text cursor is inside the code chunk and press SHIFT+CMD+ENTER/SHIFT+CTRL+ENTER. This too runs all the code in the code chunk.\n\nIf you want to run line by line in the code chunk, you can place the text cursor on the line you want to run and press CMD+ENTER/CTRL+ENTER. The current line is run and the text cursor is moved to the next line. Just like in the .R scripts that we’ve been using in past weeks.\n\n\nRun the setup chunk now.\n\n\n\n\n\nYou will see messages printed below the code chunk, in your Quarto file (don’t worry about the Conflicts, they just tell you that some functions from the tidyverse packages have replaced the base R functions, which is OK).\n\n\n\n\n\n\nPractice 1\n\n\n\nTry this yourself:\n\nCreate a new second-level heading (with ##) called Read data.\nCreate a new R code chunk.\nSet the label of the chunk to read-data.\nAdd code to read the following files (hint: think about where these files are located relative to the working directory, that is, the project folder). Assign the datasets to the variable names polite and glot_status respectively.\n\nwinter2012/polite.csv\ncoretta2022/glot_status.rds\n\nRun the code.\n\n\n\n\n\n1.7 Render Quarto files to HTML\nYou can render a .qmd file into a nicely formatted HTML file.\nTo render a Quarto file, just click on the Render button and an HTML file will be created and saved in the same location of the Quarto file.\n\n\n\n\n\nIt may be shown in the Viewer pane (like in the picture below) or in a new browser window. There are a few ways you can set this option to whichever version you prefer. Follow the instructions that work for you—they all do the same thing.\n\nTools &gt; Global Options &gt; R Markdown &gt; Show output preview in…\nPreferences &gt; R Markdown &gt; Basics &gt; Show output preview in….\nRight beside the Render button, you will see a little white gear. Click on that gear, and a drop-down menu will open. Click on Preview in Window or Preview in Viewer Pane, whichever you prefer.\n\n\n\n\n\n\nRendering Quarto files is not restricted to HTML, but also PDFs and even Word documents!\nThis is very handy when you are writing an analysis report you need to share with others.\n\n\n\n\n\n\nQuarto: Rendering\n\n\n\nQuarto files can be rendered into other formats, like HTML, PDF and Word documents.\n\n\nThe assessments of this course will require you to write text and code in a Quarto file and render it to HTML.\nYou could even write your dissertation in Quarto!\nThe following sections will introduce you to the basics of plotting data. You will keep learning how to create plots throughout the course."
  },
  {
    "objectID": "posts/intro-quarto.html#render-your-quarto-file",
    "href": "posts/intro-quarto.html#render-your-quarto-file",
    "title": "Introduction to Quarto",
    "section": "2 Render your Quarto file!",
    "text": "2 Render your Quarto file!\nNow that you have done all of this hard work, why don’t you try and render the Quarto file you’ve been working on to an HTML file?\nGo ahead, click on the “Render” button and if everything works fine you should see a rendered HTML file in a second!\nNote that you will be asked to render your Quarto files for the assessments, so I recommend you try this out now."
  },
  {
    "objectID": "posts/intro-quarto.html#summary",
    "href": "posts/intro-quarto.html#summary",
    "title": "Introduction to Quarto",
    "section": "3 Summary",
    "text": "3 Summary\nThat’s all for this week!\n\n\n\n\n\n\n\nQuarto files can be used to create dynamic and reproducible reports.\nMark-up languages are text-formatting systems that specify text formatting and structure using symbols or keywords. Markdown is the mark-up language that is used in Quarto documents.\nThe main parts of a .qmd file are the YAML header, text and code chunks.\n\n\n\n\n\n\n\n\n\n\nNext\n\n\n\n\nData types"
  },
  {
    "objectID": "posts/r-troubleshooting.html#troubleshooting",
    "href": "posts/r-troubleshooting.html#troubleshooting",
    "title": "R Troubleshooting",
    "section": "1 Troubleshooting",
    "text": "1 Troubleshooting\nGo to https://bookdown.org/yih_huynh/Guide-to-R-Book/trouble.html and read about common errors and how to fix them.\n\n\n\n\n\n\nTerminology\n\n\n\nThe linked page talks about “loading libraries” and “loading packages”, but you know very well by now that we attach packages and we don’t “load libraries”.\nFor a technical explanation about loading vs attaching and packages vs libraries see: https://r-pkgs.org/dependencies-mindset-background.html#sec-dependencies-namespace and following sections."
  },
  {
    "objectID": "posts/r-troubleshooting.html#fix-me",
    "href": "posts/r-troubleshooting.html#fix-me",
    "title": "R Troubleshooting",
    "section": "2 Fix me!",
    "text": "2 Fix me!\nThe following code chunks have code that throws errors or generate mistakes (without throwing errors). Try to run them in R, study the error and fix the code!\nExample 1:\n\na &lt;- \"2\"\na + 1\n\nExample 2:\nNote: The goal of the following code chunk is to produce a tibble/data frame.\n\niris |&gt;\n  group_by(species) |&gt;\n  summary(\n    mean = mean(Petal.Lengths)\n  )\n\nExample 3:\n\nlibrary(dplyr)\nlibrary(MASS)\n\niris |&gt; \n  select(Species, Petal.Length, Petal.Width)\n\nExample 4:\n\nlirbary(tidyvrese)\n\nstarwars\n  mutate(\n    sex = fatcor(sex, level = c(\"female\", \"hermaphrodite\", \"male\", \"none\"))\n  ) |&gt; \n  count(Sex)\n\nDo all of the levels of the variable sex show up in your tibble/data frame of counts?\nExample 4:\n\niris |&gt; \n  ggplot(x = Species, y = Petal.Length) |&gt; \n  geom_jiter()\n\nExample 5:\n\nbat &lt;- tibble(\n  id = c(A, B, C),\n  response = rbinom(10, 1, 0.5)\n)\n\ncat(bat.response)\n\nExample 6:\n\nstarwars %&gt;% \n  filter(sex != is.na(sex)) %&gt;% \n  ggplot(aes(x = mass, y = height, colour = sex, shape = sex) +\n  geom_point() +\n  scale_x_log10()\n  labs(\n    x = \"Mass\"\n    y = \"Height\"\n    colour = \"Sex\"\n    shape = \"Sex\n  )\n\nExample 7:\n\ntucker2019 &lt;- read_csv(\"data/tucker2019/mald_1_1.rds\")\n\ntucker2019 %&gt;% \n  ggplot(aes(x = isword, y = RT, colour = ACC, fill = ACC)) +\n  geom_jitter(\n    alpha = 0.3,\n    position_jitterdodge(),\n  ) +\n  scale_colour_manual(values = c(\"#dc050c\", \"#196b50\", ))"
  },
  {
    "objectID": "posts/r-troubleshooting.html#where-to-ask-for-help-online",
    "href": "posts/r-troubleshooting.html#where-to-ask-for-help-online",
    "title": "R Troubleshooting",
    "section": "3 Where to ask for help online",
    "text": "3 Where to ask for help online\nThere are a few places you can ask for help online:\n\nStackOverflow is always a good place to start. You should first check if your question or a similar one has been answered already (in most cases, it has!) and if not you can post your question. Make sure you include a Minimal Working Example.\nAnother place is the Posit Community. It’s similar to StackOverflow, but it is specific to R and RStudio related things.\nYou can also ask ChatGPT for help. It is usually good for programming/coding stuff, but not so much for statistics.\nIf you have issues with specific R packages, you could open an issue on GitHub on the respective package repository to let the developer know about the issue.\nIn most cases, even just searching for your question using any web search engine will take you to the right places.\n\n\n\n\n\n\n\nDisclosing help in assessments\n\n\n\nNote that if you ask for help online as part of your assessment and you receive an answer (either by a human or by AI), you are obliged to disclose it in your assessment.\nAsking for help won’t impact your mark, but make sure that proper attribution is given in the assessment (for example, if somebody shared some code with you, you should acknowledge the author of the code)."
  },
  {
    "objectID": "posts/import-multiple.html",
    "href": "posts/import-multiple.html",
    "title": "Reading multiple data files",
    "section": "",
    "text": "Prerequisites\n\n\n\n\nData types"
  },
  {
    "objectID": "posts/import-multiple.html#reading-multiple-files-at-once",
    "href": "posts/import-multiple.html#reading-multiple-files-at-once",
    "title": "Reading multiple data files",
    "section": "1 Reading multiple files at once",
    "text": "1 Reading multiple files at once\nAn important skill to learn is how to read multiple files at once and save the output into a single tibble/data frame.\nThis can be achieved with the list.files() function.\nFor example, let’s read individual files with tongue contours data from ultrasound tongue imaging (UTI). These files are in data/coretta2018/ultrasound/.\nWe can list all files like so:\n\nlist.files(\"data/coretta2018/ultrasound\")\n\n [1] \"it01-tongue-cart.tsv\"  \"it01-vowel-series.tsv\" \"it02-tongue-cart.tsv\" \n [4] \"it02-vowel-series.tsv\" \"it03-tongue-cart.tsv\"  \"it03-vowel-series.tsv\"\n [7] \"it04-tongue-cart.tsv\"  \"it04-vowel-series.tsv\" \"it05-tongue-cart.tsv\" \n[10] \"it05-vowel-series.tsv\" \"it07-tongue-cart.tsv\"  \"it07-vowel-series.tsv\"\n[13] \"it09-tongue-cart.tsv\"  \"it09-vowel-series.tsv\" \"it11-tongue-cart.tsv\" \n[16] \"it11-vowel-series.tsv\" \"it12-tongue-cart.tsv\"  \"it12-vowel-series.tsv\"\n[19] \"it13-tongue-cart.tsv\"  \"it13-vowel-series.tsv\" \"it14-tongue-cart.tsv\" \n[22] \"it14-vowel-series.tsv\" \"pl02-tongue-cart.tsv\"  \"pl02-vowel-series.tsv\"\n[25] \"pl03-tongue-cart.tsv\"  \"pl03-vowel-series.tsv\" \"pl04-tongue-cart.tsv\" \n[28] \"pl04-vowel-series.tsv\" \"pl05-tongue-cart.tsv\"  \"pl05-vowel-series.tsv\"\n[31] \"pl06-tongue-cart.tsv\"  \"pl06-vowel-series.tsv\" \"pl07-tongue-cart.tsv\" \n[34] \"pl07-vowel-series.tsv\"\n\n\nBy default, the list.files() function returns just the name of the file, but we need the full path for the files to be read in R.\n\nlist.files(\"data/coretta2018/ultrasound\", full.names = TRUE)\n\n [1] \"data/coretta2018/ultrasound/it01-tongue-cart.tsv\" \n [2] \"data/coretta2018/ultrasound/it01-vowel-series.tsv\"\n [3] \"data/coretta2018/ultrasound/it02-tongue-cart.tsv\" \n [4] \"data/coretta2018/ultrasound/it02-vowel-series.tsv\"\n [5] \"data/coretta2018/ultrasound/it03-tongue-cart.tsv\" \n [6] \"data/coretta2018/ultrasound/it03-vowel-series.tsv\"\n [7] \"data/coretta2018/ultrasound/it04-tongue-cart.tsv\" \n [8] \"data/coretta2018/ultrasound/it04-vowel-series.tsv\"\n [9] \"data/coretta2018/ultrasound/it05-tongue-cart.tsv\" \n[10] \"data/coretta2018/ultrasound/it05-vowel-series.tsv\"\n[11] \"data/coretta2018/ultrasound/it07-tongue-cart.tsv\" \n[12] \"data/coretta2018/ultrasound/it07-vowel-series.tsv\"\n[13] \"data/coretta2018/ultrasound/it09-tongue-cart.tsv\" \n[14] \"data/coretta2018/ultrasound/it09-vowel-series.tsv\"\n[15] \"data/coretta2018/ultrasound/it11-tongue-cart.tsv\" \n[16] \"data/coretta2018/ultrasound/it11-vowel-series.tsv\"\n[17] \"data/coretta2018/ultrasound/it12-tongue-cart.tsv\" \n[18] \"data/coretta2018/ultrasound/it12-vowel-series.tsv\"\n[19] \"data/coretta2018/ultrasound/it13-tongue-cart.tsv\" \n[20] \"data/coretta2018/ultrasound/it13-vowel-series.tsv\"\n[21] \"data/coretta2018/ultrasound/it14-tongue-cart.tsv\" \n[22] \"data/coretta2018/ultrasound/it14-vowel-series.tsv\"\n[23] \"data/coretta2018/ultrasound/pl02-tongue-cart.tsv\" \n[24] \"data/coretta2018/ultrasound/pl02-vowel-series.tsv\"\n[25] \"data/coretta2018/ultrasound/pl03-tongue-cart.tsv\" \n[26] \"data/coretta2018/ultrasound/pl03-vowel-series.tsv\"\n[27] \"data/coretta2018/ultrasound/pl04-tongue-cart.tsv\" \n[28] \"data/coretta2018/ultrasound/pl04-vowel-series.tsv\"\n[29] \"data/coretta2018/ultrasound/pl05-tongue-cart.tsv\" \n[30] \"data/coretta2018/ultrasound/pl05-vowel-series.tsv\"\n[31] \"data/coretta2018/ultrasound/pl06-tongue-cart.tsv\" \n[32] \"data/coretta2018/ultrasound/pl06-vowel-series.tsv\"\n[33] \"data/coretta2018/ultrasound/pl07-tongue-cart.tsv\" \n[34] \"data/coretta2018/ultrasound/pl07-vowel-series.tsv\"\n\n\nYou see now the full path is return, relative to the Quarto Project directory.\nIn our case, we really just want to read the *-tongue-cart.tsv files, so we can specify a regular expression to list only those files that contain -tongue-cart.tsv.\n\nlist.files(\"data/coretta2018/ultrasound\", full.names = TRUE, pattern = \"*-tongue-cart.tsv\")\n\n [1] \"data/coretta2018/ultrasound/it01-tongue-cart.tsv\"\n [2] \"data/coretta2018/ultrasound/it02-tongue-cart.tsv\"\n [3] \"data/coretta2018/ultrasound/it03-tongue-cart.tsv\"\n [4] \"data/coretta2018/ultrasound/it04-tongue-cart.tsv\"\n [5] \"data/coretta2018/ultrasound/it05-tongue-cart.tsv\"\n [6] \"data/coretta2018/ultrasound/it07-tongue-cart.tsv\"\n [7] \"data/coretta2018/ultrasound/it09-tongue-cart.tsv\"\n [8] \"data/coretta2018/ultrasound/it11-tongue-cart.tsv\"\n [9] \"data/coretta2018/ultrasound/it12-tongue-cart.tsv\"\n[10] \"data/coretta2018/ultrasound/it13-tongue-cart.tsv\"\n[11] \"data/coretta2018/ultrasound/it14-tongue-cart.tsv\"\n[12] \"data/coretta2018/ultrasound/pl02-tongue-cart.tsv\"\n[13] \"data/coretta2018/ultrasound/pl03-tongue-cart.tsv\"\n[14] \"data/coretta2018/ultrasound/pl04-tongue-cart.tsv\"\n[15] \"data/coretta2018/ultrasound/pl05-tongue-cart.tsv\"\n[16] \"data/coretta2018/ultrasound/pl06-tongue-cart.tsv\"\n[17] \"data/coretta2018/ultrasound/pl07-tongue-cart.tsv\"\n\n\nThere’s another catch. These files don’t have column headings! We need to supply them ourselves as a character vector to the col_names argument of read_tsv(). Alternatively you can set that to FALSE and automatic column names will be created for you.\nFinally, we might want to create a new column on the fly which has the file path. This is helpful when the files you are reading don’t have a column that allows you to distinguish data from different files (in these files the first column do this for us).\nYou can create a new column with the path by specifying a name for this new column as the value of the id argument. With id = \"file\" a new column called file will be created with the path of the file.\n\nfiles &lt;- list.files(\n  \"data/coretta2018/ultrasound\",\n  full.names = TRUE,\n  pattern = \"*-tongue-cart.tsv\"\n)\n\n# Column names of the first 14 columns. The rest of the columns are X and Y\n# coordinates of tongue contours of 42 points along the contour:\n# X1,Y1,X2,Y2,X3,Y3,...,X42,Y42.\n#\n# Note that R automatically names unnamed columns with X followed by\n# the column number, so the 84 coordinate columns will be all named Xn.\ncolumns &lt;- c(\n  \"speaker\",\n  \"seconds\",\n  \"rec_date\",\n  \"prompt\",\n  \"label\",\n  \"TT_displacement_sm\",\n  \"TT_velocity\",\n  \"TT_velocity_abs\",\n  \"TD_displacement_sm\",\n  \"TD_velocity\",\n  \"TD_velocity_abs\",\n  \"TR_displacement_sm\",\n  \"TR_velocity\",\n  \"TR_velocity_abs\"\n)\n\ntongue &lt;- read_tsv(files, id = \"file\", col_names = columns, na = \"*\")\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 7598 Columns: 99\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (4): speaker, rec_date, prompt, label\ndbl (94): seconds, TT_displacement_sm, TT_velocity, TT_velocity_abs, TD_disp...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntongue\n\n\n  \n\n\n\nIf you are wondering what na = \"*\" does, it just tells R that cells with * in them should be treated as NAs.\nFab! Now you have a single tibble, tongue with data from all the files!"
  },
  {
    "objectID": "posts/plot-density.html",
    "href": "posts/plot-density.html",
    "title": "Density plots",
    "section": "",
    "text": "Prerequisites\n\n\n\n\nPlotting basics in R\nMutate data"
  },
  {
    "objectID": "posts/plot-density.html#density-plots",
    "href": "posts/plot-density.html#density-plots",
    "title": "Density plots",
    "section": "1 Density plots",
    "text": "1 Density plots\n\n\n\n\n\n\nDensity plots\n\n\n\nDensity plots show the distribution (i.e. the probability density) of the values of a continuous variable.\nThey are created with geom_density().\n\n\nLet’s plot the VOT data from alb_vot.\n\nalb_vot &lt;- read_csv(\"data/coretta2021/alb-vot.csv\") |&gt; \n  mutate(\n    # Multiply by 1000 to get ms from s\n    vot = (voi_onset - release) * 1000\n  )\n\nRows: 180 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): speaker, file, label, consonant\ndbl (2): release, voi_onset\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nalb_vot\n\n\n  \n\n\n\nVOT is a numeric continuous variable so density plots are appropriate.\nTo plot the probability density of a continuous variable, you can use the density geometry. Remember, all geometry functions start with geom_.\nFill in the … in the following code to create a density plot of VOT values in alb_vot.\n\nalb_vot %&gt;%\n  ggplot(aes(x = vot)) +\n  ...\n\nNote that to create a density plot, you only need to specify the x-axis. The y-axis is the probability density, which is automatically calculated (a bit like counts in bar charts, remember?).\nThis is what the plot should look like.\n\n\n\n\n\n\n\n\n\n\n1.1 Make things cosy with a rug\nThe density line shows you a smoothed representation of the data distribution over the VOT values, but you might also want to see the raw data.\nYou can do so by adding the rug geometry. Go ahead and add a rug…\n\nalb_vot %&gt;%\n  ggplot(aes(vot)) +\n  geom_density() +\n  ...\n\nYou should get the following:\n\n\n\n\n\n\n\n\n\nNice huh?\n\n\n\n\n\n\nRug\n\n\n\nRaw data can be shown with a rug, i.e. ticks on the axes that mark where the data is.\nYou can add a rug with geom_rug().\n\n\n\n\n\n\n\n\nQuiz 1\n\n\n\nWhat can you notice about the distribution of VOT values?\n\n\n\n\n\n\nHint\n\n\n\n\n\nAre there multiple peaks in the distribution?\n\n\n\n\n\n\n\n\n\n\n\nNext\n\n\n\n\nFaceting plots\nAdvanced plotting"
  },
  {
    "objectID": "posts/wrangle-mutate.html",
    "href": "posts/wrangle-mutate.html",
    "title": "Mutate data",
    "section": "",
    "text": "library(tidyverse)\n\nglot_status &lt;- readRDS(\"data/coretta2022/glot_status.rds\")\n\nTo change existing columns or create new columns, we can use the mutate() function from the dplyr package.\nTo learn how to use mutate(), we will re-create the status column (let’s call it Status this time) from the Code_ID column in glot_status.\nThe Code_ID column contains the status of each language in the form aes-STATUS where STATUS is one of not_endangered, threatened, shifting, moribund, nearly_extinct and extinct.\n\n\n[1] \"aes-shifting\"       \"aes-extinct\"        \"aes-moribund\"      \n[4] \"aes-nearly_extinct\" \"aes-threatened\"     \"aes-not_endangered\"\n\n\nWe want to create a new column called Status which has only the STATUS label (without the aes- part). To remove aes- from the Code_ID column we can use the str_remove() function from the stringr package. Check the documentation of ?str_remove to learn which arguments it uses.\n\nglot_status &lt;- glot_status |&gt;\n  mutate(\n    Status = str_remove(Code_ID, \"aes-\")\n  )\n\nIf you check glot_status now you will find that a new column, Status, has been added. This column is a character column (chr).\nLet’s reproduce the bar chart from above but with all the data from glot_status, using now the Status column.\n\nglot_status |&gt;\n  ggplot(aes(x = Status)) +\n  geom_bar()\n\n\n\n\n\n\n\nFigure 1: Number of languages by endangerment status (repeated).\n\n\n\n\n\nBut something is not quite right… The order of the levels of Status does not match the order that makes sense (from least to most endangered)! Why?\nThis is because status (the pre-existing column) is a factor column, rather than a simple character column. What is a factor vector/column?\n\n\n\n\n\n\nFactor vector\n\n\n\nA factor vector (or column) is a vector that contains a list of values (called levels) from a closed set.\nThe levels of a factor are ordered alphabetically by default.\n\n\nA vector/column can be mutated into a factor column with the as.factor() function. In the following code, we change the existing column Status, in other words we overwrite it (this happens automatically, because the Status column already exists, so it is replaced).\n\nglot_status &lt;- glot_status |&gt;\n  mutate(\n    Status = as.factor(Status)\n  )\n\n# read below for an explanation of the dollar disgn $ syntax\nlevels(glot_status$Status)\n\n[1] \"extinct\"        \"moribund\"       \"nearly_extinct\" \"not_endangered\"\n[5] \"shifting\"       \"threatened\"    \n\n\nThe levels() functions returns the levels of a factor column in the order they are stored in the factor: by default the order is alphabetical. But wait, what is that $ in glot_status$Status?\nThe dollar sign $ a base R way of extracting a single column (in this case Status) from a data frame (glot_status).\n\n\n\n\n\n\nThe dollar sign `$`\n\n\n\nYou can use the dollar sign $ to extract a single column from a data frame as a vector.\n\n\nWhat if we want the levels of Status to be ordered in a more logical manner: not_endangered, threatened, shifting, moribund, nearly_extinct and extinct? Easy! We can use the factor() function instead of as.factor() and specify the levels and their order.\n\nglot_status &lt;- glot_status |&gt;\n  mutate(\n    Status = factor(Status, levels = c(\"not_endangered\", \"threatened\", \"shifting\", \"moribund\", \"nearly_extinct\", \"extinct\"))\n  )\n\nlevels(glot_status$Status)\n\n[1] \"not_endangered\" \"threatened\"     \"shifting\"       \"moribund\"      \n[5] \"nearly_extinct\" \"extinct\"       \n\n\nYou see that now the order of the levels returned by levels() is the one we specified.\nTransforming character columns to vector columns is helpful to specify a particular order of the levels which can then be used when plotting.\n\nglot_status |&gt;\n  ggplot(aes(x = Status)) +\n  geom_bar()\n\n\n\n\n\n\n\nFigure 2: Number of languages by endangerment status (repeated)."
  },
  {
    "objectID": "posts/wrangle-mutate.html#mutate",
    "href": "posts/wrangle-mutate.html#mutate",
    "title": "Mutate data",
    "section": "",
    "text": "library(tidyverse)\n\nglot_status &lt;- readRDS(\"data/coretta2022/glot_status.rds\")\n\nTo change existing columns or create new columns, we can use the mutate() function from the dplyr package.\nTo learn how to use mutate(), we will re-create the status column (let’s call it Status this time) from the Code_ID column in glot_status.\nThe Code_ID column contains the status of each language in the form aes-STATUS where STATUS is one of not_endangered, threatened, shifting, moribund, nearly_extinct and extinct.\n\n\n[1] \"aes-shifting\"       \"aes-extinct\"        \"aes-moribund\"      \n[4] \"aes-nearly_extinct\" \"aes-threatened\"     \"aes-not_endangered\"\n\n\nWe want to create a new column called Status which has only the STATUS label (without the aes- part). To remove aes- from the Code_ID column we can use the str_remove() function from the stringr package. Check the documentation of ?str_remove to learn which arguments it uses.\n\nglot_status &lt;- glot_status |&gt;\n  mutate(\n    Status = str_remove(Code_ID, \"aes-\")\n  )\n\nIf you check glot_status now you will find that a new column, Status, has been added. This column is a character column (chr).\nLet’s reproduce the bar chart from above but with all the data from glot_status, using now the Status column.\n\nglot_status |&gt;\n  ggplot(aes(x = Status)) +\n  geom_bar()\n\n\n\n\n\n\n\nFigure 1: Number of languages by endangerment status (repeated).\n\n\n\n\n\nBut something is not quite right… The order of the levels of Status does not match the order that makes sense (from least to most endangered)! Why?\nThis is because status (the pre-existing column) is a factor column, rather than a simple character column. What is a factor vector/column?\n\n\n\n\n\n\nFactor vector\n\n\n\nA factor vector (or column) is a vector that contains a list of values (called levels) from a closed set.\nThe levels of a factor are ordered alphabetically by default.\n\n\nA vector/column can be mutated into a factor column with the as.factor() function. In the following code, we change the existing column Status, in other words we overwrite it (this happens automatically, because the Status column already exists, so it is replaced).\n\nglot_status &lt;- glot_status |&gt;\n  mutate(\n    Status = as.factor(Status)\n  )\n\n# read below for an explanation of the dollar disgn $ syntax\nlevels(glot_status$Status)\n\n[1] \"extinct\"        \"moribund\"       \"nearly_extinct\" \"not_endangered\"\n[5] \"shifting\"       \"threatened\"    \n\n\nThe levels() functions returns the levels of a factor column in the order they are stored in the factor: by default the order is alphabetical. But wait, what is that $ in glot_status$Status?\nThe dollar sign $ a base R way of extracting a single column (in this case Status) from a data frame (glot_status).\n\n\n\n\n\n\nThe dollar sign `$`\n\n\n\nYou can use the dollar sign $ to extract a single column from a data frame as a vector.\n\n\nWhat if we want the levels of Status to be ordered in a more logical manner: not_endangered, threatened, shifting, moribund, nearly_extinct and extinct? Easy! We can use the factor() function instead of as.factor() and specify the levels and their order.\n\nglot_status &lt;- glot_status |&gt;\n  mutate(\n    Status = factor(Status, levels = c(\"not_endangered\", \"threatened\", \"shifting\", \"moribund\", \"nearly_extinct\", \"extinct\"))\n  )\n\nlevels(glot_status$Status)\n\n[1] \"not_endangered\" \"threatened\"     \"shifting\"       \"moribund\"      \n[5] \"nearly_extinct\" \"extinct\"       \n\n\nYou see that now the order of the levels returned by levels() is the one we specified.\nTransforming character columns to vector columns is helpful to specify a particular order of the levels which can then be used when plotting.\n\nglot_status |&gt;\n  ggplot(aes(x = Status)) +\n  geom_bar()\n\n\n\n\n\n\n\nFigure 2: Number of languages by endangerment status (repeated)."
  },
  {
    "objectID": "posts/intro-regression.html",
    "href": "posts/intro-regression.html",
    "title": "Introduction to regression models",
    "section": "",
    "text": "Prerequisites\n\n\n\n\nStatistical variables"
  },
  {
    "objectID": "posts/intro-regression.html#regression-to-the-mean",
    "href": "posts/intro-regression.html#regression-to-the-mean",
    "title": "Introduction to regression models",
    "section": "1 Regression (to the mean)",
    "text": "1 Regression (to the mean)\n\nlibrary(brms)\n\nLoading required package: Rcpp\n\n\nLoading 'brms' package (version 2.21.0). Useful instructions\ncan be found by typing help('brms'). A more detailed introduction\nto the package is available through vignette('brms_overview').\n\n\n\nAttaching package: 'brms'\n\n\nThe following object is masked from 'package:stats':\n\n    ar\n\n\nRegression models (also called linear models or linear regression models) are a type of statistical model.\nIn the most general sense, regression models can be used to estimate the underlying process that generates some data. Somewhat more specifically, regression models use the given data to estimate the set of parameters of the equation that is believed to have generated the data.\n\n\n\n\n\n\nWhy “regression”?\n\n\n\n…\n\n\nThe formula of an example of simplest regression model is the following:\n\\[\ny \\sim Gaussian(\\mu, \\sigma)\n\\]\n\n\\(y\\) is the data\n\\(\\sim\\) means “is distributed according to”, or “is generated by”.\n\\(Gaussian(\\mu, \\sigma)\\) is a Gaussian probability distribution with the parameters \\(\\mu\\) (the mean) and \\(\\sigma\\) (the standard deviation)."
  },
  {
    "objectID": "posts/intro-regression.html#simulating-gaussian-data",
    "href": "posts/intro-regression.html#simulating-gaussian-data",
    "title": "Introduction to regression models",
    "section": "2 Simulating Gaussian data",
    "text": "2 Simulating Gaussian data\nLet’s move onto an example of how we can use regression models to estimate the mean and standard deviation of data that is distributed according to a Gaussian probability distribution.\nAlas, it is very difficult to find in nature data that is truly Gaussian, so we will simulate some.\nTo make things a little bit more worldly, we will simulate data of human adult height.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nset.seed(62854)\n\nheight &lt;- tibble(\n  h = round(rnorm(200, 165, 8))\n)\n\n\nheight |&gt; \n  ggplot(aes(h)) +\n  geom_density(fill = \"darkgreen\", alpha = 0.2) +\n  geom_rug() +\n  geom_vline(aes(xintercept = mean(h)), colour = \"purple\", linewidth = 1)"
  },
  {
    "objectID": "posts/intro-regression.html#fitting-a-bayesian-regression-model",
    "href": "posts/intro-regression.html#fitting-a-bayesian-regression-model",
    "title": "Introduction to regression models",
    "section": "3 Fitting a Bayesian regression model",
    "text": "3 Fitting a Bayesian regression model\n\nlibrary(brms)\n\n\nh_1 &lt;- brm(\n  h ~ 1,\n  data = height,\n  chain = 1\n)\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.8e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.023 seconds (Warm-up)\nChain 1:                0.02 seconds (Sampling)\nChain 1:                0.043 seconds (Total)\nChain 1:"
  },
  {
    "objectID": "posts/intro-regression.html#model-summary",
    "href": "posts/intro-regression.html#model-summary",
    "title": "Introduction to regression models",
    "section": "4 Model summary",
    "text": "4 Model summary\nWe can now inspect the summary of the model.\n\nsummary(h_1)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: h ~ 1 \n   Data: height (Number of observations: 200) \n  Draws: 1 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 1000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   165.19      0.59   164.02   166.30 1.00      535      531\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     8.39      0.43     7.58     9.28 1.00      694      498\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nLet’s break that down bit by bit:\n\nThe first few lines are a reminder of the model we fitted:\n\nFamily is the chosen distribution for the outcome variable, a Gaussian distribution.\nLinks lists the link functions.\nFormula is the model formula.\nData reports the name of the data and the number of observations.\nFinally, Draws has information about the MCMC draws.\n\nThen, the Regression Coefficients are listed as a table. In this model we only have one regression coefficient, Intercept which corresponds to \\(\\mu\\) from the formula above. The table has 7 named columns:\n\nEstimate, the mean estimate of the coefficient (i.e. the mean of the posterior distribution of the coefficient).\nEst.error, the error of the mean estimate (i.e. the standard deviation of the posterior distribution of the coefficient).\nl-95% CI and u-95% CI, the lower and upper limits of the 95% Bayesian Credible Interval.\nRhat, Bulk_ESS, Tail_ESS are diagnostics of the MCMC chains.\n\nThen Further distributional parameters are tabled. Here we only have sigma which is \\(\\sigma\\) from the formula above. The table has the same columns as the Regression Coefficients table."
  },
  {
    "objectID": "posts/intro-regression.html#plot-the-posterior-distributions-of-the-coefficients",
    "href": "posts/intro-regression.html#plot-the-posterior-distributions-of-the-coefficients",
    "title": "Introduction to regression models",
    "section": "5 Plot the posterior distributions of the coefficients",
    "text": "5 Plot the posterior distributions of the coefficients\n\nplot(h_1, combo = c(\"dens\", \"trace\"))\n\n\n\n\n\n\n\n\nThe plot above shows the posterior distribution of the estimated coefficient b_Intercept and sigma, which correspond to the \\(\\mu\\) and \\(\\sigma\\) of the formula above, respectively.\nFor the b_Intercept coefficient, the posterior probability encompasses values between 163 and 167 cm, approximately. But some values are more probable then others: the values in the centre of the distribution have a higher probability density then the values on the sides. In other words, values around 165 cm are more probable than values below 164 and above 166, for example.\nHowever, looking at a full probability distribution like that is not super straightforward. Credible Intervals (CrIs) can help summarise the posterior distributions."
  },
  {
    "objectID": "posts/intro-regression.html#interpreting-credible-intervals",
    "href": "posts/intro-regression.html#interpreting-credible-intervals",
    "title": "Introduction to regression models",
    "section": "6 Interpreting Credible Intervals",
    "text": "6 Interpreting Credible Intervals\nAnother way of obtaining summaries of the coefficients is to use the posterior_summary() function. Ignore the last three lines.\n\nposterior_summary(h_1)\n\n               Estimate  Est.Error        Q2.5       Q97.5\nb_Intercept  165.188975 0.58641126  164.023908  166.303072\nsigma          8.391712 0.42565293    7.584844    9.279723\nIntercept    165.188975 0.58641126  164.023908  166.303072\nlprior        -6.030282 0.06212402   -6.160126   -5.914505\nlp__        -712.569786 0.98626592 -715.219667 -711.585583\n\n\nThe 95% CrI of the Intercept is between 164 and 166. This means there is a 95% probability, or that we can be 95% confident, that the Intercept value is within that range.\nFor sigma, the 95% CrI is between 7.6 and 9.3. So, again, there is a 95% probability that the sigma value is between those values."
  },
  {
    "objectID": "posts/intro-regression.html#reporting",
    "href": "posts/intro-regression.html#reporting",
    "title": "Introduction to regression models",
    "section": "7 Reporting",
    "text": "7 Reporting\nWe could report the model and the results like this.\n\n\n\n\n\n\nTip\n\n\n\nWe fitted a Bayesian regression using the brms package [XXX] in R [XXX]. The outcome variable was height and we did not include any predictor, to estimate the overall mean and standard deviation of height.\nBased on the model results, there is a 95% probability that the mean is between 164 and 166 cm and that the standard deviation is between 7.6 and 9.3 cm.\n\n\n\n\n\n\n\n\nNext\n\n\n\n\nIntroduction to regression models (Part II): include numeric predictors"
  },
  {
    "objectID": "posts/intro-plot.html",
    "href": "posts/intro-plot.html",
    "title": "Introduction to plotting",
    "section": "",
    "text": "Prerequisites\n\n\n\n\nStatistical variables"
  },
  {
    "objectID": "posts/intro-plot.html#good-data-visualisation",
    "href": "posts/intro-plot.html#good-data-visualisation",
    "title": "Introduction to plotting",
    "section": "1 Good data visualisation",
    "text": "1 Good data visualisation\nAlberto Cairo has identified four common features of good data visualisation (Spiegelhalter 2019:64–66):\n\n\n\n\n\n\nTip\n\n\n\n\nIt contains reliable information.\nThe design has been chosen so that relevant patterns become noticeable.\nIt is presented in an attractive manner, but appearance should not get in the way of honesty, clarity and depth.\nWhen appropriate, it is organized in a way that enables some exploration.\n\n\n\nLet’s see a few examples."
  },
  {
    "objectID": "posts/intro-plot.html#information-is-not-reliable",
    "href": "posts/intro-plot.html#information-is-not-reliable",
    "title": "Introduction to plotting",
    "section": "2 Information is (not) reliable",
    "text": "2 Information is (not) reliable\nLet’s use the glot_status data. You will not see the code used to create the plots because you will learn about it in later tutorials, but if you are curious you can find the code here.\n\nglot_status\n\n\n  \n\n\n\nThe following plot is titled Number of endangered languages by macroarea and status, but the plot contains both endangered and non-endangered languages.\n\n\n\n\n\n\n\n\n\nWe can fix that by filtering the data so that it contains only endangered languages."
  },
  {
    "objectID": "posts/intro-plot.html#patterns-are-not-noticeable",
    "href": "posts/intro-plot.html#patterns-are-not-noticeable",
    "title": "Introduction to plotting",
    "section": "3 Patterns are (not) noticeable",
    "text": "3 Patterns are (not) noticeable\nThe albvot data contains data on VOT in Albanian. It has data from 6 speakers.\nThe following plot uses a bar chart to show the VOT of different stops, but what you can’t really see is that there is a lot of variability within and among stops and within and among speakers.\n\n\n\n\n\n\n\n\n\nWe can do better. The following plot shows individual measurements of VOT for different stops and speakers. Now an interesting pattern emerges: speaker 5 (s05) has particularly long VOT for /t/ and /k/ relative to the other speakers.\n\n\n\n\n\n\n\n\n\nBar charts are unfortunately overused in research, even in those cases when they are not appropriate. You can learn more about bar charts, and when to use them, in the Bar charts post."
  },
  {
    "objectID": "posts/intro-plot.html#aesthetics-should-not-get-in-the-way",
    "href": "posts/intro-plot.html#aesthetics-should-not-get-in-the-way",
    "title": "Introduction to plotting",
    "section": "4 Aesthetics (should not) get in the way",
    "text": "4 Aesthetics (should not) get in the way\n\n\n\n\n\nThe graph above has a lot of issues:\n\nThe bar length and thickness are not proportional. Compare Japanese with 123 million speakers vs English with 765 million speakers.\nThe graph mixes two scales: million speakers and billion speakers. This makes it look as if Chinese does not have that many more speakers.\nThe shade of orange of the bars does not seem to become proportionally darker with more speakers. Look at Arabic and Hindi: they have a very similar number of speakers but one bar is darker than the other.\nThe three dudes speaking are just fillers. Are they really necessary? Also, they are all white men…\n\nCan you find other issues?\nSee more examples on Ugly Charts."
  },
  {
    "objectID": "posts/intro-plot.html#does-not-enable-exploration",
    "href": "posts/intro-plot.html#does-not-enable-exploration",
    "title": "Introduction to plotting",
    "section": "5 (Does not) enable exploration",
    "text": "5 (Does not) enable exploration\nThe plot below shows the number of gestures enacted by infants of English, Bengali and Chinese background as recorded during a controlled session. Three different types of gestures are shown: hold out and give gestures (ho_gv), index-finger pointing (point) and reach out gestures (reach). Moreover the plot shows the number of gestures at 10 and 12 months.\n\n\n\n\n\n\n\n\n\nA bar chart is appropriate with count data, like in this case, but it does not allow for much exploration. Each infant was recorded at 10 and 12 months of age, but in the plot you don’t see whether individual infants changed their number of gestures. We can only notice that overall the number of gestures increases from 10 to 12 months old.\nWe can use a “connected point” plot: each infant is represented by a dot at 10 and 12 months and the dots of the same infant are connected by a line. This allows us to see whether an individual infant uses more gestures at 12 months.\n\n\n\n\n\n\n\n\n\nYou will notice that some infants don’t really use more gestures and others even use slightly less gestures. You would not be able to see any of this if you used a bar chart, like we used above."
  },
  {
    "objectID": "posts/intro-plot.html#practical-tips",
    "href": "posts/intro-plot.html#practical-tips",
    "title": "Introduction to plotting",
    "section": "6 Practical tips",
    "text": "6 Practical tips\n\n\n\n\n\n\nTip\n\n\n\n\nShow raw data (e.g. individual observations, participants, items…).\nSeparate data in different panels as needed.\nUse simple but informative labels for axes, panels, etc…\nUse colour as a visual aid, not just for aesthetics.\nReuse labels, colours, shapes throughout different plots to indicate the same thing.\n\n\n\n\n\n\n\n\n\nNext\n\n\n\n\nPlotting basics"
  },
  {
    "objectID": "posts/regression-bernoulli.html",
    "href": "posts/regression-bernoulli.html",
    "title": "Regression: binary outcome variables",
    "section": "",
    "text": "Prerequisites\n\n\n\n\nRegression: Indexing of categorical predictors."
  },
  {
    "objectID": "posts/regression-bernoulli.html#binary-outcomes-and-the-bernoulli-family",
    "href": "posts/regression-bernoulli.html#binary-outcomes-and-the-bernoulli-family",
    "title": "Regression: binary outcome variables",
    "section": "1 Binary outcomes and the Bernoulli family",
    "text": "1 Binary outcomes and the Bernoulli family\nBinary outcome variables are very common in linguistics. These are categorical variable that have two levels, e.g.:\n\nyes / no\ngrammatical / ungrammatical\nSpanish / English\ndirect object (gave the girl the book) / prepositional phrase (gave the book to the girl)\ncorrect / incorrect\n\nSo far you have been fitting regression models in which the outcome variable was numeric and continuous. However, a lot of studies use binary outcome variables and it thus important to learn how to deal with those. This is what this post is about.\nWhen modelling binary outcomes, what the researcher is usually interested in is the probability of obtaining one of the two levels. For example, in a lexical decision task one might want to know the probability that real words were recognised as such (in other words, we are interested in accuracy: incorrect or correct response).\nLet’s say there is an 80% probability of responding correctly. So (\\(p()\\) stands for “probability of”):\n\n\\(p(\\text{correct}) = 0.8\\)\n\\(p(\\text{incorrect}) = 1 - p(\\text{correct}) = 0.2\\)\n\nYou see that if you know the probability of one level (correct) you automatically know the probability of the other level, since there are only two levels and the total probability has to sum to 1.\nThe distribution family for binary probabilities is the Bernoulli family. The Bernoulli family has only one parameter, \\(p\\), which is the probability of obtaining one of the two levels (one generally picks which level).\nWith out lexical decision task example, we can write:\n\\[\n\\begin{align}\n\\text{resp}_{correct} & \\sim Bernoulli(p) \\\\\np & = 0.8\n\\end{align}\n\\]\nYou can read it as:\n\nThe probability of getting a correct response follows a Bernoulli distribution with \\(p\\) = 0.8.\n\nIf you randomly sampled from \\(Bernoulli(0.8)\\) you would get “correct” 80% of the times and “incorrect” 20% of the times.\nNow, what we are trying to do when modelling binary outcome variables is to estimate the probability \\(p\\) from the data. But there is a catch: probabilities are bounded between 0 and 1 and regression models don’t work with bounded variables out of the box!\nBounded probabilities are transformed into an unbounded numeric variable. The following section explains how."
  },
  {
    "objectID": "posts/regression-bernoulli.html#probability-and-log-odds",
    "href": "posts/regression-bernoulli.html#probability-and-log-odds",
    "title": "Regression: binary outcome variables",
    "section": "2 Probability and log-odds",
    "text": "2 Probability and log-odds\nAs we have just learnt probabilities are bounded between 0 and 1 but we need something that is not bounded because regression models don’t work with bounded numeric variables.\nThis is where the logit function comes in: the logit function (from “logistic unit”) is a mathematical function that transforms probabilities into log-odds.\nThe plot below shows the correspondence of probabilities (on the y-axis) and log-odds (on the x-axis), as marked by the black S-shaped line. Since probabilities can’t be smaller than 0 and greater than 1, the black line slopes in either direction and it approaches 0 and 1 on the y-axis without ever reaching them (in mathematical terms, it’s an asymptotic line). It is helpful to just memorise that probability 0.5 corresponds to log-odds 0.\n\n\n\n\n\n\n\n\n\nWhen you fit a regression model with a binary outcome and a Bernoulli family, the estimates are in log-odds. To transform log-odds back into probability, one uses the inverse logit (or logistic) function. The logit and inverse logit functions in R are applied with the qlogis() and plogis() functions respectively.\n\np &lt;- 0.3\n\n# from probability to log-odds\nLO &lt;- qlogis(p)\nLO\n\n[1] -0.8472979\n\n# from log-odds to probability\nplogis(LO)\n\n[1] 0.3\n\n\nThat is all you have to understand for now to be able to fit and interpret a Bernoulli regression!"
  },
  {
    "objectID": "posts/regression-bernoulli.html#fitting-a-bernoulli-model",
    "href": "posts/regression-bernoulli.html#fitting-a-bernoulli-model",
    "title": "Regression: binary outcome variables",
    "section": "3 Fitting a Bernoulli model",
    "text": "3 Fitting a Bernoulli model\n\n\n\n\n\n\nBernoulli, binomial and logistic regression\n\n\n\n\n\nA lot of researchers know Bernoulli models under the name “binomial” or “logistic” regression. Please, note that these are exactly equivalent: they refer to a model with a Bernoulli distribution for the outcome variable. It is just that different research traditions call them differently.\nSo if somebody asks you to run a logistic regression, or if you read a paper that reports one, what they just mean is to run a regression with a binary outcome variable and a Bernoulli distribution!\n\n\n\n\n3.1 The data\nTo illustrate how to fit a Bernoulli model, we will use data from Brentari 2024 on the emergent Nicaraguan Sign Language (Lengua de Señas Nicaragüense, NSL).\n\nverb_org &lt;- read_csv(\"data/brentari2024/verb_org.csv\")\n\nRows: 630 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Group, Object, Number, Agency, Num_Predicates\ndbl (1): Participant\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nverb_org contains information on predicates as signed by three groups (Group): home-signers (homesign), first generation NSL signers (NSL1) and second generation NSL signers (NSL2). Specifically, the data coded in Num_Predicates whether the predicates uttered by the signer were single-verb predicates (SVP, single) or a multi-verb predicates (MVP, multiple). The hypothesis of the study is that use of multi-verb predicates would increase with each generation, i.e. that NSL1 signers would use more MVPs than home-signers and that NSL2 signers would use more MVPs than home-signers and NSL1 signers. (For the linguistic reasons behind this hypothesis, check the paper linked above).\nLet’s plot the data to learn a bit more about it.\n\nverb_org |&gt; \n  ggplot(aes(Group, fill = Num_Predicates)) +\n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\nWhat do you notice about the type of predicates in the three groups?\nTo assess the study hypothesis, we can fit a Bernoulli model with Num_Predicates as the outcome variable and Group as the predictor.\nBefore we move on onto fitting the model, it is useful to transform Num_Predicates into a factor and specify the order of the levels so that single is the first level and multiple is the second level.\nThis is useful because Bernoulli models estimate the probability (the parameter \\(p\\) in \\(Bernoulli(p)\\) of getting the second level in the outcome variable.\nYou can also think of this in terms of 0s and 1s: the first level is assigned to 0 and the second level is assigned to 1. Then a Bernoulli distribution with probability \\(p\\) tells you the probability of getting a 1. It doesn’t matter how you prefer to think about Bernoulli distributions, as long as you remember that the probability being estimated is the probability of the second level.\nNow let’s mutate verb_org.\n\nverb_org &lt;- verb_org |&gt; \n  mutate(\n    Num_Predicates = factor(Num_Predicates, levels = c(\"single\", \"multiple\"))\n  )\n\nIf you reproduce the plot above you will see now that the order of Num_Predicates in the legend is “single” then “multiple” and that the order of the proportions in the bar chart have flipped.\nNow we can move on onto modelling.\n\n\n3.2 The model\n\\[\n\\begin{align}\n\\text{Num\\_Preds}_{MVP} & \\sim Bernoulli(p_i) \\\\\nlogit(p_i) & = \\alpha_{\\text{Group}[i]} \\\\\n\\end{align}\n\\]\n\nThe probability of using an MVP follows a Bernoulli distribution with probability \\(p\\).\nThe log-odds of \\(p\\) are equal to \\(\\alpha\\) for each Group.\n\nIn other words, the model estimates \\(p\\) for each group. Here is the code. Remember that to use the indexing approach for categorical predictors (Group) we need to suppress the intercept with the 0 + syntax.\n\nmvp_bm &lt;- brm(\n  Num_Predicates ~ 0 + Group,\n  family = bernoulli,\n  data = verb_org,\n  cores = 4,\n  seed = 1329,\n  file = \"data/cache/regression-bernoulli_mvp_bm\"\n)\n\nLet’s inspect the model summary (we will get 80% CrIs).\n\nsummary(mvp_bm, prob = 0.8)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: Num_Predicates ~ 0 + Group \n   Data: verb_org (Number of observations: 630) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n              Estimate Est.Error l-80% CI u-80% CI Rhat Bulk_ESS Tail_ESS\nGrouphomesign    -0.57      0.15    -0.76    -0.38 1.00     4424     3005\nGroupNSL1        -1.46      0.17    -1.68    -1.24 1.00     3797     2995\nGroupNSL2        -0.02      0.14    -0.21     0.16 1.00     4020     2851\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nBased on the model, there is an 80% probability that the log-odds of a MVP are between -0.76 and -0.38 in home-signers, between -1.68 and -1.24 in NSL1 signers and between -0.21 and 0.16 in NSL2 signers.\nIt’s easier to understand the results if we convert the log-odds to probabilities. The quickest way to do this is to get the Regression Coefficients table from the summary with fixef() and mutate the Q columns with plogis().\n\nfixef(mvp_bm, prob = c(0.1, 0.9)) |&gt;\n  # we need to convert the output of fixef() to a tibble to use mutate()\n  as_tibble() |&gt;\n  # we plogis() the Q columns and round to the second digit\n  mutate(\n    Q10 = round(plogis(Q10), 2),\n    Q90 = round(plogis(Q90), 2)\n  )\n\n\n  \n\n\n\nBased on the model, there is an 80% probability that the probability of using an MVP is between 32-41% in home-signers, between 16-22% in NSL1 signers and between 45-54% in NSL2 signers.\nWe can now see more clearly that the hypothesis of the study is not fully borne out by the data: while NSL2 signers are more likely to use an MVP than home-signers and NSL1 signers, it is not the case that NSL1 signers are more likely to use MVPs than home-signers.\nTo conclude this introduction to Bernoulli models (aka binomial/logistic regressions) we can get the predicted probabilities of use of MVPs in the three groups with conditional_effects().\n\nconditional_effects(mvp_bm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext\n\n\n\n\nPoisson regression: count data"
  },
  {
    "objectID": "posts/intro-regression-predictors.html",
    "href": "posts/intro-regression-predictors.html",
    "title": "Introduction to regression models (Part II): include numeric predictors",
    "section": "",
    "text": "Prerequisites\n\n\n\n\nIntroduction to regression models."
  },
  {
    "objectID": "posts/intro-regression-predictors.html#a-straight-line",
    "href": "posts/intro-regression-predictors.html#a-straight-line",
    "title": "Introduction to regression models (Part II): include numeric predictors",
    "section": "1 A straight line",
    "text": "1 A straight line\nA regression model is a statistical model that estimates the relationship between an outcome variable and one or more predictor variables (more on outcome/predictor below).\nRegression models are based on the formula of a straight line.\n\\[\ny = \\alpha + \\beta * x\n\\]\n\n1.1 Back to school\nYou might remember from school when you were asked to find the values of \\(y\\) given certain values of \\(x\\) and specific values of \\(\\alpha\\) and \\(\\beta\\).1.\nFor example, you were given the following formula of a line:\n\\[\ny = 3 + 2 * x\n\\]\nand the values \\(x = (2, 4, 5, 8, 10, 23, 36)\\). The homework was to calculate the values of \\(y\\) and maybe plot them on a Cartesian coordinate space.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nline &lt;- tibble(\n  x = c(2, 4, 5, 8, 10, 23, 36),\n  y = 3 + 2 * x\n)\n\nggplot(line, aes(x, y)) +\n  geom_point(size = 4) +\n  geom_line(colour = \"red\") +\n  labs(title = bquote(italic(y) == 3 + 2 * italic(x)))\n\n\n\n\n\n\n\n\nUsing the formula, we are able to find the values of \\(y\\). Note that in \\(y = 3 + 2 * x\\), \\(\\alpha = 3\\) and \\(\\beta = 2\\). Importantly, \\(\\alpha\\) is the value of \\(y\\) when \\(x = 0\\). \\(\\alpha\\) is commonly called the intercept of the line.\n\\[\n\\begin{align}\ny & = 3 + 2 * x\\\\\n& = 3 + 2 * 0\\\\\n& = 3\\\\\n\\end{align}\n\\]\nAnd \\(\\beta\\) is the number to add to 3 for each unit increase of \\(x\\). \\(\\beta\\) is commonly called the slope of the line.\n\\[\n\\begin{align}\ny & = 3 + 2 * x\\\\\n& = 3 + 2 * 1 = 3 + 2 = 5\\\\\n& = 3 + 2 * 2 = 3 + (2 + 2) = 7\\\\\n& = 3 + 2 * 3 = 3 + (2 + 2 + 2) = 9\\\\\n\\end{align}\n\\]\nThe following plot should clarify.\n\n\n\n\n\n\n\n\n\nIn the plot, the dashed line indicates the increase in \\(y\\) for every unit increase of \\(x\\) (i.e., every time \\(x\\) increases by 1, \\(y\\) increases by 2).\nNow, in the context of research, you usually measure \\(x\\) (the predictor variable) and \\(y\\) (the outcome variable). Then you have to estimate \\(\\alpha\\) and \\(\\beta\\).\nThis is what regression models are for: given the measured \\(y\\) and \\(x\\), the model estimates \\(\\alpha\\) and \\(\\beta\\).\nYou can play around with this web app by changing the intercept and slope of the line. Note the alternative notation:\n\nThe intercept is \\(\\beta_0\\) rather than \\(\\alpha\\).\nThe slope is \\(\\beta_1\\) rather than \\(\\beta\\).\n\nLet’s use this alternative notation going forward (using \\(\\beta\\)’s with subscript indexes will help understand the process of extracting information from regression models)."
  },
  {
    "objectID": "posts/intro-regression-predictors.html#add-some-error",
    "href": "posts/intro-regression-predictors.html#add-some-error",
    "title": "Introduction to regression models (Part II): include numeric predictors",
    "section": "2 Add some error…",
    "text": "2 Add some error…\nOf course, measurements are noisy: they usually contain some error. Error can have many different causes, but we are usually not interested in learning about those causes. Rather, we just want our model to be able to deal with error.\nLook at the following plot. The plot shows measurements of \\(x\\) and \\(y\\). The points corresponding to the observed measurements are almost on a straight line, but not quite. The vertical distance between the observed points and the expected straight line is the residual error (red lines in the plot).\n\n\n\n\n\n\n\n\n\nThe blue line in the plot is the expected line as estimated by a regression model (you’ll learn how to run such model below), based on the \\(x\\) and \\(y\\) observations.\nThe formula of this regression model is the following:\n\\[\ny = \\beta_0 + \\beta_1 * x + \\epsilon\n\\]\nwhere \\(\\epsilon\\) is the error. In other words, \\(y\\) is the sum of \\(\\beta_0\\) and \\(\\beta_1 * x\\), plus some error.\nGiven that the error \\(\\epsilon\\) is assumed to come from a distribution defined as \\(Gaussian(0, \\sigma)\\), the formula can be rewritten by using a probability distribution (see (Introduction to regression models)[posts/intro-regression.qmd]).\n\\[\n\\begin{align}\ny & \\sim Gaussian(\\mu, \\sigma)\\\\\n\\mu & = \\beta_0 + \\beta_1 * x\\\\\n\\end{align}\n\\]"
  },
  {
    "objectID": "posts/intro-regression-predictors.html#run-a-regression-model",
    "href": "posts/intro-regression-predictors.html#run-a-regression-model",
    "title": "Introduction to regression models (Part II): include numeric predictors",
    "section": "3 Run a regression model",
    "text": "3 Run a regression model\nLet’s get our hands dirty and model some data using a regression model.\nFirst, let’s attach the brms package.\n\nlibrary(brms)\n\nLoading required package: Rcpp\n\n\nLoading 'brms' package (version 2.21.0). Useful instructions\ncan be found by typing help('brms'). A more detailed introduction\nto the package is available through vignette('brms_overview').\n\n\n\nAttaching package: 'brms'\n\n\nThe following object is masked from 'package:stats':\n\n    ar\n\n\nWe will analyse the duration of vowels in Italian.\n\n3.1 Vowel duration in Italian\nLet’s read the R data file coretta2018a/ita_egg.rda. It contains several phonetic measurements obtained from audio and electroglottographic recordings.\nIn this tutorial, we will model vowel duration as a function of speech rate (i.e. number of syllables per second).\nThe expectation is that vowels get shorter with increasing speech rate. You will notice how this is a very vague hypothesis: how shorter do they get? is the shortening the same across all speech rates, or does it get weaker with high speech rates? Our expectation/hypothesis simply states that vowels get shorter with increasing speech rate.\nMaybe we could do better and use what we know from speech production and come up with something more precise, but these type of vague hypothesis are very common, if not standard, in language research, so we will stick to it for practical and pedagogical reasons.\nHere’s the data\n\nload(\"data/coretta2018a/ita_egg.rda\")\n\nita_egg\n\n\n  \n\n\n\nLet’s plot vowel duration and speech rate in a scatter plot.\n\nita_egg |&gt; \n  ggplot(aes(speech_rate, v1_duration)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 15 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 15 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nBy glancing at the raw data, we see a negative relationship between speech rate and vowel duration: vowels get shorter with greater speech rate.\nYou might have noticed a warning about missing values. This is because some observations actually are missing (NA). Let’s drop them from the tibble with drop_na().\n\nita_egg_clean &lt;- ita_egg |&gt; \n  drop_na(v1_duration)\n\nWe will use ita_egg_clean for the rest of the tutorial.\n\n\n3.2 The model\n\\[\n\\begin{align}\n\\text{v1_duration} & \\sim Gaussian(\\mu, \\sigma)\\\\\n\\mu & = \\beta_0 + \\beta_1 * \\text{speech\\_rate}\\\\\n\\end{align}\n\\]\nBefore that though, create a folder called cache/ in the data/ folder of the RStudio project of the course. We will use this folder to save the output of model fitting so that you don’t have to refit the model every time. This is useful because as models get more and more complex, they can take quite a while to fit.\n\nvow_bm &lt;- brm(\n  v1_duration ~ speech_rate,\n  family = gaussian,\n  data = ita_egg_clean,\n  cores = 4,\n  seed = 20912,\n  file = \"data/cache/vow_bm\"\n)\n\nThe model will be fitted and saved in data/cache/ with the file name vow_bm.rds. If you now re-run the same code again, you will notice that brm() does not fit the model again, but rather reads it from the file (no output is shown, but trust me, it works! Check the contents of data/cache/ to see for yourself.).\n\n\n\n\n\n\nWarning\n\n\n\nWhen you save the model fit to a file, R does not keep track of changes in the model specification, so if you make changes to the formula or data, you need to delete the saved model file before re-running the code for the changes to have effect!"
  },
  {
    "objectID": "posts/intro-regression-predictors.html#interpret-the-model-summary",
    "href": "posts/intro-regression-predictors.html#interpret-the-model-summary",
    "title": "Introduction to regression models (Part II): include numeric predictors",
    "section": "4 Interpret the model summary",
    "text": "4 Interpret the model summary\nTo obtain a summary of the model, use the summary() function.\n\nsummary(vow_bm)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: v1_duration ~ speech_rate \n   Data: ita_egg_clean (Number of observations: 3253) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     198.48      3.34   191.84   204.98 1.00     4285     2913\nspeech_rate   -21.73      0.62   -22.96   -20.51 1.00     4350     3040\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    21.66      0.27    21.14    22.17 1.00     3694     2919\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nLet’s focus on the “Regression Coefficients” table of the summary. To understand what they are, just remember the equation of line and the model formula above.\n\nIntercept is \\(\\beta_0\\): this is the mean vowel duration, when speech rate is 0.\nspeech_rate is \\(\\beta_1\\): this is the change in vowel duration for each unit increase of speech rate.\n\nThis should make sense, if you understand the equation of a line: \\(y = \\beta_0 + \\beta_1 x\\). If you are still uncertain, play around with the web app.\nRecall that the Estimate and Est.Error column are simply the mean and standard deviation of the posterior probability distributions of the estimate of Intercept and speech_rate respectively.\nLooking at the 95% Credible Intervals (CrIs), we can say that based on the model and data:\n\nThe mean vowel duration, when speech rate is 0 syl/s, is between 192 and 205 ms, at 95% confidence.\nWe can be 95% confident that, for each unit increase of speech rate (i.e. for each increase of one syllable per second), the duration of the vowel decreases by 20.5-23 ms.\n\nTo see what the posterior probability densities of \\beta_0, \\beta_1 and \\sigma look like, you can quickly plot them with the plot() function.\n\nplot(vow_bm)\n\n\n\n\n\n\n\n\nIf you prefer to see density plots instead of histograms, you can specify the combo argument.\n\nplot(vow_bm, combo = c(\"dens\", \"trace\"))"
  },
  {
    "objectID": "posts/intro-regression-predictors.html#plot-the-model-predictions",
    "href": "posts/intro-regression-predictors.html#plot-the-model-predictions",
    "title": "Introduction to regression models (Part II): include numeric predictors",
    "section": "5 Plot the model predictions",
    "text": "5 Plot the model predictions\nYou should always also plot the model predictions, i.e. the predicted values of vowel duration based on the model predictors (here just speech_rate).\nYou will learn more advanced methods later on, but for now you can use conditional_effects() from the brms package.\n\nconditional_effects(vow_bm, effects = \"speech_rate\")\n\n\n\n\n\n\n\n\nIf you wish to include the raw data in the plot, you can wrap conditional_effects() in plot() and specify points = TRUE. Any argument that needs to be passed to geom_point() (these are all ggplot2 plots!) can be specified in a list as the argument point_args. Here we are making the points transparent.\n\nplot(conditional_effects(vow_bm, effects = \"speech_rate\"), points = TRUE, point_args = list(alpha = 0.1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext\n\n\n\n\nIntroduction to regression models (Part III): include categorical predictors"
  },
  {
    "objectID": "posts/intro-regression-predictors.html#footnotes",
    "href": "posts/intro-regression-predictors.html#footnotes",
    "title": "Introduction to regression models (Part II): include numeric predictors",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDepending on which country you did your schooling in, you might be used to slightly different notation, like \\(mx + c\\). In \\(mx + c\\), \\(m\\) is \\(\\beta\\) and \\(c\\) is \\(\\alpha\\).↩︎"
  },
  {
    "objectID": "posts/wrangle-filter.html",
    "href": "posts/wrangle-filter.html",
    "title": "Filter data",
    "section": "",
    "text": "Data transformation is a fundamental aspect of data analysis.\nAfter the data you need to use is imported into R, you will have to filter rows, create new columns, or join data frames, among many other transformation operations.\nIn this tutorial we will learn how to filter() the data and mutate() or create new columns. In Week 6 (after Flexible Learning week) you will learn how to obtain summary measures and how to count occurrences using the summarise(), group_by() and count() functions."
  },
  {
    "objectID": "posts/wrangle-filter.html#data-transformation",
    "href": "posts/wrangle-filter.html#data-transformation",
    "title": "Filter data",
    "section": "",
    "text": "Data transformation is a fundamental aspect of data analysis.\nAfter the data you need to use is imported into R, you will have to filter rows, create new columns, or join data frames, among many other transformation operations.\nIn this tutorial we will learn how to filter() the data and mutate() or create new columns. In Week 6 (after Flexible Learning week) you will learn how to obtain summary measures and how to count occurrences using the summarise(), group_by() and count() functions."
  },
  {
    "objectID": "posts/wrangle-filter.html#filter",
    "href": "posts/wrangle-filter.html#filter",
    "title": "Filter data",
    "section": "2 Filter",
    "text": "2 Filter\nFiltering data based on specific criteria couldn’t be easier with filter(), from the dplyr package (one of the tidyverse core packages),\nLet’s work with the coretta2022/glot_status data frame. It’s an .rds file, so you need to use the readRDS() function. Go ahead and read the data into glot_status.\nThe glot_status data frame contains the endangerment status for 7,845 languages from Glottolog. There are thousands of languages in the world, but most of them are losing speakers, and some are already no longer spoken. The endangerment status of a language in the data is on a scale from not endangered (languages with large populations of speakers) through threatened, shifting and nearly extinct, to extinct (languages that have no living speakers left).\n\nglot_status\n\n\n  \n\n\n\nBefore we can move on onto filtering data, we first need to learn about logical operators.\n\n2.1 Logical operators\nThere are four main logical operators:\n\nx == y: x equals y.\nx != y: x is not equal to y.\nx &gt; y: x is greater than y.\nx &lt; y: x is smaller than y.\n\nLogical operators return TRUE or FALSE depending on whether the statement they convey is true or false. Remember, TRUE and FALSE are logical values.\nTry these out in the Console:\n\n# This will return FALSE\n1 == 2\n\n[1] FALSE\n\n# FALSE\n\"apples\" == \"oranges\"\n\n[1] FALSE\n\n# TRUE\n10 &gt; 5\n\n[1] TRUE\n\n# FALSE\n10 &gt; 15\n\n[1] FALSE\n\n# TRUE\n3 &lt; 4\n\n[1] TRUE\n\n\n\n\n\n\n\n\nLogical operators\n\n\n\nLogical operators are symbols that compare two objects and return either TRUE or FALSE.\nThe most common logical operators are ==, !=, &gt;, and &lt;.\n\n\n\n\n\n\n\n\nQuiz 2\n\n\n\n\nWhich of the following does not contain a logical operator?\n\n 3 &gt; 1 \"a\" = \"a\" \"b\" != \"b\" 19 &lt; 2\n\nWhich of the following returns c(FALSE, TRUE)?\n\n 3 &gt; c(1, 5) c(\"a\", \"b\") != c(\"a\") \"apple\" != \"apple\"\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n2a.\nCheck for errors in the logical operators.\n2b.\nRun them in the console to see the output.\n\n\n\n\n\n\n\n\n\nExplanation\n\n\n\n\n\n2a.\nThe logical operator == has TWO equal signs. A single equal sign = is an alternative way of writing the assignment operator &lt;-, so that a = 1 and a &lt;- 1 are equivalent.\n2b.\nLogical operators are “vectorised” (you will learn more about this below), i.e they are applied sequentially to all elements in pairs. If the number of elements on one side does not match than of the other side of the operator, the elements on the side that has the smaller number of elements will be recycled.\n\n\n\n\n\nNow let’s see how these work with filter()!\n\n\n2.2 The filter() function\nFiltering in R with the tidyverse is straightforward. You can use the filter() function.\nfilter() takes one or more statements with logical operators.\nLet’s try this out. The following code filters the status column so that only the extinct status is included in the new data frame extinct.\nYou’ll notice we are using the pipe |&gt; to transfer the data into the filter() function; the output of the filter function is assigned &lt;- to extinct. The flow might seem a bit counter-intuitive but you will get used to think like this when writing R code soon enough!\n\nextinct &lt;- glot_status |&gt;\n  filter(status == \"extinct\")\n\nextinct\n\n\n  \n\n\n\nNeat! What if we want to include all statuses except extinct? Easy, we use the non-equal operator !=.\n\nnot_extinct &lt;- glot_status |&gt;\n  filter(status != \"extinct\")\n\nnot_extinct\n\n\n  \n\n\n\nAnd if we want only non-extinct languages from South America? We can include multiple statements separated by a comma!\n\nsouth_america &lt;- glot_status |&gt;\n  filter(status != \"extinct\", Macroarea == \"South America\")\n\nsouth_america\n\n\n  \n\n\n\nCombining statements like this will give you only those rows where both conditions apply. You can add as many statements as you need.\nNow try to filter the data so that you include only not_endangered languages from all macro-areas except Eurasia. This time, don’t save the output to a new data frame. What happens? Where is the output shown?\n\nglot_status |&gt;\n  filter(...)\n\nThis is all great, but what if we want to include more than one status or macro-area?\nTo do that we need another operator: %in%.\n\n\n2.3 The %in% operator\n\n\n\n\n\n\n%in%\n\n\n\nThe %in% operator is a special logical operator that returns TRUE if the value to the left of the operator is one of the values in the vector to its right, and FALSE if not.\n\n\nTry these in the Console:\n\n# TRUE\n5 %in% c(1, 2, 5, 7)\n\n[1] TRUE\n\n# FALSE\n\"apples\" %in% c(\"oranges\", \"bananas\")\n\n[1] FALSE\n\n\nBut %in% is even more powerful because the value on the left does not have to be a single value, but it can also be a vector! We say %in% is vectorised because it can work with vectors (most functions and operators in R are vectorised).\n\n# TRUE, TRUE\nc(1, 5) %in% c(4, 1, 7, 5, 8)\n\n[1] TRUE TRUE\n\nstocked &lt;- c(\"durian\", \"bananas\", \"grapes\")\nneeded &lt;- c(\"durian\", \"apples\")\n\n# TRUE, FALSE\nneeded %in% stocked\n\n[1]  TRUE FALSE\n\n\nTry to understand what is going on in the code above before moving on.\n\n\n2.4 Now filter the data\nNow we can filter glot_status to include only the macro-areas of the Global South and only languages that are either “threatened”, “shifting”, “moribund” or “nearly_extinct”. I have started the code for you, you just need to write the line for filtering status.\n\nglobal_south &lt;- glot_status |&gt;\n  filter(\n    Macroarea %in% c(\"Africa\", \"Australia\", \"Papunesia\", \"South America\"),\n    ...\n  )\n\nThis should not look too alien! The first statement, Macroarea %in% c(\"Africa\", \"Australia\", \"Papunesia\", \"South America\") looks at the Macroarea column and, for each row, it returns TRUE if the current row value is in c(\"Africa\", \"Australia\", \"Papunesia\", \"South America\"), and FALSE if not."
  },
  {
    "objectID": "posts/plotting-basics.html",
    "href": "posts/plotting-basics.html",
    "title": "Plotting basics",
    "section": "",
    "text": "Prerequisites\n\n\n\n\nIntroduction to plotting"
  },
  {
    "objectID": "posts/plotting-basics.html#plotting-basics",
    "href": "posts/plotting-basics.html#plotting-basics",
    "title": "Plotting basics",
    "section": "1 Plotting basics",
    "text": "1 Plotting basics\nPlotting data in R is easy once you understand the basics.\n\n1.1 Graphic systems\nIn R, you can create plots using different systems.\n\nBase R.\nlattice.\nggplot2.\nmore…\n\nIn this course you will learn how to use the ggplot2 system, but before we dive in, let’s have a look at the base R plotting system too.\n\n\n1.2 Base R plotting function\nLet’s create two vectors, x and y and plot them.\n\nx &lt;- 1:10\ny &lt;- x^3\n\nplot(x, y)\n\n\n\n\n\n\n\n\nEasy!\nNow let’s add a few more things.\n\nplot(x, y, type = \"l\", col = \"purple\", lwd = 3, lty = \"dashed\")\n\n\n\n\n\n\n\n\nWith plots as simple as this one, the base R plotting system is sufficient, but to create more complex plots (which is virtually always the case), base R gets incredibly complicated.\nInstead we can use the tidyverse package ggplot2. ggplot2 works well with the other tidyverse packages and it follows the same principles, so it is convenient to use it for data visualisation instead of base R!"
  },
  {
    "objectID": "posts/plotting-basics.html#your-first-ggplot2-plot",
    "href": "posts/plotting-basics.html#your-first-ggplot2-plot",
    "title": "Plotting basics",
    "section": "2 Your first ggplot2 plot",
    "text": "2 Your first ggplot2 plot\nThe tidyverse package ggplot2 provides users with a consistent set of functions to create captivating graphics.\n\n\n\n\n\n\nWarning\n\n\n\nTo be able to use the functions in a package, you first need to attach the package. We have already attached the library(tidyverse) packages, among which there is ggplot2, so you don’t need to do anything else.\n\n\nWe will first use the polite data to learn the basics of plotting using ggplot.\n\nlibrary(tidyverse)\n\npolite &lt;- read_csv(\"data/winter2012/polite.csv\")\n\nIn this tutorial we will use the following columns:\n\nf0mn: mean f0 (fundamental frequency).\nH1H2: difference between H2 and H1 (second and first harmonic). A higher H1-H2 difference indicates greater breathiness.\n\n\n2.1 A basic plot\nThese are the minimum constituents of a ggplot2 plot.\n\n\n\n\n\n\nggplot basics\n\n\n\n\nThe data: you have to specify the data frame with the data you want to plot.\nThe mapping: the mapping tells ggplot how to map data columns to parts of the plot like the axes or groupings within the data. (For example, which variable is shown on the x axis, and which one is on the y axis? If data comes from two different groups, should each group get its own colour?) These different parts of the plot are called aesthetics, or aes for short.\n\n\n\nYou can specify the data and mapping with the data and mapping arguments of the ggplot() function.\nNote that the mapping argument is always specified with aes(): mapping = aes(…).\nIn the following bare plot, we are just mapping f0mn to the x-axis and H1H2 to the y-axis, from the polite data frame.\nCreate a new code chunk, copy the following code and run it. From this point on I will assume you’ll create a new code chunk and run the code yourself, without explicit instructions.\n\nggplot(\n  data = polite,\n  mapping = aes(x = f0mn, y = H1H2)\n)\n\n\n\n\n\n\n\n\nNot much to see here: just two axes! So where’s the data? Don’t worry, we didn’t do anything wrong. Showing the data itself requires a further step, which we’ll turn to next.\n\n\n\n\n\n\nQuiz 2\n\n\n\nIs the following code correct? Justify your answer. TRUEFALSE\nggplot(\n  data = polite,\n  mapping = c(x = total_duration, y = articulation_rate)\n)\n\n\n\n\n2.2 Let’s add geometries\nOur code so far makes nice axes, but we are missing the most important part: showing the data!\nData is represented with geometries, or geoms for short. geoms are added to the base ggplot with functions whose names all start with geom_.\n\n\n\n\n\n\nGeometries\n\n\n\nGeometries are plot elements that show the data through geometric shapes.\nDifferent geometries are added to a ggplot using one of the geom_*() functions.\n\n\nFor this plot, you want to use geom_point(). This geom simply adds point to the plot based on the data in the polite data frame.\nTo add geoms to a plot, you write a + at the end of the ggplot() command and include the geom on the next line. For example:\n\nggplot(\n  data = polite,\n  mapping = aes(x = f0mn, y = H1H2)\n) +\n  geom_point()\n\n\n\n\n\n\n\nFigure 1: Scatter plot of mean f0 and H1-H2 difference.\n\n\n\n\n\nThis type of plot, with two continuous axes and data represented by points, is called a scatter plot.\n\n\n\n\n\n\nScatter plot\n\n\n\nA scatter plot is a plot with two numeric axes and points indicating the data. It is used when you want to show the relationship between two numeric variables.\nTo create a scatter plot, use the geom_point() geometry.\n\n\nWhen writing your results section, you could describe the plot this way:\n\nFigure 1 shows a scatter plot of mean f0 on the x-axis and H1-H2 difference on the y-axis. The plot suggest an overall negative relationship between mean f0 and H1-H2 difference. In other words, increasing mean f0 corresponds to decreasing breathiness.\n\n\n\n\n\n\n\nWarning\n\n\n\nNote that using the + is a quirk of ggplot(). The idea behind it is that you start from a bare plot and you add (+) layers of data on top of it. This is because of the philosophy behind the package, called the Layered Grammar of Graphics. In fact, Grammar of Graphics is where you get the GG in ggplot!\n\n\n\n\n2.3 Function arguments\nNote that the data and mapping arguments don’t have to be named explicitly (with data = and mapping =) in the ggplot() function, since they are obligatory and they are specified in that order.\n\nggplot(\n  polite,\n  aes(x = f0mn, y = H1H2)\n) +\n  geom_point()\n\n\n\n\n\n\n\n\nIn fact, you can also leave out x = and y =.\n\nggplot(\n  polite,\n  aes(f0mn, H1H2)\n) +\n  geom_point()\n\n\n\n\n\n\n\n\nTry running ?ggplot in the Console to see the arguments of the function and the order they appear in.\n\n\n\n\n\n\nQuiz 3\n\n\n\nWhich of the following will produce the same plot as the one above? Reason through it first without running the code, then run all of these to check whether they look the way you expected.\n\n ggplot(polite, aes(H1H2, f0mn)) + geom_point() ggplot(polite, aes(y = H1H2, x = f0mn)) + geom_point() ggplot(polite, aes(y = f0mn, x = H1H2)) + geom_point()\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nWhen specifying arguments, the order matters when not using the argument names.\nSo aes(a, b) is different from aes(b, a).\nBut aes(y = b, x = a) is the same as aes(a, b).\n\n\n\n\n\n\n\n2.4 What the pipe?!\nThe code of the latest plot can also be written this way.\n\npolite |&gt;\n  ggplot(aes(f0mn, H1H2)) +\n    geom_point()\n\nWait, what is that thing, |&gt;?\nIt’s called a pipe. Think of a pipe as a teleporter.\nThe pipe |&gt; teleports the data polite into the following function as the first argument. So polite |&gt; ggplot() is equivalent to ggplot(polite).\nFor now it might not make much sense using the pipe, but you will learn next week how to chain many functions one after the other using the pipe, at which point its usefulness will be more obvious.\nAs a sneak peek, you will be able to filter the data before plotting it, like so:\n\npolite |&gt;\n  # include only rows where f0mn &lt; 300\n  filter(f0mn &lt; 300) |&gt;\n  ggplot(aes(f0mn, H1H2)) +\n    geom_point()"
  },
  {
    "objectID": "posts/plotting-basics.html#changing-aesthetics",
    "href": "posts/plotting-basics.html#changing-aesthetics",
    "title": "Plotting basics",
    "section": "3 Changing aesthetics",
    "text": "3 Changing aesthetics\n\n3.1 colour aesthetic\nYou might notice that there seems to be two subgroups within the data: one below about 200 Hz and one above about it.\nIn fact, these subgroups are related to gender. Let’s colour the points by gender then.\nYou can use the colour aesthetic to colour the points by gender, like so:\n\npolite |&gt;\n  ggplot(aes(f0mn, H1H2, colour = gender)) +\n  geom_point()\n\n\n\n\n\n\n\nFigure 2: Scatter plot of mean f0 and H1-H2 difference, by gender.\n\n\n\n\n\n\n\n\n\n\n\ncolour or color?\n\n\n\nTo make ggplot easy for users of different Englishes, it’s possible to write the colour aesthetic either as the British-style colour or the American-style color! Both will get the job done.\n\n\nNotice how colour = gender must be inside the aes() function, because we are trying to map colour to the values of the column gender. Colours are automatically assigned to each level in gender.\nThe default colour palette is used, but you can customise it. You will learn later in the course how to create custom palettes, but you can quickly change palette using one of the scale_colour_*() functions.\nA useful function is the scale_colour_brewer() function. This function creates palettes based on ColorBrewer 2.0. There are three types of palettes (see the linked website for examples):\n\nSequential (seq): a gradient sequence of hues from lighter to darker.\nDiverging (div): useful when you need a neutral middle colour and sequential colours on either side of the neutral colour.\nQualitative (qual): useful for categorical variables.\n\nLet’s use the default qualitative palette.\n\npolite |&gt;\n  ggplot(aes(f0mn, H1H2, colour = gender)) +\n  geom_point() +\n  scale_color_brewer(type = \"qual\")\n\n\n\n\n\n\n\nFigure 3: Scatter plot of mean f0 and H1-H2 difference, by gender.\n\n\n\n\n\nNow try changing the palette argument of the scale_colour_brewer() function to different palettes. (Check the function documentation for a list).\nAnother set of palettes is provided by scale_colour_viridis_d() (the d stands for “discrete” palette, to be used for categorical variables). Here’s an example.\n\npolite |&gt;\n  ggplot(aes(f0mn, H1H2, colour = gender)) +\n  geom_point() +\n  scale_color_viridis_d(option = \"B\")\n\n\n\n\n\n\n\nFigure 4: Scatter plot of mean f0 and H1-H2 difference, by gender.\n\n\n\n\n\n\n\n\n\n\n\nExtra: The default colour palette\n\n\n\n\n\nIf you want to know more about the default colour palette, check this blog post out.\n\n\n\n\n\n3.2 alpha aesthetic\nAnother useful ggplot2 aesthetic is alpha. This aesthetic sets the transparency of the geometry: 0 means completely transparent and 1 means completely opaque.\nChange alpha in the code below to 0.5.\n\npolite |&gt;\n  ggplot(aes(f0mn, H1H2, colour = gender)) +\n  geom_point(alpha = ...) +\n  scale_color_brewer(type = \"qual\")\n\nWhen you are setting a value yourself that should apply to all instances of some geometry, rather than mapping an aesthetic to values in a specific column (like we did above with colour), you should add the aesthetic outside of aes() and usually in the geom function you want to modify.\nSetting a lower alpha is useful when there are a lot of points or other geometries that overlap with each other and it just looks like a blob of colour (you can’t really see the individual geometries; you will see an example next week). It is not the case here, and in fact reducing the alpha makes the plot quite illegible!"
  },
  {
    "objectID": "posts/plotting-basics.html#labels",
    "href": "posts/plotting-basics.html#labels",
    "title": "Plotting basics",
    "section": "4 Labels",
    "text": "4 Labels\nIf you want to change the labels of the axes and the legend, you can use the labs() function, like this.\n\npolite |&gt;\n  ggplot(aes(f0mn, H1H2, colour = gender)) +\n  geom_point() +\n  labs(\n    x = \"Mean f0 (Hz)\",\n    y = \"H1-H2 difference (dB)\",\n    colour = \"Gender\"\n  )\n\n\n\n\n\n\n\nFigure 5: Scatter plot of mean f0 and H1-H2 difference, by gender.\n\n\n\n\n\nAlso add a title and a subtitle (use these two arguments within the labs() function).\n\n\n\n\n\n\nHint\n\n\n\n\n\nFor example, labs(title = \"...\", ...)."
  },
  {
    "objectID": "posts/plotting-basics.html#summary",
    "href": "posts/plotting-basics.html#summary",
    "title": "Plotting basics",
    "section": "5 Summary",
    "text": "5 Summary\nThat’s all for this week!\n\n\n\n\n\n\n\nggplot2 is a plotting package from the tidyverse.\nTo create a basic plot, you use the ggplot() function and specify data and mapping.\n\nThe aes() function allows you to specify aesthetics (like axes, colours, …) in the mapping argument.\nGeometries map data values onto shapes in the plot. All geometry functions are of the type geom_*().\n\nScatter plots are created with geom_point() and can be used with two numeric variables.\n\n\n\n\n\n\n\n\n\n\nNext\n\n\n\n\nBar charts"
  },
  {
    "objectID": "posts/r-basics.html",
    "href": "posts/r-basics.html",
    "title": "R basics",
    "section": "",
    "text": "Prerequisites\n\n\n\n\nComputer basics"
  },
  {
    "objectID": "posts/r-basics.html#why-r",
    "href": "posts/r-basics.html#why-r",
    "title": "R basics",
    "section": "1 Why R?",
    "text": "1 Why R?\nR can be used to analyse all sorts of data, from tabular data (also known as “spreadsheets”), textual data, geographic data and even images.\n\nThis course will focus on the analysis of tabular data, since all of the techniques relevant to this type of data also apply to the other types.\n\nThe R community is a very inclusive community and it’s easy to find help. There are several groups that promote R in minority/minoritised groups, like R-Ladies, Africa R, and Rainbow R just to mention a few.\nMoreover, R is open source and free!"
  },
  {
    "objectID": "posts/r-basics.html#r-vs-rstudio",
    "href": "posts/r-basics.html#r-vs-rstudio",
    "title": "R basics",
    "section": "2 R vs RStudio",
    "text": "2 R vs RStudio\nBeginners usually have trouble understanding the difference between R and RStudio.\nLet’s use a car analogy.\nWhat makes the car go is the engine and you can control the engine through the dashboard.\nYou can think of R as an engine and RStudio as the dashboard.\n\n\n\n\n\n\n\nR\n\n\n\n\nR is a programming language.\nWe use programming languages to interact with computers.\nYou run commands written in a console and the related task is executed.\n\n\n\n\n\n\n\n\n\nRStudio\n\n\n\n\nRStudio is an Integrated Development Environment or IDE.\nIt helps you using R more efficiently.\nIt has a graphical user interface or GUI.\n\n\n\nThe next section will give you a tour of RStudio."
  },
  {
    "objectID": "posts/r-basics.html#rstudio-1",
    "href": "posts/r-basics.html#rstudio-1",
    "title": "R basics",
    "section": "3 RStudio",
    "text": "3 RStudio\nWhen you open RStudio, you can see the window is divided into 3 panels:\n\nBlue (left): the Console.\nGreen (top-right): the Environment tab.\nPurple (bottom-right): the Files tab.\n\n\nThe Console is where R commands can be executed. Think of this as the interface to R.\nThe Environment tab lists the objects created with R, while in the Files tab you can navigate folders on your computer to get to files and open them in the file Editor.\n\n3.1 RStudio projects\nRStudio is an IDE (see above) which allows you to work efficiently with R, all in one place.\nNote that files and data live in folders on your computer, outside of RStudio: do not think of RStudio as an app where you can save files in.\nAll the files that you see in the Files tab are files on your computer and you can access them from the Finder or File Explorer as you would with any other file.\nIn principle, you can open RStudio and then navigate to any folder or file on your computer.\nHowever, there is a more efficient way of working with RStudio: RStudio Projects.\n\n\n\n\n\n\nRStudio Projects\n\n\n\nAn RStudio Project is a folder on your computer that has an .Rproj file.\n\n\nYou can create as many RStudio Projects as you wish, and I recommend to create one per project (your dissertation, a research project, a course, etc…).\nWe will create an RStudio Project for this course (meaning, you will create a folder for the course which will be the RStudio Project). You will have to use this project/folder throughout the semester.\nTo create a new RStudio Project, click on the button that looks like a transparent light blue box with a plus, in the top-left corner of RStudio. A window like the one below will pop up.\n\nClick on New Directory then New Project.\n\nNow, this will create a new folder (aka directory) on your computer and will make that an RStudio Project (meaning, it will add a file with the .Rproj extension to the folder; the name of the file will be the name of the project/folder).\nGive a name to your new project, something like the name of the course and year (e.g. qml-2023).\nThen you need to specify where to create this new folder/Project. Click on Browse… and navigate to the folder you want to create the new folder/Project in.\nWhen done, click on Create Project. RStudio will automatically open your new project.\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen working through these tutorials, always make sure you are in the course RStudio Project you just created.\nYou know you are in an RStudio Project because you can see the name of the Project in the top-right corner of RStudio, next to the light blue cube icon.\nIf you see Project (none) in the top-right corner, that means your are not in an RStudio Project.\nTo make sure you are in the RStudio project, go to the project folder in File Explorer or Finder and double click on the .Rproj file.\n\n\nThere are several ways of opening an RStudio Project:\n\nYou can go to the RStudio Project folder in Finder or File Explorer and double click on the .Rproj file.\nYou can click on File &gt; Open Project in the RStudio menu.\nYou can click on the project name in the top-right corner of RStudio, which will bring up a list of projects. Click on the desired project to open it.\n\n\n\n3.2 A few important settings\nBefore moving on, there are a few important settings that you need to change.\n\n\nOpen the RStudio preferences (Tools &gt; Global options...).\nUn-tick Restore .RData into workspace at startup.\n\nThis mean that every time you start RStudio you are working with a clean Environment. Not restoring the workspace ensures that the code you write is fully reproducible.\n\nSelect Never in Save workspace to .RData on exit.\n\nSince we are not restoring the workspace at startup, we don’t need to save it. Remember that as long as you save the code, you will not lose any of your work! You will learn how to save code from next week.\n\nClick OK to confirm the changes.\n\n\n\n\n\n\n\nQuiz 1\n\n\n\nTrue or false?\n\nRStudio executes the code. TRUEFALSE\nR is a programming language. TRUEFALSE\nAn IDE is necessary to run R. TRUEFALSE\nRStudio projects are folders with an .Rproj file. TRUEFALSE\nThe project name is shown in the top-right corner of RStudio. TRUEFALSE"
  },
  {
    "objectID": "posts/r-basics.html#r-basics",
    "href": "posts/r-basics.html#r-basics",
    "title": "R basics",
    "section": "4 R basics",
    "text": "4 R basics\nIn this part of the tutorial you will learn the very basics of R.\nIf you have prior experience with programming, you should find all this familiar. If not, not to worry! Make sure you understand the concept highlighted in the green boxes and practice the related skills.\nFor this tutorial, you will just run code directly in the R Console in RStudio, i.e. you will type code in the Console and press ENTER to run it.\nIn future tutorials, you will learn how to save your code in a script file, so that you can keep track of which code you have run and make your work reproducible.\n\n4.1 R as a calculator\nWrite this code 1 + 2 in the Console, then press ENTER/RETURN to run the code.\nFantastic! You should see that the answer of the addition has been printed in the Console, like this:\n[1] 3\n(Never mind the [1] part for now).\nNow, try some more operations (write each of the following in the Console and press ENTER). Feel free to add your own operations to the mix!\n\n67 - 13\n2 * 4\n268 / 43\n\nYou can also chain multiple operations.\n\n6 + 4 - 1 + 2\n4 * 2 + 3 * 2\n\n\n\n\n\n\n\nQuiz 2\n\n\n\nAre the following pairs of operations equivalent?\n\n3 * 2 / 4 = 3 * (2 / 4) TRUEFALSE\n10 * 2 + 5 * 0.2 = (10 * 2 + 5) * 0.2 TRUEFALSE\n\n\n\n\n\n\n\n\n\nExtra: Arithmetics\n\n\n\n\n\nIf you need a maths refresher, I recommend checking the following pages:\n\nhttps://www.mathsisfun.com/definitions/order-of-operations.html\nhttps://www.mathsisfun.com/algebra/introduction.html\n\n\n\n\n\n\n4.2 Variables\n\nForget-me-not.\n\nMost times, we want to store a certain value so that we can use it again later.\nWe can achieve this by creating variables.\n\n\n\n\n\n\nVariable\n\n\n\nA variable holds one or more values and it’s stored in the computer memory for later use.\n\n\nYou can create a variable by using the assignment operator &lt;-.\nLet’s assign the value 156 to the variable my_num.\n\nmy_num &lt;- 156\n\nNow, check the list of variables in the Environment tab of the top-right panel of RStudio. You should see the my_num variable and its value there.\nNow, you can just call the variable back when you need it! Write the following in the Console and press ENTER.\n\nmy_num\n\n[1] 156\n\n\nA variable like my_num is also called a numeric vector: i.e. a vector that contains a number (hence numeric).\n\n\n\n\n\n\nVector\n\n\n\nA vector is an R object that contains one or more values of the same type.\n\n\nA vector is a type of variable and a numeric vector is a type of vector. However, it’s fine in most cases to use the word variable to mean vector (just note that a variable can also be something else than a vector; you will learn about other R objects from next week).\nLet’s now try some operations using variables.\n\nincome &lt;- 1200\nexpenses &lt;- 500\nincome - expenses\n\n[1] 700\n\n\nSee? You can use operations with variables too!\nAnd you can also go all the way with variables.\n\nsavings &lt;- income - expenses\n\nAnd check the value…\n\nsavings\n\n[1] 700\n\n\nVectors can hold more than one item or value.\nJust use the combine c() function to create a vector containing multiple values.\nThe following are all numeric vectors.\n\none_i &lt;- 6\n# Vector with 2 values\ntwo_i &lt;- c(6, 8)\n# Vector with 3 values\nthree_i &lt;- c(6, 8, 42)\n\nCheck the list of variables in the Environment tab. You will see now that before the values of two_i and three_i you get the vector type num for numeric. (If the vector has only one value, you don’t see the type in the Enviroment list but it is still of a specific type).\n\n\n\n\n\n\nNumeric vector\n\n\n\nA numeric vector is a vector that holds one or more numeric values.\n\n\nNote that the following are the same:\n\none_i &lt;- 6\none_i\n\n[1] 6\n\none_ii &lt;- c(6)\none_ii\n\n[1] 6\n\n\nAnother important aspect of variables is that they are… variable! Meaning that once you assign a value to one variable, you can overwrite the value by assigning a new one to the same variable.\n\nmy_num &lt;- 88\nmy_num &lt;- 63\nmy_num\n\n[1] 63\n\n\n\n\n\n\n\n\nQuiz 3\n\n\n\nTrue or false?\n\nA vector is a type of variable. TRUEFALSE\nNot all variables are vectors. TRUEFALSE\nA numeric vector can only hold numeric values. TRUEFALSE\n\n\n\n\n\n4.3 Functions\n\nR cannot function without… functions.\n\n\n\n\n\n\n\nFunction\n\n\n\nA function usually runs an operation on one or more specified arguments.\n\n\nA function in R has the form function() where:\n\nfunction is the name of the function, like sum.\n() are round parentheses, inside of which you write arguments, separated by commas.\n\nLet’s see an example:\n\nsum(3, 5)\n\n[1] 8\n\n\nThe sum() function sums the number listed as arguments. Above, the arguments are 3 and 5.\nAnd of course arguments can be vectors!\n\nmy_nums &lt;- c(3, 5, 7)\n\nsum(my_nums)\n\n[1] 15\n\nmean(my_nums)\n\n[1] 5\n\n\n\n\n\n\n\n\nQuiz 4\n\n\n\nTrue or false?\n\nFunctions can take other functions as arguments. TRUEFALSE\nAll function arguments must be specified. TRUEFALSE\nAll functions need at least one argument. TRUEFALSE\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe Sys.Date() function and other functions like it don’t take any arguments.\n\n\n\n\n\n\n\n\n\nExtra: R vs Python\n\n\n\n\n\nIf you are familiar with Python, you will soon realise that R and Python, although they share many concepts and types of objects, they can differ substantially. This is because R is a functional programming language (based on functions) while Python is an Object Oriented programming language (based on methods applied on objects).\nGenerally speaking, functions look like print(x) while methods look like x.print()\n\n\n\n\n\n4.4 String and logical vectors\n\nNot just numbers.\n\nWe have seen that variables can hold numeric vectors. But vectors are not restricted to being numeric. They can also store strings.\nA string is basically a set of characters (a word, a sentence, a full text).\nIn R, strings have to be quoted using double quotes \" \".\nChange the following strings to your name and surname. Remember to keep the double quotes\n\nname &lt;- \"Stefano\"\nsurname &lt;- \"Coretta\"\n\nname\n\n[1] \"Stefano\"\n\n\nStrings can be used as arguments in functions, like numbers can.\n\ncat(\"My name is\", name, surname)\n\nMy name is Stefano Coretta\n\n\nRemember that you can reuse the same variable name to override the variable value.\n\nname &lt;- \"Raj\"\n\ncat(\"My name is\", name, surname)\n\nMy name is Raj Coretta\n\n\nYou can combine multiple strings into a character vector, using c().\n\n\n\n\n\n\nCharacter vector\n\n\n\nA character vector is a vector that holds one or more strings.\n\n\n\nfruit &lt;- c(\"apple\", \"oranges\", \"bananas\")\nfruit\n\n[1] \"apple\"   \"oranges\" \"bananas\"\n\n\nCheck the Environment tab. Character vectors have chr before the values.\nAnother type of vector is one that contains either TRUE or FALSE. Vectors of this type are called logical vectors and they are listed as logi in the Environment tab.\n\n\n\n\n\n\nLogical vector\n\n\n\nA logical vector is a vector that holds one or more TRUE or FALSE values.\n\n\n\ngroceries &lt;- c(\"apple\", \"flour\", \"margarine\", \"sugar\")\nin_pantry &lt;- c(TRUE, TRUE, FALSE, TRUE)\n\ndata.frame(groceries, in_pantry)\n\n\n  \n\n\n\nTRUE and FALSE values must be written in all capitals and without double quotes (they are not strings!).\n(We will talk about data frames, another type of object in R, in the following weeks.)\n\n\n\n\n\n\nQuiz 5\n\n\n\n\nWhich of the following is not a character vector.\n\n c(1, 2, \"43\") \"s\" c(apple) (assuming apple &lt;- 45) c(letters)\n\nWhich of the following is not a logical vector.\n\n c(T, T, F) TRUE \"FALSE\" c(FALSE)\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou can use the class() function to check the type (“class”) of a vector.\n\nclass(FALSE)\n\n[1] \"logical\"\n\nclass(c(1, 45))\n\n[1] \"numeric\"\n\nclass(c(\"a\", \"b\"))\n\n[1] \"character\"\n\n\n\n\n\n\n\n\n\n\n\nExplanation\n\n\n\n\n\n5a\n\nc(1, 2, \"43\") is a character vector because the last number \"43\" is a string (it’s between double quotes!). A vector cannot have a mix of types of elements: they have to be all numbers or all strings or else, but not some numbers and some strings. Numbers are special in that if you include a number in a character vector without quoting it, it is automatically converted into a string. Try the following:\n\n\nchar &lt;- c(\"a\", \"b\", \"c\")\nchar &lt;- c(char, 1)\nchar\nclass(char)\n\n\nc(letters) is a character vector because letters contains the letters of the alphabet as strings (this vector comes with base R).\nc(apple) is not a character vector because the variable apple holds a number, 45!\n\n5b\n\n\"FALSE\" is not a logical vector because FALSE has been quoted (anything that is quoted is a string!).\n\n\n\n\n\n\n\n\n\n\n\n\nExtra: For-loops and if-else statements\n\n\n\n\n\nThis course does not cover programming in R in the strict sense, but if you are curious here’s a short primer on for-loops and if-else statements in R.\nFor-loops\n\nfruits &lt;- c(\"apples\", \"mangos\", \"durians\")\n\nfor (fruit in fruits) {\n  cat(\"I like\", fruit, \"\\n\")\n}\n\nI like apples \nI like mangos \nI like durians \n\n\nIf-else\n\nfor (fruit in fruits) {\n  if (grepl(\"n\", fruit)) {\n    cat(fruit, \"has an 'n'\", \"\\n\")\n  } else {\n    cat(fruit, \"does not have an 'n'\", \"\\n\")\n  }\n}\n\napples does not have an 'n' \nmangos has an 'n' \ndurians has an 'n'"
  },
  {
    "objectID": "posts/r-basics.html#summary",
    "href": "posts/r-basics.html#summary",
    "title": "R basics",
    "section": "5 Summary",
    "text": "5 Summary\nYou made it! You completed this week’s tutorial.\nHere’s a summary of what you learnt.\n\n\n\n\n\n\n\nR is a programming language while RStudio is an IDE.\nRStudio projects are folders with an .Rproj file (you can see the name of the project you are currently in in the top-right corner of RStudio).\nYou can perform mathematical operations with +, -, *, /.\nYou can store values in variables.\nA typical object to be stored in a variable is a vector: there are different type of vectors, like numeric, character and logical.\nFunctions are used to perform an operation on its arguments: sum() sums it’s arguments, mean() calculates the mean and cat() prints the arguments.\n\n\n\n\n\n\n\n\n\n\nExtra: Programming in R\n\n\n\n\n\nIf you are interested in learning about programming in R, I recommend you go through Chapters 26-28 of the R4DS book and the Advanced R book.\nNote that these topics are not covered in the course, nor will be assessed.\n\n\n\n\n\n\n\n\n\nNext\n\n\n\n\nIntroduction to Quarto\nTroubleshooting"
  }
]